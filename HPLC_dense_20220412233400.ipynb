{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3VoDnJFuApPY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lS8T-dxnwfjO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xFjap2PTfYbv"
      },
      "outputs": [],
      "source": [
        "#emoji_url=\"https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/5e7d7d29f58be438f33fa0c0bc2fd251dea0b453/emoji.py\"\n",
        "#!rm emoji.py\n",
        "#!wget $emoji_url\n",
        "#import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDUZMjUkgIIe",
        "outputId": "74e2bb0e-f630-46ac-8bc8-dcf308892d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*Nmer2*': No such file or directory\n",
            "--2022-04-12 19:31:53--  https://github.com/sipocz/LSTM_HPLC/raw/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/LSTM_HPLC/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv [following]\n",
            "--2022-04-12 19:31:53--  https://raw.githubusercontent.com/sipocz/LSTM_HPLC/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15772588 (15M) [text/plain]\n",
            "Saving to: ‘Nmer2.csv’\n",
            "\n",
            "Nmer2.csv           100%[===================>]  15.04M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-12 19:31:54 (135 MB/s) - ‘Nmer2.csv’ saved [15772588/15772588]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!rm *Nmer2*\n",
        "\n",
        "!wget https://github.com/sipocz/LSTM_HPLC/raw/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv\n",
        "#!wget https://github.com/sipocz/LSTM_HPLC/raw/4371b1ef331c9d0f304e25117f160b979bb39661/orig/N_m3.csv\n",
        "\n",
        "_MODE_=\"Tesztel\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NFiNZuPMkfGl"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"Nmer2.csv\",sep=\",\",skiprows=1,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nCjVAcb2iJZy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "YDtEcJB5haQF",
        "outputId": "309a013a-d99f-4c4c-b277-0510639b7d11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 time        0  0.00333333  0.00666667      0.01  0.0133333  \\\n",
              "0  M13127N_detect3020  1.68793     1.41315    0.755643  0.480864   0.255152   \n",
              "1  M13144N_detect3020  1.92345     1.32483    0.863592  0.559372   0.323847   \n",
              "2  M13153N_detect3020  1.88420     1.27576    0.853778  0.569185   0.333661   \n",
              "3  M14028N_detect3020  0.00000     1.42296    0.942100  0.598626   0.382728   \n",
              "4  M14049N_detect3020  1.59961     1.07949    0.706575  0.441609   0.235525   \n",
              "\n",
              "   0.0166667      0.02  0.0233333  0.0266667  ...  47.9167  47.92  47.9233  \\\n",
              "0   0.107949  0.000000        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "1   0.176644  0.039254        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "2   0.166830  0.039254        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "3   0.206084  0.058881        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "4   0.117762  0.000000        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "\n",
              "   47.9267  47.93  47.9333  47.9367  47.94  Unnamed: 14384  Unnamed: 14385  \n",
              "0      0.0    0.0      0.0      0.0    0.0     2524.709798        megfelel  \n",
              "1      0.0    0.0      0.0      0.0    0.0     2422.244040        megfelel  \n",
              "2      0.0    0.0      0.0      0.0    0.0     2529.530277        megfelel  \n",
              "3      0.0    0.0      0.0      0.0    0.0     2433.146990        megfelel  \n",
              "4      0.0    0.0      0.0      0.0    0.0     2652.605677        megfelel  \n",
              "\n",
              "[5 rows x 14386 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d727610-ee18-4a20-bff8-654f59840caf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>0</th>\n",
              "      <th>0.00333333</th>\n",
              "      <th>0.00666667</th>\n",
              "      <th>0.01</th>\n",
              "      <th>0.0133333</th>\n",
              "      <th>0.0166667</th>\n",
              "      <th>0.02</th>\n",
              "      <th>0.0233333</th>\n",
              "      <th>0.0266667</th>\n",
              "      <th>...</th>\n",
              "      <th>47.9167</th>\n",
              "      <th>47.92</th>\n",
              "      <th>47.9233</th>\n",
              "      <th>47.9267</th>\n",
              "      <th>47.93</th>\n",
              "      <th>47.9333</th>\n",
              "      <th>47.9367</th>\n",
              "      <th>47.94</th>\n",
              "      <th>Unnamed: 14384</th>\n",
              "      <th>Unnamed: 14385</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M13127N_detect3020</td>\n",
              "      <td>1.68793</td>\n",
              "      <td>1.41315</td>\n",
              "      <td>0.755643</td>\n",
              "      <td>0.480864</td>\n",
              "      <td>0.255152</td>\n",
              "      <td>0.107949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2524.709798</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M13144N_detect3020</td>\n",
              "      <td>1.92345</td>\n",
              "      <td>1.32483</td>\n",
              "      <td>0.863592</td>\n",
              "      <td>0.559372</td>\n",
              "      <td>0.323847</td>\n",
              "      <td>0.176644</td>\n",
              "      <td>0.039254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2422.244040</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M13153N_detect3020</td>\n",
              "      <td>1.88420</td>\n",
              "      <td>1.27576</td>\n",
              "      <td>0.853778</td>\n",
              "      <td>0.569185</td>\n",
              "      <td>0.333661</td>\n",
              "      <td>0.166830</td>\n",
              "      <td>0.039254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2529.530277</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M14028N_detect3020</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.42296</td>\n",
              "      <td>0.942100</td>\n",
              "      <td>0.598626</td>\n",
              "      <td>0.382728</td>\n",
              "      <td>0.206084</td>\n",
              "      <td>0.058881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2433.146990</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M14049N_detect3020</td>\n",
              "      <td>1.59961</td>\n",
              "      <td>1.07949</td>\n",
              "      <td>0.706575</td>\n",
              "      <td>0.441609</td>\n",
              "      <td>0.235525</td>\n",
              "      <td>0.117762</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2652.605677</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 14386 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d727610-ee18-4a20-bff8-654f59840caf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d727610-ee18-4a20-bff8-654f59840caf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d727610-ee18-4a20-bff8-654f59840caf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGJM5jz-elNX",
        "outputId": "8d7da5ea-d49d-4b0e-833c-81af102ac969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time              M5C011N_detect3020\n",
              "0                                0.0\n",
              "0.00333333                   1.21688\n",
              "0.00666667                  0.755643\n",
              "0.01                         0.47105\n",
              "                         ...        \n",
              "47.9333                          0.0\n",
              "47.9367                          0.0\n",
              "47.94                            0.0\n",
              "Unnamed: 14384            2190.83243\n",
              "Unnamed: 14385              megfelel\n",
              "Name: 31, Length: 14386, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.iloc[31]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4arFXoEAuVVh"
      },
      "outputs": [],
      "source": [
        "col=df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DxpDNE_3TgCM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dA8Mk878pUdU"
      },
      "outputs": [],
      "source": [
        "df_X=df[col[11000:-2]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "fkrSKZswuyF1",
        "outputId": "313c0afe-259e-4f39-9ea2-c3581003c2fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   36.6633  36.6667  36.67  36.6733  36.6767  36.68  36.6833  36.6867  36.69  \\\n",
              "0      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   36.6933  ...  47.91  47.9133  47.9167  47.92  47.9233  47.9267  47.93  \\\n",
              "0      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   47.9333  47.9367  47.94  \n",
              "0      0.0      0.0    0.0  \n",
              "1      0.0      0.0    0.0  \n",
              "2      0.0      0.0    0.0  \n",
              "3      0.0      0.0    0.0  \n",
              "4      0.0      0.0    0.0  \n",
              "\n",
              "[5 rows x 3384 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02cba682-9028-4391-a0cb-fde168bd37ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>36.6633</th>\n",
              "      <th>36.6667</th>\n",
              "      <th>36.67</th>\n",
              "      <th>36.6733</th>\n",
              "      <th>36.6767</th>\n",
              "      <th>36.68</th>\n",
              "      <th>36.6833</th>\n",
              "      <th>36.6867</th>\n",
              "      <th>36.69</th>\n",
              "      <th>36.6933</th>\n",
              "      <th>...</th>\n",
              "      <th>47.91</th>\n",
              "      <th>47.9133</th>\n",
              "      <th>47.9167</th>\n",
              "      <th>47.92</th>\n",
              "      <th>47.9233</th>\n",
              "      <th>47.9267</th>\n",
              "      <th>47.93</th>\n",
              "      <th>47.9333</th>\n",
              "      <th>47.9367</th>\n",
              "      <th>47.94</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3384 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02cba682-9028-4391-a0cb-fde168bd37ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-02cba682-9028-4391-a0cb-fde168bd37ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-02cba682-9028-4391-a0cb-fde168bd37ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x0YQyPvuTa_9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OczaGqbTqKgz"
      },
      "outputs": [],
      "source": [
        "df_y=df[col[-2:-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dLG_fYjUiUSU"
      },
      "outputs": [],
      "source": [
        "df_y2=df[col[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCzr89LSiaM8",
        "outputId": "0005e194-9064-4e61-e37e-805c45451bda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      megfelel\n",
              "1      megfelel\n",
              "2      megfelel\n",
              "3      megfelel\n",
              "4      megfelel\n",
              "         ...   \n",
              "200    megfelel\n",
              "201    megfelel\n",
              "202    megfelel\n",
              "203    megfelel\n",
              "204    megfelel\n",
              "Name: Unnamed: 14385, Length: 205, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df_y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K9oEj5UHjCg_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "df[\"coded\"] = LE.fit_transform(df_y2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3B5dWWjxB2yN"
      },
      "outputs": [],
      "source": [
        "df_y.columns=[\"Value\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_b6_UVJN64t",
        "outputId": "ff71b147-9361-4ab3-c39a-3e29c89ba73c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     205.000000\n",
              "mean     2185.517049\n",
              "std       370.014482\n",
              "min       231.169975\n",
              "25%      2054.040078\n",
              "50%      2243.486861\n",
              "75%      2422.116922\n",
              "max      2739.643728\n",
              "Name: Value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df_y.Value.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LDFiWnELjilf"
      },
      "outputs": [],
      "source": [
        "df_y2=df[\"coded\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fo6w0xCWjij9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KOBK9cMiBw2h"
      },
      "outputs": [],
      "source": [
        "def min_max_scaling(series):\n",
        "    return (series - series.min()) / (series.max() - series.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TflhyEeDCnWk",
        "outputId": "6ae9460a-d041-4230-cce8-a89b672c70d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231.1699753"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df_y.Value.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d-MBR9tevMFW"
      },
      "outputs": [],
      "source": [
        "df_y_scaled=min_max_scaling(df_y.Value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vGSDdwGqneG",
        "outputId": "49a57ded-a14e-4e8f-95cf-f57b84fb581b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.914317\n",
              "1    0.873469\n",
              "2    0.916239\n",
              "3    0.877815\n",
              "4    0.965302\n",
              "Name: Value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df_y_scaled.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UwyEcsOxmiC1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMOTKbN2G3R"
      },
      "source": [
        "##Mentés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "t8zcn0gafG-W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fsQObqfegZ7M"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LshFLrqKMVuR"
      },
      "outputs": [],
      "source": [
        "#df_train=pd.read_csv(\"train_200.csv\")\n",
        "#df_test=pd.read_csv(\"test_200.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x3HJcQ35MWAn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XomafEhUMWAp"
      },
      "source": [
        "### adatbetöltés mondatok betöltése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9w60OcdVMWAp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqbe8kFxgJav"
      },
      "source": [
        "##Tanulás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tRf3_wt9Bekb"
      },
      "outputs": [],
      "source": [
        "__MAXWORD__=len(df_X.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UwmBdEkijzv",
        "outputId": "801a559d-8eda-4065-8401-f469609fe681"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3384"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "__MAXWORD__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "59MTn_Fs3SvW"
      },
      "outputs": [],
      "source": [
        "lstm_size=25   #25 90% körül teljesített \n",
        "max_input_length=__MAXWORD__\n",
        "\n",
        "\n",
        "n_out=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bD2F2akc1TJ5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS2bWgXsz2zc",
        "outputId": "274bc094-9cef-4867-ec7c-4fcb8dd184b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "\n",
        "#print(lens1)\n",
        "n_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "VKzA6vcSCEm-",
        "outputId": "a0a6fad8-a894-4ca4-8543-eb417cef981f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Value\n",
              "count   205.000000\n",
              "mean   2185.517049\n",
              "std     370.014482\n",
              "min     231.169975\n",
              "25%    2054.040078\n",
              "50%    2243.486861\n",
              "75%    2422.116922\n",
              "max    2739.643728"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f478c7c9-d07d-47fc-b41e-5f1a01958120\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>205.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2185.517049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>370.014482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>231.169975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2054.040078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2243.486861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2422.116922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2739.643728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f478c7c9-d07d-47fc-b41e-5f1a01958120')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f478c7c9-d07d-47fc-b41e-5f1a01958120 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f478c7c9-d07d-47fc-b41e-5f1a01958120');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "df_y.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "B7UMww7mFieO"
      },
      "outputs": [],
      "source": [
        "bins_list=[i*20 for i in range(150)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8u2USy7wPTrB",
        "outputId": "525ad09a-6a8c-410e-b10b-b2744b9a5eab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Value\n",
              "0  2524.709798\n",
              "1  2422.244040\n",
              "2  2529.530277\n",
              "3  2433.146990\n",
              "4  2652.605677"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ff93aca-d2f5-4a93-883f-19d079b6f32e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2524.709798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2422.244040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2529.530277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2433.146990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2652.605677</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ff93aca-d2f5-4a93-883f-19d079b6f32e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ff93aca-d2f5-4a93-883f-19d079b6f32e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ff93aca-d2f5-4a93-883f-19d079b6f32e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "df_y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dXmx6l3eFicN"
      },
      "outputs": [],
      "source": [
        "to_big=df_y.Value[df_y.Value>3000].index.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGwptS6DR5d1",
        "outputId": "b2860bb7-ebdd-42e8-c924-b5cc718c27aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "to_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "cVrXRp4UE5Qk",
        "outputId": "6622864d-1aa7-4891-c8c9-d35378cd3151"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f01d1755c50>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df_y.hist(bins=bins_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JMwhuw-yQRZy"
      },
      "outputs": [],
      "source": [
        "for i in to_big:\n",
        "    df_X=df_X.drop(index=i)\n",
        "    df_y=df_y.drop(index=i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9XgbsvsWRhX9"
      },
      "outputs": [],
      "source": [
        "max_y=df_y.Value.max()\n",
        "min_y=df_y.Value.min()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iyuQlwKRw72",
        "outputId": "a350b0dc-38ef-46fb-c3d0-bc2f95fd23d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2739.643728, 231.1699753)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "(max_y,min_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "V3ujsNSQTYe4"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "df_y_minmax=min_max_scaler.fit_transform(pd.array(df_y.Value).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "N2AHvh8HU0JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "q019xqnl-vmH"
      },
      "outputs": [],
      "source": [
        "df_X_minmax=df_X/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "C01xxvPgJ1wP"
      },
      "outputs": [],
      "source": [
        "# start learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "26U6UXRhmSUE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model_name=\"HPLC\"\n",
        "def scheduler(epoch, lr):\n",
        "       #return 0.00003\n",
        "\n",
        "    '''    \n",
        "    maxx=0.00001\n",
        "    minn=0.001\n",
        "    frekvency=3\n",
        "    o=(epoch % frekvency)/frekvency * (maxx-minn)+minn\n",
        "    return o\n",
        "    '''\n",
        "        \n",
        "    if epoch<4000:\n",
        "        return 0.0005  \n",
        "    elif epoch <80:\n",
        "        return 0.0001\n",
        "    elif epoch <85:\n",
        "        return 0.0005\n",
        "    elif epoch <190:\n",
        "        return 0.0001\n",
        "    \n",
        "    \n",
        "    return 0.00001\n",
        "    \n",
        "callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_loss_{loss:.4f}.hdf5\", monitor='loss',\n",
        "                            verbose=1, save_best_only=False, mode='min')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uvvPuVNSxNa7",
        "outputId": "b8b469b6-a3dd-4555-c73a-2a77a5775b75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional, Conv1D, MaxPooling1D, Dropout\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.optimizers import Adadelta,Adam,SGD,Adamax,RMSprop\\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error, mean_absolute_error, binary_crossentropy\\nfrom tensorflow.keras import backend as K\\n\\nK.clear_session\\nx= Input(shape=(max_input_length,1,))\\n\\n#conv1=Conv1D(filters=8, kernel_size=2, padding=\\'same\\', activation=\\'relu\\')(embedded_x)\\n#MP=MaxPooling1D(pool_size=1)(conv1)\\nlstm_output= Bidirectional(LSTM(units=lstm_size*3,return_sequences=True,dropout=0.1))(x)  #,return_state=True\\nlstm_output= Bidirectional(LSTM(units=lstm_size*2,return_sequences=True,dropout=0.1))(lstm_output)\\nlstm_output= Bidirectional(LSTM(units=lstm_size,return_sequences=True,dropout=0.1))(lstm_output)\\n\\n\\nlstm_output=Dropout(0.1)(lstm_output)\\n\\nDense_out=Flatten()(lstm_output)\\n#Dense_out= Dense(50, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\\n\\npredictions= Dense(n_out, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\\nmodel=Model(inputs=x, outputs=predictions)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "'''\n",
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional, Conv1D, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD,Adamax,RMSprop\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error, mean_absolute_error, binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,1,))\n",
        "\n",
        "#conv1=Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(embedded_x)\n",
        "#MP=MaxPooling1D(pool_size=1)(conv1)\n",
        "lstm_output= Bidirectional(LSTM(units=lstm_size*3,return_sequences=True,dropout=0.1))(x)  #,return_state=True\n",
        "lstm_output= Bidirectional(LSTM(units=lstm_size*2,return_sequences=True,dropout=0.1))(lstm_output)\n",
        "lstm_output= Bidirectional(LSTM(units=lstm_size,return_sequences=True,dropout=0.1))(lstm_output)\n",
        "\n",
        "\n",
        "lstm_output=Dropout(0.1)(lstm_output)\n",
        "\n",
        "Dense_out=Flatten()(lstm_output)\n",
        "#Dense_out= Dense(50, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\n",
        "\n",
        "predictions= Dense(n_out, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW5XdP2VBiiU",
        "outputId": "c051b3f1-af35-4556-c77f-22a0795c641c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3384\n"
          ]
        }
      ],
      "source": [
        "print(max_input_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kSk7_6Tt_ssM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional, Conv1D, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD,Adamax,RMSprop\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error, mean_absolute_error, binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,))\n",
        "\n",
        "#conv1=Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(embedded_x)\n",
        "#MP=MaxPooling1D(pool_size=1)(conv1)\n",
        "dense_o=Dense(units=1000,activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(x)\n",
        "dense_o=Dropout(0.02)(dense_o)\n",
        "dense_o=Dense(units=150,activation=\"sigmoid\",kernel_initializer=\"HeNormal\", name=\"vektor\")(dense_o)  # unit = 100 esetén : 0.0042\n",
        "dense_o=Dropout(0.02)(dense_o)\n",
        "dense_o=Dense(units=1000,activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(dense_o)\n",
        "dense_o=Dropout(0.02)(dense_o)\n",
        "\n",
        "\n",
        "\n",
        "predictions= Dense(units=max_input_length, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(dense_o)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "OPg96E_EHxew"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rK1gsmAC8Wm5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQpADS6gMHmd",
        "outputId": "34158d1f-11ec-45bb-92bf-e02401383b8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3384)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              3385000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " vektor (Dense)              (None, 150)               150150    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1000)              151000    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3384)              3387384   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,073,534\n",
            "Trainable params: 7,073,534\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SzJYM8QeJyvI"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP22GO0AoZkD",
        "outputId": "fe5880ea-d00a-4cd6-9b0b-6e255724e118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'HPLC*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm  HPLC*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JssV4hq6oZih"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ACrVf-TbJYDe"
      },
      "outputs": [],
      "source": [
        "#train_x = np.asarray(xtrain)\n",
        "#train_y = np.asarray(ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jcUtrkpY_Ny-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "RWIM4DP36S2L"
      },
      "outputs": [],
      "source": [
        "# Loss \n",
        "\n",
        "loss =mean_absolute_error #binary_crossentropy  #mean_absolute_error #categorical_crossentropy \n",
        "# Optimizer\n",
        "optimizer = Adam(learning_rate=0.1) #Ízlés szerint...\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss,metrics=[\"mean_absolute_error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FK7a0SH9oYM3"
      },
      "outputs": [],
      "source": [
        "def save_model():\n",
        "    _MODEL_TYPE_=\"lstm_hplc\"\n",
        "\n",
        "    model_name=_MODEL_TYPE_.upper()\n",
        "    callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_LOSS_{loss:.5f}_VLOSS_{val_loss:.5f}_ACC_{accuracy:.4f}_VACC_{val_accuracy:.4f}_.hdf5\", monitor='val_binary_accuracy',\n",
        "                            verbose=1, save_best_only=True, mode='max')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ONvuYqbc-8qV",
        "outputId": "13224d1a-c9e3-462e-90ea-e276ac0fac49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   36.6633  36.6667  36.67  36.6733  36.6767  36.68  36.6833  36.6867  36.69  \\\n",
              "0      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   36.6933  ...  47.91  47.9133  47.9167  47.92  47.9233  47.9267  47.93  \\\n",
              "0      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   47.9333  47.9367  47.94  \n",
              "0      0.0      0.0    0.0  \n",
              "1      0.0      0.0    0.0  \n",
              "2      0.0      0.0    0.0  \n",
              "3      0.0      0.0    0.0  \n",
              "4      0.0      0.0    0.0  \n",
              "\n",
              "[5 rows x 3384 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c807b5ff-5098-4424-aa57-ecaee98418a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>36.6633</th>\n",
              "      <th>36.6667</th>\n",
              "      <th>36.67</th>\n",
              "      <th>36.6733</th>\n",
              "      <th>36.6767</th>\n",
              "      <th>36.68</th>\n",
              "      <th>36.6833</th>\n",
              "      <th>36.6867</th>\n",
              "      <th>36.69</th>\n",
              "      <th>36.6933</th>\n",
              "      <th>...</th>\n",
              "      <th>47.91</th>\n",
              "      <th>47.9133</th>\n",
              "      <th>47.9167</th>\n",
              "      <th>47.92</th>\n",
              "      <th>47.9233</th>\n",
              "      <th>47.9267</th>\n",
              "      <th>47.93</th>\n",
              "      <th>47.9333</th>\n",
              "      <th>47.9367</th>\n",
              "      <th>47.94</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3384 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c807b5ff-5098-4424-aa57-ecaee98418a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c807b5ff-5098-4424-aa57-ecaee98418a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c807b5ff-5098-4424-aa57-ecaee98418a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "df_X_minmax.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wI1SqakKjNO6"
      },
      "outputs": [],
      "source": [
        "#model.load_weights(\"XXXX_0.06212.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yQ8xl2wAVOV",
        "outputId": "39c98b1b-78c0-45dd-b66f-c1e55c70d6a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "df_y2[188]=0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuuuxuvLAozz",
        "outputId": "cfd72c34-050a-4b08-eb79-232cfd609f3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "df_y2[188]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cN_KulIeAoyW"
      },
      "outputs": [],
      "source": [
        "_MODE_=\"Tanul\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDs1Ey1HAZnw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "SYe1udAjRRnM"
      },
      "outputs": [],
      "source": [
        "#_MODE_=\"load\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "hAO4BBOFRHpR"
      },
      "outputs": [],
      "source": [
        "if _MODE_==\"load\":\n",
        "    url=\"https://github.com/sipocz/LSTM_HPLC/raw/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\"\n",
        "    !wget $url\n",
        "    fname=\"XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\"\n",
        "    model.load_weights(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0CeUxTwZRr7U"
      },
      "outputs": [],
      "source": [
        "_MODE_=\"Tanul\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op-I834SFpUX",
        "outputId": "cda521f8-2fb6-4ab3-954f-984e701b36f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.2511 - mean_absolute_error: 0.2511 \n",
            "Epoch 1: saving model to HPLC_loss_0.2243.hdf5\n",
            "16/16 [==============================] - 4s 30ms/step - loss: 0.2243 - mean_absolute_error: 0.2243 - val_loss: 0.1092 - val_mean_absolute_error: 0.1092 - lr: 5.0000e-04\n",
            "Epoch 2/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1104 - mean_absolute_error: 0.1104\n",
            "Epoch 2: saving model to HPLC_loss_0.1104.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.1104 - mean_absolute_error: 0.1104 - val_loss: 0.0973 - val_mean_absolute_error: 0.0973 - lr: 5.0000e-04\n",
            "Epoch 3/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1084 - mean_absolute_error: 0.1084\n",
            "Epoch 3: saving model to HPLC_loss_0.1068.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1068 - mean_absolute_error: 0.1068 - val_loss: 0.0978 - val_mean_absolute_error: 0.0978 - lr: 5.0000e-04\n",
            "Epoch 4/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1022 - mean_absolute_error: 0.1022\n",
            "Epoch 4: saving model to HPLC_loss_0.1032.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1032 - mean_absolute_error: 0.1032 - val_loss: 0.0927 - val_mean_absolute_error: 0.0927 - lr: 5.0000e-04\n",
            "Epoch 5/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0956 - mean_absolute_error: 0.0956\n",
            "Epoch 5: saving model to HPLC_loss_0.0956.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0956 - mean_absolute_error: 0.0956 - val_loss: 0.0865 - val_mean_absolute_error: 0.0865 - lr: 5.0000e-04\n",
            "Epoch 6/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0829 - mean_absolute_error: 0.0829\n",
            "Epoch 6: saving model to HPLC_loss_0.0838.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0838 - mean_absolute_error: 0.0838 - val_loss: 0.0834 - val_mean_absolute_error: 0.0834 - lr: 5.0000e-04\n",
            "Epoch 7/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0715 - mean_absolute_error: 0.0715\n",
            "Epoch 7: saving model to HPLC_loss_0.0733.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0733 - mean_absolute_error: 0.0733 - val_loss: 0.0781 - val_mean_absolute_error: 0.0781 - lr: 5.0000e-04\n",
            "Epoch 8/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0720 - mean_absolute_error: 0.0720\n",
            "Epoch 8: saving model to HPLC_loss_0.0676.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0676 - mean_absolute_error: 0.0676 - val_loss: 0.0755 - val_mean_absolute_error: 0.0755 - lr: 5.0000e-04\n",
            "Epoch 9/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0629 - mean_absolute_error: 0.0629\n",
            "Epoch 9: saving model to HPLC_loss_0.0637.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0637 - mean_absolute_error: 0.0637 - val_loss: 0.0735 - val_mean_absolute_error: 0.0735 - lr: 5.0000e-04\n",
            "Epoch 10/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0617 - mean_absolute_error: 0.0617\n",
            "Epoch 10: saving model to HPLC_loss_0.0608.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0608 - mean_absolute_error: 0.0608 - val_loss: 0.0612 - val_mean_absolute_error: 0.0612 - lr: 5.0000e-04\n",
            "Epoch 11/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0589 - mean_absolute_error: 0.0589\n",
            "Epoch 11: saving model to HPLC_loss_0.0589.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0589 - mean_absolute_error: 0.0589 - val_loss: 0.0600 - val_mean_absolute_error: 0.0600 - lr: 5.0000e-04\n",
            "Epoch 12/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0538 - mean_absolute_error: 0.0538\n",
            "Epoch 12: saving model to HPLC_loss_0.0542.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0574 - val_mean_absolute_error: 0.0574 - lr: 5.0000e-04\n",
            "Epoch 13/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0506 - mean_absolute_error: 0.0506\n",
            "Epoch 13: saving model to HPLC_loss_0.0514.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0514 - mean_absolute_error: 0.0514 - val_loss: 0.0535 - val_mean_absolute_error: 0.0535 - lr: 5.0000e-04\n",
            "Epoch 14/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0515 - mean_absolute_error: 0.0515\n",
            "Epoch 14: saving model to HPLC_loss_0.0484.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0484 - mean_absolute_error: 0.0484 - val_loss: 0.0430 - val_mean_absolute_error: 0.0430 - lr: 5.0000e-04\n",
            "Epoch 15/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0454 - mean_absolute_error: 0.0454\n",
            "Epoch 15: saving model to HPLC_loss_0.0446.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0446 - mean_absolute_error: 0.0446 - val_loss: 0.0473 - val_mean_absolute_error: 0.0473 - lr: 5.0000e-04\n",
            "Epoch 16/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0410 - mean_absolute_error: 0.0410\n",
            "Epoch 16: saving model to HPLC_loss_0.0419.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0419 - mean_absolute_error: 0.0419 - val_loss: 0.0357 - val_mean_absolute_error: 0.0357 - lr: 5.0000e-04\n",
            "Epoch 17/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0408 - mean_absolute_error: 0.0408\n",
            "Epoch 17: saving model to HPLC_loss_0.0406.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0406 - mean_absolute_error: 0.0406 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332 - lr: 5.0000e-04\n",
            "Epoch 18/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0404 - mean_absolute_error: 0.0404\n",
            "Epoch 18: saving model to HPLC_loss_0.0386.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0386 - mean_absolute_error: 0.0386 - val_loss: 0.0347 - val_mean_absolute_error: 0.0347 - lr: 5.0000e-04\n",
            "Epoch 19/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0383 - mean_absolute_error: 0.0383\n",
            "Epoch 19: saving model to HPLC_loss_0.0376.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0376 - mean_absolute_error: 0.0376 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324 - lr: 5.0000e-04\n",
            "Epoch 20/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
            "Epoch 20: saving model to HPLC_loss_0.0364.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404 - lr: 5.0000e-04\n",
            "Epoch 21/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0367 - mean_absolute_error: 0.0367\n",
            "Epoch 21: saving model to HPLC_loss_0.0367.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0367 - mean_absolute_error: 0.0367 - val_loss: 0.0323 - val_mean_absolute_error: 0.0323 - lr: 5.0000e-04\n",
            "Epoch 22/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_absolute_error: 0.0336\n",
            "Epoch 22: saving model to HPLC_loss_0.0349.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0349 - mean_absolute_error: 0.0349 - val_loss: 0.0342 - val_mean_absolute_error: 0.0342 - lr: 5.0000e-04\n",
            "Epoch 23/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0323\n",
            "Epoch 23: saving model to HPLC_loss_0.0338.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0338 - mean_absolute_error: 0.0338 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307 - lr: 5.0000e-04\n",
            "Epoch 24/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
            "Epoch 24: saving model to HPLC_loss_0.0342.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0342 - mean_absolute_error: 0.0342 - val_loss: 0.0318 - val_mean_absolute_error: 0.0318 - lr: 5.0000e-04\n",
            "Epoch 25/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
            "Epoch 25: saving model to HPLC_loss_0.0320.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0258 - val_mean_absolute_error: 0.0258 - lr: 5.0000e-04\n",
            "Epoch 26/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
            "Epoch 26: saving model to HPLC_loss_0.0320.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0308 - val_mean_absolute_error: 0.0308 - lr: 5.0000e-04\n",
            "Epoch 27/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0329 - mean_absolute_error: 0.0329\n",
            "Epoch 27: saving model to HPLC_loss_0.0318.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0249 - val_mean_absolute_error: 0.0249 - lr: 5.0000e-04\n",
            "Epoch 28/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
            "Epoch 28: saving model to HPLC_loss_0.0304.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0304 - mean_absolute_error: 0.0304 - val_loss: 0.0240 - val_mean_absolute_error: 0.0240 - lr: 5.0000e-04\n",
            "Epoch 29/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
            "Epoch 29: saving model to HPLC_loss_0.0297.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0241 - val_mean_absolute_error: 0.0241 - lr: 5.0000e-04\n",
            "Epoch 30/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
            "Epoch 30: saving model to HPLC_loss_0.0294.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0232 - val_mean_absolute_error: 0.0232 - lr: 5.0000e-04\n",
            "Epoch 31/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
            "Epoch 31: saving model to HPLC_loss_0.0291.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0272 - val_mean_absolute_error: 0.0272 - lr: 5.0000e-04\n",
            "Epoch 32/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0283 - mean_absolute_error: 0.0283\n",
            "Epoch 32: saving model to HPLC_loss_0.0291.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260 - lr: 5.0000e-04\n",
            "Epoch 33/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
            "Epoch 33: saving model to HPLC_loss_0.0288.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0238 - val_mean_absolute_error: 0.0238 - lr: 5.0000e-04\n",
            "Epoch 34/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0274 - mean_absolute_error: 0.0274\n",
            "Epoch 34: saving model to HPLC_loss_0.0279.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0279 - mean_absolute_error: 0.0279 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243 - lr: 5.0000e-04\n",
            "Epoch 35/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0265 - mean_absolute_error: 0.0265\n",
            "Epoch 35: saving model to HPLC_loss_0.0274.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0274 - mean_absolute_error: 0.0274 - val_loss: 0.0245 - val_mean_absolute_error: 0.0245 - lr: 5.0000e-04\n",
            "Epoch 36/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0280 - mean_absolute_error: 0.0280\n",
            "Epoch 36: saving model to HPLC_loss_0.0278.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0278 - mean_absolute_error: 0.0278 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250 - lr: 5.0000e-04\n",
            "Epoch 37/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0266 - mean_absolute_error: 0.0266\n",
            "Epoch 37: saving model to HPLC_loss_0.0275.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0275 - mean_absolute_error: 0.0275 - val_loss: 0.0226 - val_mean_absolute_error: 0.0226 - lr: 5.0000e-04\n",
            "Epoch 38/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0276 - mean_absolute_error: 0.0276\n",
            "Epoch 38: saving model to HPLC_loss_0.0271.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0271 - mean_absolute_error: 0.0271 - val_loss: 0.0237 - val_mean_absolute_error: 0.0237 - lr: 5.0000e-04\n",
            "Epoch 39/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0272 - mean_absolute_error: 0.0272\n",
            "Epoch 39: saving model to HPLC_loss_0.0268.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0268 - mean_absolute_error: 0.0268 - val_loss: 0.0218 - val_mean_absolute_error: 0.0218 - lr: 5.0000e-04\n",
            "Epoch 40/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0254 - mean_absolute_error: 0.0254\n",
            "Epoch 40: saving model to HPLC_loss_0.0259.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0259 - mean_absolute_error: 0.0259 - val_loss: 0.0216 - val_mean_absolute_error: 0.0216 - lr: 5.0000e-04\n",
            "Epoch 41/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0276 - mean_absolute_error: 0.0276\n",
            "Epoch 41: saving model to HPLC_loss_0.0263.hdf5\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - val_loss: 0.0207 - val_mean_absolute_error: 0.0207 - lr: 5.0000e-04\n",
            "Epoch 42/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0259 - mean_absolute_error: 0.0259\n",
            "Epoch 42: saving model to HPLC_loss_0.0252.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0252 - mean_absolute_error: 0.0252 - val_loss: 0.0208 - val_mean_absolute_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 43/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0241 - mean_absolute_error: 0.0241\n",
            "Epoch 43: saving model to HPLC_loss_0.0247.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0247 - mean_absolute_error: 0.0247 - val_loss: 0.0218 - val_mean_absolute_error: 0.0218 - lr: 5.0000e-04\n",
            "Epoch 44/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244\n",
            "Epoch 44: saving model to HPLC_loss_0.0247.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0247 - mean_absolute_error: 0.0247 - val_loss: 0.0210 - val_mean_absolute_error: 0.0210 - lr: 5.0000e-04\n",
            "Epoch 45/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0253 - mean_absolute_error: 0.0253\n",
            "Epoch 45: saving model to HPLC_loss_0.0248.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0248 - mean_absolute_error: 0.0248 - val_loss: 0.0219 - val_mean_absolute_error: 0.0219 - lr: 5.0000e-04\n",
            "Epoch 46/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262\n",
            "Epoch 46: saving model to HPLC_loss_0.0254.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0254 - mean_absolute_error: 0.0254 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250 - lr: 5.0000e-04\n",
            "Epoch 47/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0258 - mean_absolute_error: 0.0258\n",
            "Epoch 47: saving model to HPLC_loss_0.0262.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - val_loss: 0.0238 - val_mean_absolute_error: 0.0238 - lr: 5.0000e-04\n",
            "Epoch 48/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0252 - mean_absolute_error: 0.0252\n",
            "Epoch 48: saving model to HPLC_loss_0.0243.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - val_loss: 0.0204 - val_mean_absolute_error: 0.0204 - lr: 5.0000e-04\n",
            "Epoch 49/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0230 - mean_absolute_error: 0.0230\n",
            "Epoch 49: saving model to HPLC_loss_0.0239.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0239 - mean_absolute_error: 0.0239 - val_loss: 0.0197 - val_mean_absolute_error: 0.0197 - lr: 5.0000e-04\n",
            "Epoch 50/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0238 - mean_absolute_error: 0.0238\n",
            "Epoch 50: saving model to HPLC_loss_0.0237.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0237 - mean_absolute_error: 0.0237 - val_loss: 0.0189 - val_mean_absolute_error: 0.0189 - lr: 5.0000e-04\n",
            "Epoch 51/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0247 - mean_absolute_error: 0.0247\n",
            "Epoch 51: saving model to HPLC_loss_0.0235.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0235 - mean_absolute_error: 0.0235 - val_loss: 0.0195 - val_mean_absolute_error: 0.0195 - lr: 5.0000e-04\n",
            "Epoch 52/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0233 - mean_absolute_error: 0.0233\n",
            "Epoch 52: saving model to HPLC_loss_0.0233.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0233 - mean_absolute_error: 0.0233 - val_loss: 0.0194 - val_mean_absolute_error: 0.0194 - lr: 5.0000e-04\n",
            "Epoch 53/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0229 - mean_absolute_error: 0.0229\n",
            "Epoch 53: saving model to HPLC_loss_0.0236.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0236 - mean_absolute_error: 0.0236 - val_loss: 0.0212 - val_mean_absolute_error: 0.0212 - lr: 5.0000e-04\n",
            "Epoch 54/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0232 - mean_absolute_error: 0.0232\n",
            "Epoch 54: saving model to HPLC_loss_0.0239.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - mean_absolute_error: 0.0239 - val_loss: 0.0190 - val_mean_absolute_error: 0.0190 - lr: 5.0000e-04\n",
            "Epoch 55/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0236 - mean_absolute_error: 0.0236\n",
            "Epoch 55: saving model to HPLC_loss_0.0244.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - val_loss: 0.0190 - val_mean_absolute_error: 0.0190 - lr: 5.0000e-04\n",
            "Epoch 56/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0237 - mean_absolute_error: 0.0237\n",
            "Epoch 56: saving model to HPLC_loss_0.0245.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275 - lr: 5.0000e-04\n",
            "Epoch 57/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0234 - mean_absolute_error: 0.0234\n",
            "Epoch 57: saving model to HPLC_loss_0.0235.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0235 - mean_absolute_error: 0.0235 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201 - lr: 5.0000e-04\n",
            "Epoch 58/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0232 - mean_absolute_error: 0.0232\n",
            "Epoch 58: saving model to HPLC_loss_0.0231.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0231 - mean_absolute_error: 0.0231 - val_loss: 0.0194 - val_mean_absolute_error: 0.0194 - lr: 5.0000e-04\n",
            "Epoch 59/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0231 - mean_absolute_error: 0.0231\n",
            "Epoch 59: saving model to HPLC_loss_0.0227.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0227 - mean_absolute_error: 0.0227 - val_loss: 0.0200 - val_mean_absolute_error: 0.0200 - lr: 5.0000e-04\n",
            "Epoch 60/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0233 - mean_absolute_error: 0.0233\n",
            "Epoch 60: saving model to HPLC_loss_0.0233.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0233 - mean_absolute_error: 0.0233 - val_loss: 0.0195 - val_mean_absolute_error: 0.0195 - lr: 5.0000e-04\n",
            "Epoch 61/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0231 - mean_absolute_error: 0.0231\n",
            "Epoch 61: saving model to HPLC_loss_0.0223.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0223 - mean_absolute_error: 0.0223 - val_loss: 0.0182 - val_mean_absolute_error: 0.0182 - lr: 5.0000e-04\n",
            "Epoch 62/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0225 - mean_absolute_error: 0.0225\n",
            "Epoch 62: saving model to HPLC_loss_0.0222.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0222 - mean_absolute_error: 0.0222 - val_loss: 0.0171 - val_mean_absolute_error: 0.0171 - lr: 5.0000e-04\n",
            "Epoch 63/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0223 - mean_absolute_error: 0.0223\n",
            "Epoch 63: saving model to HPLC_loss_0.0216.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0216 - mean_absolute_error: 0.0216 - val_loss: 0.0181 - val_mean_absolute_error: 0.0181 - lr: 5.0000e-04\n",
            "Epoch 64/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0225 - mean_absolute_error: 0.0225\n",
            "Epoch 64: saving model to HPLC_loss_0.0217.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0217 - mean_absolute_error: 0.0217 - val_loss: 0.0181 - val_mean_absolute_error: 0.0181 - lr: 5.0000e-04\n",
            "Epoch 65/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0226 - mean_absolute_error: 0.0226\n",
            "Epoch 65: saving model to HPLC_loss_0.0220.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0220 - mean_absolute_error: 0.0220 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176 - lr: 5.0000e-04\n",
            "Epoch 66/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0221 - mean_absolute_error: 0.0221\n",
            "Epoch 66: saving model to HPLC_loss_0.0220.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0220 - mean_absolute_error: 0.0220 - val_loss: 0.0196 - val_mean_absolute_error: 0.0196 - lr: 5.0000e-04\n",
            "Epoch 67/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0222 - mean_absolute_error: 0.0222\n",
            "Epoch 67: saving model to HPLC_loss_0.0219.hdf5\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0219 - mean_absolute_error: 0.0219 - val_loss: 0.0215 - val_mean_absolute_error: 0.0215 - lr: 5.0000e-04\n",
            "Epoch 68/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0228 - mean_absolute_error: 0.0228\n",
            "Epoch 68: saving model to HPLC_loss_0.0222.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0222 - mean_absolute_error: 0.0222 - val_loss: 0.0216 - val_mean_absolute_error: 0.0216 - lr: 5.0000e-04\n",
            "Epoch 69/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0229 - mean_absolute_error: 0.0229\n",
            "Epoch 69: saving model to HPLC_loss_0.0222.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0222 - mean_absolute_error: 0.0222 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178 - lr: 5.0000e-04\n",
            "Epoch 70/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0209 - mean_absolute_error: 0.0209\n",
            "Epoch 70: saving model to HPLC_loss_0.0211.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0211 - mean_absolute_error: 0.0211 - val_loss: 0.0165 - val_mean_absolute_error: 0.0165 - lr: 5.0000e-04\n",
            "Epoch 71/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0215 - mean_absolute_error: 0.0215\n",
            "Epoch 71: saving model to HPLC_loss_0.0210.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0210 - mean_absolute_error: 0.0210 - val_loss: 0.0167 - val_mean_absolute_error: 0.0167 - lr: 5.0000e-04\n",
            "Epoch 72/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0217 - mean_absolute_error: 0.0217\n",
            "Epoch 72: saving model to HPLC_loss_0.0212.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0212 - mean_absolute_error: 0.0212 - val_loss: 0.0173 - val_mean_absolute_error: 0.0173 - lr: 5.0000e-04\n",
            "Epoch 73/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0215 - mean_absolute_error: 0.0215\n",
            "Epoch 73: saving model to HPLC_loss_0.0211.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0211 - mean_absolute_error: 0.0211 - val_loss: 0.0170 - val_mean_absolute_error: 0.0170 - lr: 5.0000e-04\n",
            "Epoch 74/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0216 - mean_absolute_error: 0.0216\n",
            "Epoch 74: saving model to HPLC_loss_0.0209.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0209 - mean_absolute_error: 0.0209 - val_loss: 0.0180 - val_mean_absolute_error: 0.0180 - lr: 5.0000e-04\n",
            "Epoch 75/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0226 - mean_absolute_error: 0.0226\n",
            "Epoch 75: saving model to HPLC_loss_0.0213.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0213 - mean_absolute_error: 0.0213 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176 - lr: 5.0000e-04\n",
            "Epoch 76/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0207 - mean_absolute_error: 0.0207\n",
            "Epoch 76: saving model to HPLC_loss_0.0205.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0205 - mean_absolute_error: 0.0205 - val_loss: 0.0171 - val_mean_absolute_error: 0.0171 - lr: 5.0000e-04\n",
            "Epoch 77/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0199 - mean_absolute_error: 0.0199\n",
            "Epoch 77: saving model to HPLC_loss_0.0203.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0203 - mean_absolute_error: 0.0203 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160 - lr: 5.0000e-04\n",
            "Epoch 78/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0208 - mean_absolute_error: 0.0208\n",
            "Epoch 78: saving model to HPLC_loss_0.0201.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0201 - mean_absolute_error: 0.0201 - val_loss: 0.0173 - val_mean_absolute_error: 0.0173 - lr: 5.0000e-04\n",
            "Epoch 79/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0207 - mean_absolute_error: 0.0207\n",
            "Epoch 79: saving model to HPLC_loss_0.0207.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0207 - mean_absolute_error: 0.0207 - val_loss: 0.0190 - val_mean_absolute_error: 0.0190 - lr: 5.0000e-04\n",
            "Epoch 80/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0212 - mean_absolute_error: 0.0212\n",
            "Epoch 80: saving model to HPLC_loss_0.0206.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0206 - mean_absolute_error: 0.0206 - val_loss: 0.0171 - val_mean_absolute_error: 0.0171 - lr: 5.0000e-04\n",
            "Epoch 81/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0204 - mean_absolute_error: 0.0204\n",
            "Epoch 81: saving model to HPLC_loss_0.0199.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0199 - mean_absolute_error: 0.0199 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152 - lr: 5.0000e-04\n",
            "Epoch 82/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0195 - mean_absolute_error: 0.0195\n",
            "Epoch 82: saving model to HPLC_loss_0.0202.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0202 - mean_absolute_error: 0.0202 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176 - lr: 5.0000e-04\n",
            "Epoch 83/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0186 - mean_absolute_error: 0.0186\n",
            "Epoch 83: saving model to HPLC_loss_0.0194.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0194 - mean_absolute_error: 0.0194 - val_loss: 0.0170 - val_mean_absolute_error: 0.0170 - lr: 5.0000e-04\n",
            "Epoch 84/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0198 - mean_absolute_error: 0.0198\n",
            "Epoch 84: saving model to HPLC_loss_0.0196.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0196 - mean_absolute_error: 0.0196 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 85/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0193 - mean_absolute_error: 0.0193\n",
            "Epoch 85: saving model to HPLC_loss_0.0195.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0195 - mean_absolute_error: 0.0195 - val_loss: 0.0179 - val_mean_absolute_error: 0.0179 - lr: 5.0000e-04\n",
            "Epoch 86/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0195 - mean_absolute_error: 0.0195\n",
            "Epoch 86: saving model to HPLC_loss_0.0200.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0200 - mean_absolute_error: 0.0200 - val_loss: 0.0194 - val_mean_absolute_error: 0.0194 - lr: 5.0000e-04\n",
            "Epoch 87/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0194 - mean_absolute_error: 0.0194\n",
            "Epoch 87: saving model to HPLC_loss_0.0196.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0196 - mean_absolute_error: 0.0196 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146 - lr: 5.0000e-04\n",
            "Epoch 88/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0196 - mean_absolute_error: 0.0196\n",
            "Epoch 88: saving model to HPLC_loss_0.0196.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0196 - mean_absolute_error: 0.0196 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 89/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0181 - mean_absolute_error: 0.0181\n",
            "Epoch 89: saving model to HPLC_loss_0.0189.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0189 - mean_absolute_error: 0.0189 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176 - lr: 5.0000e-04\n",
            "Epoch 90/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0192 - mean_absolute_error: 0.0192\n",
            "Epoch 90: saving model to HPLC_loss_0.0195.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0195 - mean_absolute_error: 0.0195 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160 - lr: 5.0000e-04\n",
            "Epoch 91/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0192 - mean_absolute_error: 0.0192\n",
            "Epoch 91: saving model to HPLC_loss_0.0188.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0188 - mean_absolute_error: 0.0188 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - lr: 5.0000e-04\n",
            "Epoch 92/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0184 - mean_absolute_error: 0.0184\n",
            "Epoch 92: saving model to HPLC_loss_0.0187.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0187 - mean_absolute_error: 0.0187 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152 - lr: 5.0000e-04\n",
            "Epoch 93/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0185 - mean_absolute_error: 0.0185\n",
            "Epoch 93: saving model to HPLC_loss_0.0187.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0187 - mean_absolute_error: 0.0187 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150 - lr: 5.0000e-04\n",
            "Epoch 94/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0181 - mean_absolute_error: 0.0181\n",
            "Epoch 94: saving model to HPLC_loss_0.0190.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0190 - mean_absolute_error: 0.0190 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162 - lr: 5.0000e-04\n",
            "Epoch 95/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0191 - mean_absolute_error: 0.0191\n",
            "Epoch 95: saving model to HPLC_loss_0.0200.hdf5\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 0.0200 - mean_absolute_error: 0.0200 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 96/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0188 - mean_absolute_error: 0.0188\n",
            "Epoch 96: saving model to HPLC_loss_0.0190.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0190 - mean_absolute_error: 0.0190 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164 - lr: 5.0000e-04\n",
            "Epoch 97/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0188 - mean_absolute_error: 0.0188\n",
            "Epoch 97: saving model to HPLC_loss_0.0192.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0192 - mean_absolute_error: 0.0192 - val_loss: 0.0165 - val_mean_absolute_error: 0.0165 - lr: 5.0000e-04\n",
            "Epoch 98/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0201 - mean_absolute_error: 0.0201\n",
            "Epoch 98: saving model to HPLC_loss_0.0196.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0196 - mean_absolute_error: 0.0196 - val_loss: 0.0175 - val_mean_absolute_error: 0.0175 - lr: 5.0000e-04\n",
            "Epoch 99/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0183 - mean_absolute_error: 0.0183\n",
            "Epoch 99: saving model to HPLC_loss_0.0183.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0183 - mean_absolute_error: 0.0183 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - lr: 5.0000e-04\n",
            "Epoch 100/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0180 - mean_absolute_error: 0.0180\n",
            "Epoch 100: saving model to HPLC_loss_0.0179.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0163 - val_mean_absolute_error: 0.0163 - lr: 5.0000e-04\n",
            "Epoch 101/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0188 - mean_absolute_error: 0.0188\n",
            "Epoch 101: saving model to HPLC_loss_0.0179.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154 - lr: 5.0000e-04\n",
            "Epoch 102/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0176 - mean_absolute_error: 0.0176\n",
            "Epoch 102: saving model to HPLC_loss_0.0176.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0153 - val_mean_absolute_error: 0.0153 - lr: 5.0000e-04\n",
            "Epoch 103/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0177 - mean_absolute_error: 0.0177\n",
            "Epoch 103: saving model to HPLC_loss_0.0170.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 104/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0187 - mean_absolute_error: 0.0187\n",
            "Epoch 104: saving model to HPLC_loss_0.0179.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0206 - val_mean_absolute_error: 0.0206 - lr: 5.0000e-04\n",
            "Epoch 105/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0199 - mean_absolute_error: 0.0199\n",
            "Epoch 105: saving model to HPLC_loss_0.0185.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0185 - mean_absolute_error: 0.0185 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 106/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0195 - mean_absolute_error: 0.0195\n",
            "Epoch 106: saving model to HPLC_loss_0.0189.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0189 - mean_absolute_error: 0.0189 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168 - lr: 5.0000e-04\n",
            "Epoch 107/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0181 - mean_absolute_error: 0.0181\n",
            "Epoch 107: saving model to HPLC_loss_0.0185.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0185 - mean_absolute_error: 0.0185 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - lr: 5.0000e-04\n",
            "Epoch 108/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0189 - mean_absolute_error: 0.0189\n",
            "Epoch 108: saving model to HPLC_loss_0.0180.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0180 - mean_absolute_error: 0.0180 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 109/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0179 - mean_absolute_error: 0.0179\n",
            "Epoch 109: saving model to HPLC_loss_0.0184.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0184 - mean_absolute_error: 0.0184 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - lr: 5.0000e-04\n",
            "Epoch 110/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0176 - mean_absolute_error: 0.0176\n",
            "Epoch 110: saving model to HPLC_loss_0.0173.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0173 - mean_absolute_error: 0.0173 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 111/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0171 - mean_absolute_error: 0.0171\n",
            "Epoch 111: saving model to HPLC_loss_0.0168.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0168 - mean_absolute_error: 0.0168 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - lr: 5.0000e-04\n",
            "Epoch 112/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0171 - mean_absolute_error: 0.0171\n",
            "Epoch 112: saving model to HPLC_loss_0.0170.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150 - lr: 5.0000e-04\n",
            "Epoch 113/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0158 - mean_absolute_error: 0.0158\n",
            "Epoch 113: saving model to HPLC_loss_0.0170.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - lr: 5.0000e-04\n",
            "Epoch 114/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0166 - mean_absolute_error: 0.0166\n",
            "Epoch 114: saving model to HPLC_loss_0.0165.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0165 - mean_absolute_error: 0.0165 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - lr: 5.0000e-04\n",
            "Epoch 115/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.0159\n",
            "Epoch 115: saving model to HPLC_loss_0.0162.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0162 - mean_absolute_error: 0.0162 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - lr: 5.0000e-04\n",
            "Epoch 116/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0163 - mean_absolute_error: 0.0163\n",
            "Epoch 116: saving model to HPLC_loss_0.0167.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0167 - mean_absolute_error: 0.0167 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - lr: 5.0000e-04\n",
            "Epoch 117/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0160 - mean_absolute_error: 0.0160\n",
            "Epoch 117: saving model to HPLC_loss_0.0163.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0163 - mean_absolute_error: 0.0163 - val_loss: 0.0149 - val_mean_absolute_error: 0.0149 - lr: 5.0000e-04\n",
            "Epoch 118/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0167 - mean_absolute_error: 0.0167\n",
            "Epoch 118: saving model to HPLC_loss_0.0161.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0161 - mean_absolute_error: 0.0161 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147 - lr: 5.0000e-04\n",
            "Epoch 119/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0157 - mean_absolute_error: 0.0157\n",
            "Epoch 119: saving model to HPLC_loss_0.0158.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0158 - mean_absolute_error: 0.0158 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 120/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0154 - mean_absolute_error: 0.0154\n",
            "Epoch 120: saving model to HPLC_loss_0.0159.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0159 - mean_absolute_error: 0.0159 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 121/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0149 - mean_absolute_error: 0.0149\n",
            "Epoch 121: saving model to HPLC_loss_0.0156.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0156 - mean_absolute_error: 0.0156 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 122/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0152 - mean_absolute_error: 0.0152\n",
            "Epoch 122: saving model to HPLC_loss_0.0152.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0152 - mean_absolute_error: 0.0152 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - lr: 5.0000e-04\n",
            "Epoch 123/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0158 - mean_absolute_error: 0.0158\n",
            "Epoch 123: saving model to HPLC_loss_0.0160.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0160 - mean_absolute_error: 0.0160 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - lr: 5.0000e-04\n",
            "Epoch 124/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0149 - mean_absolute_error: 0.0149\n",
            "Epoch 124: saving model to HPLC_loss_0.0149.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 125/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0156 - mean_absolute_error: 0.0156\n",
            "Epoch 125: saving model to HPLC_loss_0.0149.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - lr: 5.0000e-04\n",
            "Epoch 126/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0153 - mean_absolute_error: 0.0153\n",
            "Epoch 126: saving model to HPLC_loss_0.0154.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0154 - mean_absolute_error: 0.0154 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 127/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0148 - mean_absolute_error: 0.0148\n",
            "Epoch 127: saving model to HPLC_loss_0.0154.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0154 - mean_absolute_error: 0.0154 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - lr: 5.0000e-04\n",
            "Epoch 128/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0154 - mean_absolute_error: 0.0154\n",
            "Epoch 128: saving model to HPLC_loss_0.0155.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - lr: 5.0000e-04\n",
            "Epoch 129/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0147 - mean_absolute_error: 0.0147\n",
            "Epoch 129: saving model to HPLC_loss_0.0151.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0151 - mean_absolute_error: 0.0151 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - lr: 5.0000e-04\n",
            "Epoch 130/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.0159\n",
            "Epoch 130: saving model to HPLC_loss_0.0153.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - lr: 5.0000e-04\n",
            "Epoch 131/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0155 - mean_absolute_error: 0.0155\n",
            "Epoch 131: saving model to HPLC_loss_0.0153.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.0202 - val_mean_absolute_error: 0.0202 - lr: 5.0000e-04\n",
            "Epoch 132/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0161 - mean_absolute_error: 0.0161\n",
            "Epoch 132: saving model to HPLC_loss_0.0164.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0164 - mean_absolute_error: 0.0164 - val_loss: 0.0200 - val_mean_absolute_error: 0.0200 - lr: 5.0000e-04\n",
            "Epoch 133/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0157 - mean_absolute_error: 0.0157\n",
            "Epoch 133: saving model to HPLC_loss_0.0157.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152 - lr: 5.0000e-04\n",
            "Epoch 134/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0154 - mean_absolute_error: 0.0154\n",
            "Epoch 134: saving model to HPLC_loss_0.0151.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0151 - mean_absolute_error: 0.0151 - val_loss: 0.0168 - val_mean_absolute_error: 0.0168 - lr: 5.0000e-04\n",
            "Epoch 135/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0162 - mean_absolute_error: 0.0162\n",
            "Epoch 135: saving model to HPLC_loss_0.0157.hdf5\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 136/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0149 - mean_absolute_error: 0.0149\n",
            "Epoch 136: saving model to HPLC_loss_0.0148.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0148 - mean_absolute_error: 0.0148 - val_loss: 0.0166 - val_mean_absolute_error: 0.0166 - lr: 5.0000e-04\n",
            "Epoch 137/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0146 - mean_absolute_error: 0.0146\n",
            "Epoch 137: saving model to HPLC_loss_0.0148.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0148 - mean_absolute_error: 0.0148 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154 - lr: 5.0000e-04\n",
            "Epoch 138/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
            "Epoch 138: saving model to HPLC_loss_0.0145.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0145 - mean_absolute_error: 0.0145 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146 - lr: 5.0000e-04\n",
            "Epoch 139/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0134 - mean_absolute_error: 0.0134\n",
            "Epoch 139: saving model to HPLC_loss_0.0142.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 140/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0144 - mean_absolute_error: 0.0144\n",
            "Epoch 140: saving model to HPLC_loss_0.0141.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0141 - mean_absolute_error: 0.0141 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - lr: 5.0000e-04\n",
            "Epoch 141/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
            "Epoch 141: saving model to HPLC_loss_0.0142.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 142/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0136 - mean_absolute_error: 0.0136\n",
            "Epoch 142: saving model to HPLC_loss_0.0139.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 143/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0138 - mean_absolute_error: 0.0138\n",
            "Epoch 143: saving model to HPLC_loss_0.0139.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 144/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0132 - mean_absolute_error: 0.0132\n",
            "Epoch 144: saving model to HPLC_loss_0.0135.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0135 - mean_absolute_error: 0.0135 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 145/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0133 - mean_absolute_error: 0.0133\n",
            "Epoch 145: saving model to HPLC_loss_0.0134.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152 - lr: 5.0000e-04\n",
            "Epoch 146/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
            "Epoch 146: saving model to HPLC_loss_0.0139.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0185 - val_mean_absolute_error: 0.0185 - lr: 5.0000e-04\n",
            "Epoch 147/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
            "Epoch 147: saving model to HPLC_loss_0.0139.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0156 - val_mean_absolute_error: 0.0156 - lr: 5.0000e-04\n",
            "Epoch 148/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 148: saving model to HPLC_loss_0.0144.hdf5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0144 - mean_absolute_error: 0.0144 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 149/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0134 - mean_absolute_error: 0.0134\n",
            "Epoch 149: saving model to HPLC_loss_0.0134.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - lr: 5.0000e-04\n",
            "Epoch 150/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0130 - mean_absolute_error: 0.0130\n",
            "Epoch 150: saving model to HPLC_loss_0.0130.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0130 - mean_absolute_error: 0.0130 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 151/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0128\n",
            "Epoch 151: saving model to HPLC_loss_0.0133.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 152/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0130 - mean_absolute_error: 0.0130\n",
            "Epoch 152: saving model to HPLC_loss_0.0130.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0130 - mean_absolute_error: 0.0130 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - lr: 5.0000e-04\n",
            "Epoch 153/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0133 - mean_absolute_error: 0.0133\n",
            "Epoch 153: saving model to HPLC_loss_0.0132.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0132 - mean_absolute_error: 0.0132 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 154/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0128\n",
            "Epoch 154: saving model to HPLC_loss_0.0127.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 155/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0129\n",
            "Epoch 155: saving model to HPLC_loss_0.0132.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0132 - mean_absolute_error: 0.0132 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - lr: 5.0000e-04\n",
            "Epoch 156/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0129\n",
            "Epoch 156: saving model to HPLC_loss_0.0130.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0130 - mean_absolute_error: 0.0130 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - lr: 5.0000e-04\n",
            "Epoch 157/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0127 - mean_absolute_error: 0.0127\n",
            "Epoch 157: saving model to HPLC_loss_0.0129.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0129 - mean_absolute_error: 0.0129 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 158/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 158: saving model to HPLC_loss_0.0126.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 159/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0131 - mean_absolute_error: 0.0131\n",
            "Epoch 159: saving model to HPLC_loss_0.0130.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0130 - mean_absolute_error: 0.0130 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - lr: 5.0000e-04\n",
            "Epoch 160/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 160: saving model to HPLC_loss_0.0137.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 161/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0134 - mean_absolute_error: 0.0134\n",
            "Epoch 161: saving model to HPLC_loss_0.0131.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0174 - val_mean_absolute_error: 0.0174 - lr: 5.0000e-04\n",
            "Epoch 162/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
            "Epoch 162: saving model to HPLC_loss_0.0136.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - lr: 5.0000e-04\n",
            "Epoch 163/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0131 - mean_absolute_error: 0.0131\n",
            "Epoch 163: saving model to HPLC_loss_0.0131.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 164/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0129\n",
            "Epoch 164: saving model to HPLC_loss_0.0128.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 165/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 165: saving model to HPLC_loss_0.0126.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 166/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
            "Epoch 166: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - lr: 5.0000e-04\n",
            "Epoch 167/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 167: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 168/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0130 - mean_absolute_error: 0.0130\n",
            "Epoch 168: saving model to HPLC_loss_0.0128.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 169/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0127 - mean_absolute_error: 0.0127\n",
            "Epoch 169: saving model to HPLC_loss_0.0127.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 170/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 170: saving model to HPLC_loss_0.0122.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - lr: 5.0000e-04\n",
            "Epoch 171/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 171: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 172/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 172: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 173/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 173: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 174/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 174: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - lr: 5.0000e-04\n",
            "Epoch 175/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
            "Epoch 175: saving model to HPLC_loss_0.0121.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 176/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 176: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 177/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
            "Epoch 177: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 178/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 178: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - lr: 5.0000e-04\n",
            "Epoch 179/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
            "Epoch 179: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 180/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 180: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - lr: 5.0000e-04\n",
            "Epoch 181/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0116 - mean_absolute_error: 0.0116\n",
            "Epoch 181: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 182/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 182: saving model to HPLC_loss_0.0127.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - lr: 5.0000e-04\n",
            "Epoch 183/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 183: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 184/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 184: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - lr: 5.0000e-04\n",
            "Epoch 185/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0114\n",
            "Epoch 185: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 186/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 186: saving model to HPLC_loss_0.0121.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 187/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
            "Epoch 187: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 188/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 188: saving model to HPLC_loss_0.0114.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 189/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 189: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 190/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 190: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 191/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
            "Epoch 191: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 192/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0114\n",
            "Epoch 192: saving model to HPLC_loss_0.0114.hdf5\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 193/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0114\n",
            "Epoch 193: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 194/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0128\n",
            "Epoch 194: saving model to HPLC_loss_0.0125.hdf5\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 0.0125 - mean_absolute_error: 0.0125 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 195/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0132 - mean_absolute_error: 0.0132\n",
            "Epoch 195: saving model to HPLC_loss_0.0131.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164 - lr: 5.0000e-04\n",
            "Epoch 196/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 196: saving model to HPLC_loss_0.0122.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 197/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0125 - mean_absolute_error: 0.0125\n",
            "Epoch 197: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201 - lr: 5.0000e-04\n",
            "Epoch 198/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 198: saving model to HPLC_loss_0.0122.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - lr: 5.0000e-04\n",
            "Epoch 199/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 199: saving model to HPLC_loss_0.0115.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 200/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 200: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 201/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0114\n",
            "Epoch 201: saving model to HPLC_loss_0.0114.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 202/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 202: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 203/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 203: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 204/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 204: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 205/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 205: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 206/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 206: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 207/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0116 - mean_absolute_error: 0.0116\n",
            "Epoch 207: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 208/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 208: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - lr: 5.0000e-04\n",
            "Epoch 209/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 209: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 210/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 210: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 211/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 211: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142 - lr: 5.0000e-04\n",
            "Epoch 212/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 212: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 213/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 213: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - lr: 5.0000e-04\n",
            "Epoch 214/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 214: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - lr: 5.0000e-04\n",
            "Epoch 215/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 215: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 216/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0107\n",
            "Epoch 216: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 217/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 217: saving model to HPLC_loss_0.0110.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 218/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 218: saving model to HPLC_loss_0.0108.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 219/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 219: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 220/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 220: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 221/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 221: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 222/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 222: saving model to HPLC_loss_0.0108.hdf5\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 223/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 223: saving model to HPLC_loss_0.0110.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 224/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 224: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 225/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0107\n",
            "Epoch 225: saving model to HPLC_loss_0.0107.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 226/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0109 - mean_absolute_error: 0.0109\n",
            "Epoch 226: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 227/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 227: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 228/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 228: saving model to HPLC_loss_0.0108.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 229/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 229: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - lr: 5.0000e-04\n",
            "Epoch 230/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 230: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 231/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 231: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 232/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
            "Epoch 232: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 233/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 233: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 234/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 234: saving model to HPLC_loss_0.0110.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 235/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
            "Epoch 235: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 236/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 236: saving model to HPLC_loss_0.0108.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 237/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 237: saving model to HPLC_loss_0.0108.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 238/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 238: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - lr: 5.0000e-04\n",
            "Epoch 239/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 239: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 240/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 240: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 241/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 241: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 242/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0107\n",
            "Epoch 242: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 243/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 243: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 244/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 244: saving model to HPLC_loss_0.0103.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - lr: 5.0000e-04\n",
            "Epoch 245/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 245: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 246/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 246: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 247/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 247: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 248/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 248: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 249/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 249: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 250/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 250: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 251/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 251: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 252/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 252: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 253/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 253: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 254/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 254: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 255/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 255: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 256/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 256: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 257/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 257: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 258/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 258: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 259/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 259: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 260/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 260: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 261/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 261: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 262/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 262: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 263/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 263: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 264/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0107\n",
            "Epoch 264: saving model to HPLC_loss_0.0107.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 265/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 265: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 266/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 266: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 267/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 267: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 268/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 268: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 269/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 269: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 270/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 270: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 271/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 271: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 272/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 272: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 273/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 273: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 274/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 274: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 275/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 275: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 276/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 276: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 277/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 277: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 278/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 278: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 279/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 279: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 280/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 280: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 281/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 281: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 282/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 282: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 283/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 283: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 284/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 284: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - lr: 5.0000e-04\n",
            "Epoch 285/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 285: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 286/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 286: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 287/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
            "Epoch 287: saving model to HPLC_loss_0.0103.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 288/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 288: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 289/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 289: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 290/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 290: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 291/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 291: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 292/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 292: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 293/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 293: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 294/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 294: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 295/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 295: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 296/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 296: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 297/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 297: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 298/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 298: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 299/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 299: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 300/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 300: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 301/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 301: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 302/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 302: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 303/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 303: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 304/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 304: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 305/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 305: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 306/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 306: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 307/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 307: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 308/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 308: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 309/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 309: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 310/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 310: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 311/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 311: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 312/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 312: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 313/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 313: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 314/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 314: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 315/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 315: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 316/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 316: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 317/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 317: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 318/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 318: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 319/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 319: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 320/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 320: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 321/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 321: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 322/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 322: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 323/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 323: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 324/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 324: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082 - lr: 5.0000e-04\n",
            "Epoch 325/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 325: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 326/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 326: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 327/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 327: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082 - lr: 5.0000e-04\n",
            "Epoch 328/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 328: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 329/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 329: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 330/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 330: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 331/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 331: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 332/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 332: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080 - lr: 5.0000e-04\n",
            "Epoch 333/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 333: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 334/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 334: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 335/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 335: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - lr: 5.0000e-04\n",
            "Epoch 336/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 336: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 337/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 337: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 338/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 338: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 339/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 339: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 340/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 340: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 341/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 341: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 342/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0078 - mean_absolute_error: 0.0078\n",
            "Epoch 342: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 343/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 343: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 344/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 344: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 345/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 345: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 346/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 346: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 347/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 347: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 348/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 348: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 349/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 349: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 350/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 350: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 351/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 351: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 352/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 352: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 353/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 353: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 354/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 354: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 355/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 355: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 356/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 356: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 357/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 357: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 358/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 358: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 359/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 359: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 360/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 360: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 361/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 361: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 362/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 362: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n",
            "Epoch 363/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 363: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 1s 50ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 364/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 364: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 365/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 365: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n",
            "Epoch 366/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 366: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 367/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 367: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 368/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 368: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 369/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 369: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 370/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 370: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 371/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 371: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 372/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 372: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 373/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 373: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 374/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 374: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 375/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 375: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 376/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 376: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 377/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 377: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 378/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 378: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082 - lr: 5.0000e-04\n",
            "Epoch 379/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 379: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075 - lr: 5.0000e-04\n",
            "Epoch 380/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 380: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 381/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 381: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n",
            "Epoch 382/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0079 - mean_absolute_error: 0.0079\n",
            "Epoch 382: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 383/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 383: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077 - lr: 5.0000e-04\n",
            "Epoch 384/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 384: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077 - lr: 5.0000e-04\n",
            "Epoch 385/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0076 - mean_absolute_error: 0.0076\n",
            "Epoch 385: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 386/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 386: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081 - lr: 5.0000e-04\n",
            "Epoch 387/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 387: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 388/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 388: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 389/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 389: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 390/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 390: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 391/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 391: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 392/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 392: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 393/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 393: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 394/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 394: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 395/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0079 - mean_absolute_error: 0.0079\n",
            "Epoch 395: saving model to HPLC_loss_0.0079.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 396/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 396: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 397/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 397: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n",
            "Epoch 398/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0077 - mean_absolute_error: 0.0077\n",
            "Epoch 398: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 399/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 399: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 400/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 400: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Illesszük az adatra a modellt\n",
        "if _MODE_==\"Tanul\":\n",
        "    history=model.fit(\n",
        "            x=df_X_minmax,\n",
        "            y=df_X_minmax, \n",
        "            epochs=400, \n",
        "            batch_size=12,\n",
        "            validation_split=0.1,         \n",
        "            callbacks=[callbacks]          \n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rFyo3DvKCD0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Y3yWoor3Hyt4"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    import matplotlib.pyplot as plt\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4eRDUJximCb",
        "outputId": "9722eb49-35b0-4144-df73-80048e615679"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.22425153851509094,\n",
              "  0.11037516593933105,\n",
              "  0.1067589595913887,\n",
              "  0.10319661349058151,\n",
              "  0.09556851536035538,\n",
              "  0.08377567678689957,\n",
              "  0.07332074642181396,\n",
              "  0.06757060438394547,\n",
              "  0.06365973502397537,\n",
              "  0.06083226203918457,\n",
              "  0.05889350548386574,\n",
              "  0.05424069985747337,\n",
              "  0.05136946961283684,\n",
              "  0.048376258462667465,\n",
              "  0.04458441957831383,\n",
              "  0.04191999137401581,\n",
              "  0.04064277559518814,\n",
              "  0.03859472647309303,\n",
              "  0.03756984323263168,\n",
              "  0.03643212467432022,\n",
              "  0.03668731078505516,\n",
              "  0.03486163541674614,\n",
              "  0.03379962220788002,\n",
              "  0.03420232608914375,\n",
              "  0.03198741748929024,\n",
              "  0.032034650444984436,\n",
              "  0.031842656433582306,\n",
              "  0.030431831255555153,\n",
              "  0.029670892283320427,\n",
              "  0.02939578704535961,\n",
              "  0.029145684093236923,\n",
              "  0.029138438403606415,\n",
              "  0.028768746182322502,\n",
              "  0.02787325344979763,\n",
              "  0.02739728055894375,\n",
              "  0.027801932767033577,\n",
              "  0.027474557980895042,\n",
              "  0.027148079127073288,\n",
              "  0.026828063651919365,\n",
              "  0.025910379365086555,\n",
              "  0.026256145909428596,\n",
              "  0.025221286341547966,\n",
              "  0.024699389934539795,\n",
              "  0.02466515637934208,\n",
              "  0.0248484555631876,\n",
              "  0.025354843586683273,\n",
              "  0.026205388829112053,\n",
              "  0.024292292073369026,\n",
              "  0.023852631449699402,\n",
              "  0.023658400401473045,\n",
              "  0.02345392480492592,\n",
              "  0.02329879440367222,\n",
              "  0.023568840697407722,\n",
              "  0.023947371169924736,\n",
              "  0.024436144158244133,\n",
              "  0.02451697178184986,\n",
              "  0.02348695695400238,\n",
              "  0.023122448474168777,\n",
              "  0.0227354746311903,\n",
              "  0.02330365590751171,\n",
              "  0.022325173020362854,\n",
              "  0.02220245823264122,\n",
              "  0.021615680307149887,\n",
              "  0.021741516888141632,\n",
              "  0.02199123241007328,\n",
              "  0.021997319534420967,\n",
              "  0.02192407287657261,\n",
              "  0.02223455160856247,\n",
              "  0.022150833159685135,\n",
              "  0.021114887669682503,\n",
              "  0.02101542428135872,\n",
              "  0.02115865796804428,\n",
              "  0.021101562306284904,\n",
              "  0.020926853641867638,\n",
              "  0.02133471518754959,\n",
              "  0.020534241572022438,\n",
              "  0.02034645900130272,\n",
              "  0.020101502537727356,\n",
              "  0.020653165876865387,\n",
              "  0.020628945901989937,\n",
              "  0.019903551787137985,\n",
              "  0.02019745111465454,\n",
              "  0.01939898543059826,\n",
              "  0.019584065303206444,\n",
              "  0.01950727589428425,\n",
              "  0.019990894943475723,\n",
              "  0.019570155069231987,\n",
              "  0.01959099806845188,\n",
              "  0.018859077244997025,\n",
              "  0.019549136981368065,\n",
              "  0.018809067085385323,\n",
              "  0.01866120658814907,\n",
              "  0.018694549798965454,\n",
              "  0.018978992477059364,\n",
              "  0.019962649792432785,\n",
              "  0.018963268026709557,\n",
              "  0.019196338951587677,\n",
              "  0.01964881829917431,\n",
              "  0.01833317056298256,\n",
              "  0.017857905477285385,\n",
              "  0.017945529893040657,\n",
              "  0.01755380444228649,\n",
              "  0.017011379823088646,\n",
              "  0.017909115180373192,\n",
              "  0.01853099651634693,\n",
              "  0.01889018900692463,\n",
              "  0.01849895901978016,\n",
              "  0.017978815361857414,\n",
              "  0.0184387918561697,\n",
              "  0.017302196472883224,\n",
              "  0.016812114045023918,\n",
              "  0.017037730664014816,\n",
              "  0.016966158524155617,\n",
              "  0.016452550888061523,\n",
              "  0.016158834099769592,\n",
              "  0.016696348786354065,\n",
              "  0.01633697934448719,\n",
              "  0.016146691516041756,\n",
              "  0.015849558636546135,\n",
              "  0.015877721831202507,\n",
              "  0.015554005280137062,\n",
              "  0.015247920528054237,\n",
              "  0.01602950319647789,\n",
              "  0.014872316271066666,\n",
              "  0.014896663837134838,\n",
              "  0.015372319146990776,\n",
              "  0.015438104048371315,\n",
              "  0.015519797801971436,\n",
              "  0.015129515901207924,\n",
              "  0.015303723514080048,\n",
              "  0.01532371249049902,\n",
              "  0.016423901543021202,\n",
              "  0.015714721754193306,\n",
              "  0.01507455762475729,\n",
              "  0.015675168484449387,\n",
              "  0.014753864146769047,\n",
              "  0.014783954247832298,\n",
              "  0.014452256262302399,\n",
              "  0.014155215583741665,\n",
              "  0.014093014411628246,\n",
              "  0.014169947244226933,\n",
              "  0.013877682387828827,\n",
              "  0.013860553503036499,\n",
              "  0.013512742705643177,\n",
              "  0.013424042612314224,\n",
              "  0.013934764079749584,\n",
              "  0.013893633149564266,\n",
              "  0.014444473199546337,\n",
              "  0.013429569080471992,\n",
              "  0.012989422306418419,\n",
              "  0.013260961510241032,\n",
              "  0.012996885925531387,\n",
              "  0.01317757647484541,\n",
              "  0.012746283784508705,\n",
              "  0.013183321803808212,\n",
              "  0.013003558851778507,\n",
              "  0.012900223024189472,\n",
              "  0.012596765533089638,\n",
              "  0.013016287237405777,\n",
              "  0.013743286952376366,\n",
              "  0.01307645533233881,\n",
              "  0.013571136631071568,\n",
              "  0.01312801893800497,\n",
              "  0.012824607081711292,\n",
              "  0.012639026157557964,\n",
              "  0.012252983637154102,\n",
              "  0.012433333322405815,\n",
              "  0.012784176506102085,\n",
              "  0.01266795489937067,\n",
              "  0.012158479541540146,\n",
              "  0.011813624761998653,\n",
              "  0.012042718939483166,\n",
              "  0.01168213039636612,\n",
              "  0.011731537990272045,\n",
              "  0.012063749134540558,\n",
              "  0.011868542991578579,\n",
              "  0.011928118765354156,\n",
              "  0.01197032444179058,\n",
              "  0.01178507599979639,\n",
              "  0.0118881706148386,\n",
              "  0.01186186820268631,\n",
              "  0.01268948707729578,\n",
              "  0.012375103309750557,\n",
              "  0.012379473075270653,\n",
              "  0.011720794253051281,\n",
              "  0.012083789333701134,\n",
              "  0.011703935451805592,\n",
              "  0.01142200082540512,\n",
              "  0.01169603131711483,\n",
              "  0.012255744077265263,\n",
              "  0.011723221279680729,\n",
              "  0.011442180722951889,\n",
              "  0.01184458564966917,\n",
              "  0.012509668245911598,\n",
              "  0.013112969696521759,\n",
              "  0.012225971557199955,\n",
              "  0.012304659932851791,\n",
              "  0.012174811214208603,\n",
              "  0.011513716541230679,\n",
              "  0.011671149171888828,\n",
              "  0.011430925689637661,\n",
              "  0.01105091068893671,\n",
              "  0.010911564342677593,\n",
              "  0.011632737703621387,\n",
              "  0.011331855319440365,\n",
              "  0.011257396079599857,\n",
              "  0.011629677377641201,\n",
              "  0.01110192108899355,\n",
              "  0.011134433560073376,\n",
              "  0.011083252727985382,\n",
              "  0.011160634458065033,\n",
              "  0.011644585058093071,\n",
              "  0.010938198305666447,\n",
              "  0.011204448528587818,\n",
              "  0.011241943575441837,\n",
              "  0.010866748169064522,\n",
              "  0.01098552718758583,\n",
              "  0.010795758105814457,\n",
              "  0.011323472484946251,\n",
              "  0.011117309331893921,\n",
              "  0.010515458881855011,\n",
              "  0.01076491642743349,\n",
              "  0.011043943464756012,\n",
              "  0.010577783919870853,\n",
              "  0.01066112332046032,\n",
              "  0.010419012047350407,\n",
              "  0.01050795428454876,\n",
              "  0.010811437852680683,\n",
              "  0.011242590844631195,\n",
              "  0.010425593703985214,\n",
              "  0.010640847496688366,\n",
              "  0.010214274749159813,\n",
              "  0.010895817540585995,\n",
              "  0.01095904316753149,\n",
              "  0.01060518529266119,\n",
              "  0.010760007426142693,\n",
              "  0.010760783217847347,\n",
              "  0.011666099540889263,\n",
              "  0.011565586552023888,\n",
              "  0.010555214248597622,\n",
              "  0.009918466210365295,\n",
              "  0.010508507490158081,\n",
              "  0.01001028437167406,\n",
              "  0.010270452126860619,\n",
              "  0.010640439577400684,\n",
              "  0.010042649693787098,\n",
              "  0.009369973093271255,\n",
              "  0.009842642582952976,\n",
              "  0.010868286713957787,\n",
              "  0.010562391020357609,\n",
              "  0.010634125210344791,\n",
              "  0.010432744398713112,\n",
              "  0.010173092596232891,\n",
              "  0.009883303195238113,\n",
              "  0.010014849714934826,\n",
              "  0.010217471979558468,\n",
              "  0.010044600814580917,\n",
              "  0.00985900592058897,\n",
              "  0.009705753065645695,\n",
              "  0.010058780200779438,\n",
              "  0.009906367398798466,\n",
              "  0.010449478402733803,\n",
              "  0.01057745423167944,\n",
              "  0.01065077818930149,\n",
              "  0.010241197422146797,\n",
              "  0.010058172978460789,\n",
              "  0.010003971867263317,\n",
              "  0.010567951947450638,\n",
              "  0.009787303395569324,\n",
              "  0.009523875080049038,\n",
              "  0.00983472540974617,\n",
              "  0.00954030267894268,\n",
              "  0.009595233015716076,\n",
              "  0.009714233689010143,\n",
              "  0.00953720137476921,\n",
              "  0.009318020194768906,\n",
              "  0.009376013651490211,\n",
              "  0.009876673109829426,\n",
              "  0.009590156376361847,\n",
              "  0.009790870361030102,\n",
              "  0.009926912374794483,\n",
              "  0.00972073059529066,\n",
              "  0.009158766828477383,\n",
              "  0.009452875703573227,\n",
              "  0.009917804971337318,\n",
              "  0.009967035613954067,\n",
              "  0.010265089571475983,\n",
              "  0.009575285017490387,\n",
              "  0.009833872318267822,\n",
              "  0.009283861145377159,\n",
              "  0.009779202751815319,\n",
              "  0.00963461771607399,\n",
              "  0.009311259724199772,\n",
              "  0.009511744603514671,\n",
              "  0.009016338735818863,\n",
              "  0.009356111288070679,\n",
              "  0.009558720514178276,\n",
              "  0.009927046485245228,\n",
              "  0.009587285108864307,\n",
              "  0.009783392772078514,\n",
              "  0.010195276699960232,\n",
              "  0.009661718271672726,\n",
              "  0.009343174286186695,\n",
              "  0.009247848764061928,\n",
              "  0.00909310020506382,\n",
              "  0.00952070951461792,\n",
              "  0.009235518984496593,\n",
              "  0.009263564832508564,\n",
              "  0.009282437153160572,\n",
              "  0.01003415696322918,\n",
              "  0.009849785827100277,\n",
              "  0.009008646011352539,\n",
              "  0.009234429337084293,\n",
              "  0.009379914961755276,\n",
              "  0.009559265337884426,\n",
              "  0.008975723758339882,\n",
              "  0.009931135922670364,\n",
              "  0.011081241071224213,\n",
              "  0.009620280005037785,\n",
              "  0.009410345926880836,\n",
              "  0.009131286293268204,\n",
              "  0.008883340284228325,\n",
              "  0.00947430171072483,\n",
              "  0.00883525051176548,\n",
              "  0.008720261044800282,\n",
              "  0.009511233307421207,\n",
              "  0.009037595242261887,\n",
              "  0.008805899880826473,\n",
              "  0.008951714262366295,\n",
              "  0.008694072254002094,\n",
              "  0.008747472427785397,\n",
              "  0.008415067568421364,\n",
              "  0.008648566901683807,\n",
              "  0.0088427709415555,\n",
              "  0.008748343214392662,\n",
              "  0.009048806503415108,\n",
              "  0.009298134595155716,\n",
              "  0.009005846455693245,\n",
              "  0.008755411952733994,\n",
              "  0.008762277662754059,\n",
              "  0.008573516272008419,\n",
              "  0.008107242174446583,\n",
              "  0.009022415615618229,\n",
              "  0.008775318041443825,\n",
              "  0.009033356793224812,\n",
              "  0.0090575460344553,\n",
              "  0.008685903623700142,\n",
              "  0.008546873927116394,\n",
              "  0.00811852514743805,\n",
              "  0.008290930651128292,\n",
              "  0.008943915367126465,\n",
              "  0.00857394840568304,\n",
              "  0.008667883463203907,\n",
              "  0.008885610848665237,\n",
              "  0.008437766693532467,\n",
              "  0.008778444491326809,\n",
              "  0.00864750612527132,\n",
              "  0.008394223637878895,\n",
              "  0.008426482789218426,\n",
              "  0.008426911197602749,\n",
              "  0.008539289236068726,\n",
              "  0.00888703390955925,\n",
              "  0.008883732371032238,\n",
              "  0.01053596381098032,\n",
              "  0.008804856799542904,\n",
              "  0.008822774514555931,\n",
              "  0.008535982109606266,\n",
              "  0.008115194737911224,\n",
              "  0.00836858432739973,\n",
              "  0.008930775336921215,\n",
              "  0.00951798539608717,\n",
              "  0.008620326407253742,\n",
              "  0.008530199527740479,\n",
              "  0.008503523655235767,\n",
              "  0.008195254020392895,\n",
              "  0.00798933207988739,\n",
              "  0.008567319251596928,\n",
              "  0.008350899443030357,\n",
              "  0.00804228987544775,\n",
              "  0.008289361372590065,\n",
              "  0.008481957018375397,\n",
              "  0.008083715103566647,\n",
              "  0.008764669299125671,\n",
              "  0.008255202323198318,\n",
              "  0.00807034783065319,\n",
              "  0.00913693755865097,\n",
              "  0.009435968473553658,\n",
              "  0.008784182369709015,\n",
              "  0.008281093090772629,\n",
              "  0.008313212543725967,\n",
              "  0.00814723689109087,\n",
              "  0.008192791603505611,\n",
              "  0.00800352729856968,\n",
              "  0.007993273437023163,\n",
              "  0.007871945388615131,\n",
              "  0.008240018971264362,\n",
              "  0.008018918335437775,\n",
              "  0.008027580566704273,\n",
              "  0.008421345613896847,\n",
              "  0.008198915049433708],\n",
              " 'lr': [0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005],\n",
              " 'mean_absolute_error': [0.22425153851509094,\n",
              "  0.11037516593933105,\n",
              "  0.1067589595913887,\n",
              "  0.10319661349058151,\n",
              "  0.09556851536035538,\n",
              "  0.08377567678689957,\n",
              "  0.07332074642181396,\n",
              "  0.06757060438394547,\n",
              "  0.06365973502397537,\n",
              "  0.06083226203918457,\n",
              "  0.05889350548386574,\n",
              "  0.05424069985747337,\n",
              "  0.05136946961283684,\n",
              "  0.048376258462667465,\n",
              "  0.04458441957831383,\n",
              "  0.04191999137401581,\n",
              "  0.04064277559518814,\n",
              "  0.03859472647309303,\n",
              "  0.03756984323263168,\n",
              "  0.03643212467432022,\n",
              "  0.03668731078505516,\n",
              "  0.03486163541674614,\n",
              "  0.03379962220788002,\n",
              "  0.03420232608914375,\n",
              "  0.03198741748929024,\n",
              "  0.032034650444984436,\n",
              "  0.031842656433582306,\n",
              "  0.030431831255555153,\n",
              "  0.029670892283320427,\n",
              "  0.02939578704535961,\n",
              "  0.029145684093236923,\n",
              "  0.029138434678316116,\n",
              "  0.028768746182322502,\n",
              "  0.02787325717508793,\n",
              "  0.027397282421588898,\n",
              "  0.027801932767033577,\n",
              "  0.027474557980895042,\n",
              "  0.027148080989718437,\n",
              "  0.026828063651919365,\n",
              "  0.025910377502441406,\n",
              "  0.026256145909428596,\n",
              "  0.025221286341547966,\n",
              "  0.024699389934539795,\n",
              "  0.02466515637934208,\n",
              "  0.0248484555631876,\n",
              "  0.025354843586683273,\n",
              "  0.026205388829112053,\n",
              "  0.024292292073369026,\n",
              "  0.023852631449699402,\n",
              "  0.023658400401473045,\n",
              "  0.02345392480492592,\n",
              "  0.02329879440367222,\n",
              "  0.023568840697407722,\n",
              "  0.023947371169924736,\n",
              "  0.024436144158244133,\n",
              "  0.02451697178184986,\n",
              "  0.02348695695400238,\n",
              "  0.023122448474168777,\n",
              "  0.0227354746311903,\n",
              "  0.02330365590751171,\n",
              "  0.022325173020362854,\n",
              "  0.02220245823264122,\n",
              "  0.021615680307149887,\n",
              "  0.021741516888141632,\n",
              "  0.02199123241007328,\n",
              "  0.021997319534420967,\n",
              "  0.02192407287657261,\n",
              "  0.02223455160856247,\n",
              "  0.022150833159685135,\n",
              "  0.021114887669682503,\n",
              "  0.02101542428135872,\n",
              "  0.02115865796804428,\n",
              "  0.021101558580994606,\n",
              "  0.020926853641867638,\n",
              "  0.02133471518754959,\n",
              "  0.020534241572022438,\n",
              "  0.02034645900130272,\n",
              "  0.020101502537727356,\n",
              "  0.020653165876865387,\n",
              "  0.020628945901989937,\n",
              "  0.019903551787137985,\n",
              "  0.02019745111465454,\n",
              "  0.01939898543059826,\n",
              "  0.019584065303206444,\n",
              "  0.01950727589428425,\n",
              "  0.019990894943475723,\n",
              "  0.019570155069231987,\n",
              "  0.01959099806845188,\n",
              "  0.018859077244997025,\n",
              "  0.019549136981368065,\n",
              "  0.018809067085385323,\n",
              "  0.01866120658814907,\n",
              "  0.018694549798965454,\n",
              "  0.018978992477059364,\n",
              "  0.019962649792432785,\n",
              "  0.018963266164064407,\n",
              "  0.019196338951587677,\n",
              "  0.01964881829917431,\n",
              "  0.01833317056298256,\n",
              "  0.017857905477285385,\n",
              "  0.017945529893040657,\n",
              "  0.017553800716996193,\n",
              "  0.017011379823088646,\n",
              "  0.017909115180373192,\n",
              "  0.01853099651634693,\n",
              "  0.01889018900692463,\n",
              "  0.01849895901978016,\n",
              "  0.017978815361857414,\n",
              "  0.0184387918561697,\n",
              "  0.017302196472883224,\n",
              "  0.016812114045023918,\n",
              "  0.017037730664014816,\n",
              "  0.016966158524155617,\n",
              "  0.016452550888061523,\n",
              "  0.016158834099769592,\n",
              "  0.016696348786354065,\n",
              "  0.01633697934448719,\n",
              "  0.016146691516041756,\n",
              "  0.015849558636546135,\n",
              "  0.015877721831202507,\n",
              "  0.015554005280137062,\n",
              "  0.015247920528054237,\n",
              "  0.01602950319647789,\n",
              "  0.014872316271066666,\n",
              "  0.014896663837134838,\n",
              "  0.015372319146990776,\n",
              "  0.015438104048371315,\n",
              "  0.015519797801971436,\n",
              "  0.015129515901207924,\n",
              "  0.015303723514080048,\n",
              "  0.015323711559176445,\n",
              "  0.016423901543021202,\n",
              "  0.015714721754193306,\n",
              "  0.01507455762475729,\n",
              "  0.015675168484449387,\n",
              "  0.014753864146769047,\n",
              "  0.014783954247832298,\n",
              "  0.014452256262302399,\n",
              "  0.014155215583741665,\n",
              "  0.014093014411628246,\n",
              "  0.014169947244226933,\n",
              "  0.013877682387828827,\n",
              "  0.013860553503036499,\n",
              "  0.013512742705643177,\n",
              "  0.013424042612314224,\n",
              "  0.013934764079749584,\n",
              "  0.013893633149564266,\n",
              "  0.014444473199546337,\n",
              "  0.013429569080471992,\n",
              "  0.012989422306418419,\n",
              "  0.013260961510241032,\n",
              "  0.012996885925531387,\n",
              "  0.01317757647484541,\n",
              "  0.012746283784508705,\n",
              "  0.013183321803808212,\n",
              "  0.013003558851778507,\n",
              "  0.012900223024189472,\n",
              "  0.012596765533089638,\n",
              "  0.013016287237405777,\n",
              "  0.013743286952376366,\n",
              "  0.01307645533233881,\n",
              "  0.013571136631071568,\n",
              "  0.01312801893800497,\n",
              "  0.012824607081711292,\n",
              "  0.012639026157557964,\n",
              "  0.012252983637154102,\n",
              "  0.012433333322405815,\n",
              "  0.012784176506102085,\n",
              "  0.01266795489937067,\n",
              "  0.012158479541540146,\n",
              "  0.011813624761998653,\n",
              "  0.012042718939483166,\n",
              "  0.01168213039636612,\n",
              "  0.011731537990272045,\n",
              "  0.012063749134540558,\n",
              "  0.011868542991578579,\n",
              "  0.011928118765354156,\n",
              "  0.01197032444179058,\n",
              "  0.01178507599979639,\n",
              "  0.0118881706148386,\n",
              "  0.01186186820268631,\n",
              "  0.01268948707729578,\n",
              "  0.012375103309750557,\n",
              "  0.012379473075270653,\n",
              "  0.011720794253051281,\n",
              "  0.012083789333701134,\n",
              "  0.011703935451805592,\n",
              "  0.01142200082540512,\n",
              "  0.01169603131711483,\n",
              "  0.012255744077265263,\n",
              "  0.011723221279680729,\n",
              "  0.011442180722951889,\n",
              "  0.01184458564966917,\n",
              "  0.012509668245911598,\n",
              "  0.013112969696521759,\n",
              "  0.012225971557199955,\n",
              "  0.012304659932851791,\n",
              "  0.012174811214208603,\n",
              "  0.011513716541230679,\n",
              "  0.011671149171888828,\n",
              "  0.011430925689637661,\n",
              "  0.01105091068893671,\n",
              "  0.010911564342677593,\n",
              "  0.011632737703621387,\n",
              "  0.011331855319440365,\n",
              "  0.011257396079599857,\n",
              "  0.011629677377641201,\n",
              "  0.01110192108899355,\n",
              "  0.011134433560073376,\n",
              "  0.011083252727985382,\n",
              "  0.011160634458065033,\n",
              "  0.011644585058093071,\n",
              "  0.010938198305666447,\n",
              "  0.011204448528587818,\n",
              "  0.011241943575441837,\n",
              "  0.010866748169064522,\n",
              "  0.01098552718758583,\n",
              "  0.010795758105814457,\n",
              "  0.011323472484946251,\n",
              "  0.011117309331893921,\n",
              "  0.010515458881855011,\n",
              "  0.01076491642743349,\n",
              "  0.011043943464756012,\n",
              "  0.010577783919870853,\n",
              "  0.01066112332046032,\n",
              "  0.010419012047350407,\n",
              "  0.01050795428454876,\n",
              "  0.010811437852680683,\n",
              "  0.011242590844631195,\n",
              "  0.010425593703985214,\n",
              "  0.010640847496688366,\n",
              "  0.010214274749159813,\n",
              "  0.010895817540585995,\n",
              "  0.01095904316753149,\n",
              "  0.01060518529266119,\n",
              "  0.010760007426142693,\n",
              "  0.010760783217847347,\n",
              "  0.011666099540889263,\n",
              "  0.011565586552023888,\n",
              "  0.010555214248597622,\n",
              "  0.009918466210365295,\n",
              "  0.010508507490158081,\n",
              "  0.01001028437167406,\n",
              "  0.010270452126860619,\n",
              "  0.010640439577400684,\n",
              "  0.010042649693787098,\n",
              "  0.009369973093271255,\n",
              "  0.009842642582952976,\n",
              "  0.010868286713957787,\n",
              "  0.010562391020357609,\n",
              "  0.010634125210344791,\n",
              "  0.010432743467390537,\n",
              "  0.010173091664910316,\n",
              "  0.009883303195238113,\n",
              "  0.010014849714934826,\n",
              "  0.010217471979558468,\n",
              "  0.010044600814580917,\n",
              "  0.00985900592058897,\n",
              "  0.009705753065645695,\n",
              "  0.010058779269456863,\n",
              "  0.009906367398798466,\n",
              "  0.010449477471411228,\n",
              "  0.01057745423167944,\n",
              "  0.01065077818930149,\n",
              "  0.010241197422146797,\n",
              "  0.010058172978460789,\n",
              "  0.010003971867263317,\n",
              "  0.010567951947450638,\n",
              "  0.009787303395569324,\n",
              "  0.009523876011371613,\n",
              "  0.00983472540974617,\n",
              "  0.00954030267894268,\n",
              "  0.009595233015716076,\n",
              "  0.009714233689010143,\n",
              "  0.00953720137476921,\n",
              "  0.009318020194768906,\n",
              "  0.009376013651490211,\n",
              "  0.009876673109829426,\n",
              "  0.009590156376361847,\n",
              "  0.009790870361030102,\n",
              "  0.009926912374794483,\n",
              "  0.00972073059529066,\n",
              "  0.009158766828477383,\n",
              "  0.009452875703573227,\n",
              "  0.009917804971337318,\n",
              "  0.009967035613954067,\n",
              "  0.010265089571475983,\n",
              "  0.009575285017490387,\n",
              "  0.009833872318267822,\n",
              "  0.009283861145377159,\n",
              "  0.009779202751815319,\n",
              "  0.00963461771607399,\n",
              "  0.009311259724199772,\n",
              "  0.009511744603514671,\n",
              "  0.009016338735818863,\n",
              "  0.009356111288070679,\n",
              "  0.009558720514178276,\n",
              "  0.009927046485245228,\n",
              "  0.009587285108864307,\n",
              "  0.009783392772078514,\n",
              "  0.010195276699960232,\n",
              "  0.009661718271672726,\n",
              "  0.009343174286186695,\n",
              "  0.009247848764061928,\n",
              "  0.00909310020506382,\n",
              "  0.00952070951461792,\n",
              "  0.009235518984496593,\n",
              "  0.009263564832508564,\n",
              "  0.009282437153160572,\n",
              "  0.01003415696322918,\n",
              "  0.009849785827100277,\n",
              "  0.009008646942675114,\n",
              "  0.009234429337084293,\n",
              "  0.009379914961755276,\n",
              "  0.009559265337884426,\n",
              "  0.008975723758339882,\n",
              "  0.009931135922670364,\n",
              "  0.011081241071224213,\n",
              "  0.009620280005037785,\n",
              "  0.009410345926880836,\n",
              "  0.009131286293268204,\n",
              "  0.008883340284228325,\n",
              "  0.00947430171072483,\n",
              "  0.00883525051176548,\n",
              "  0.008720261044800282,\n",
              "  0.009511233307421207,\n",
              "  0.009037595242261887,\n",
              "  0.008805899880826473,\n",
              "  0.008951714262366295,\n",
              "  0.008694072254002094,\n",
              "  0.008747472427785397,\n",
              "  0.008415067568421364,\n",
              "  0.008648566901683807,\n",
              "  0.0088427709415555,\n",
              "  0.008748343214392662,\n",
              "  0.009048806503415108,\n",
              "  0.009298134595155716,\n",
              "  0.009005846455693245,\n",
              "  0.008755412884056568,\n",
              "  0.008762277662754059,\n",
              "  0.008573516272008419,\n",
              "  0.008107242174446583,\n",
              "  0.009022415615618229,\n",
              "  0.008775318041443825,\n",
              "  0.009033356793224812,\n",
              "  0.00905754417181015,\n",
              "  0.008685903623700142,\n",
              "  0.008546873927116394,\n",
              "  0.00811852514743805,\n",
              "  0.008290929719805717,\n",
              "  0.008943915367126465,\n",
              "  0.00857394840568304,\n",
              "  0.008667882531881332,\n",
              "  0.008885610848665237,\n",
              "  0.008437766693532467,\n",
              "  0.008778444491326809,\n",
              "  0.00864750612527132,\n",
              "  0.00839422270655632,\n",
              "  0.008426484651863575,\n",
              "  0.008426911197602749,\n",
              "  0.008539289236068726,\n",
              "  0.00888703390955925,\n",
              "  0.008883732371032238,\n",
              "  0.01053596381098032,\n",
              "  0.008804856799542904,\n",
              "  0.008822774514555931,\n",
              "  0.008535982109606266,\n",
              "  0.008115194737911224,\n",
              "  0.00836858432739973,\n",
              "  0.008930775336921215,\n",
              "  0.00951798539608717,\n",
              "  0.008620326407253742,\n",
              "  0.008530199527740479,\n",
              "  0.008503523655235767,\n",
              "  0.008195254020392895,\n",
              "  0.00798933207988739,\n",
              "  0.008567319251596928,\n",
              "  0.008350899443030357,\n",
              "  0.00804228987544775,\n",
              "  0.008289361372590065,\n",
              "  0.008481957018375397,\n",
              "  0.008083715103566647,\n",
              "  0.008764669299125671,\n",
              "  0.008255202323198318,\n",
              "  0.00807034783065319,\n",
              "  0.00913693755865097,\n",
              "  0.009435968473553658,\n",
              "  0.008784182369709015,\n",
              "  0.008281093090772629,\n",
              "  0.008313212543725967,\n",
              "  0.00814723689109087,\n",
              "  0.008192791603505611,\n",
              "  0.00800352729856968,\n",
              "  0.007993273437023163,\n",
              "  0.007871945388615131,\n",
              "  0.008240018971264362,\n",
              "  0.008018918335437775,\n",
              "  0.008027580566704273,\n",
              "  0.008421345613896847,\n",
              "  0.008198915049433708],\n",
              " 'val_loss': [0.10916096717119217,\n",
              "  0.09725876897573471,\n",
              "  0.09775549173355103,\n",
              "  0.09268775582313538,\n",
              "  0.0864608958363533,\n",
              "  0.0833868756890297,\n",
              "  0.07814236730337143,\n",
              "  0.07551151514053345,\n",
              "  0.07353466004133224,\n",
              "  0.06115015223622322,\n",
              "  0.06002417206764221,\n",
              "  0.057430777698755264,\n",
              "  0.053508974611759186,\n",
              "  0.04303254559636116,\n",
              "  0.04733540490269661,\n",
              "  0.03572798892855644,\n",
              "  0.0331697054207325,\n",
              "  0.03470978885889053,\n",
              "  0.032357316464185715,\n",
              "  0.04035990685224533,\n",
              "  0.03231307119131088,\n",
              "  0.03424827754497528,\n",
              "  0.03066101297736168,\n",
              "  0.03180523216724396,\n",
              "  0.025787044316530228,\n",
              "  0.030826997011899948,\n",
              "  0.024856334552168846,\n",
              "  0.02398422174155712,\n",
              "  0.02414293773472309,\n",
              "  0.02324756607413292,\n",
              "  0.027176888659596443,\n",
              "  0.02601531520485878,\n",
              "  0.023763535544276237,\n",
              "  0.024326447397470474,\n",
              "  0.02451302297413349,\n",
              "  0.024998236447572708,\n",
              "  0.022637179121375084,\n",
              "  0.023679327219724655,\n",
              "  0.021797453984618187,\n",
              "  0.0216493159532547,\n",
              "  0.02065511792898178,\n",
              "  0.020769765600562096,\n",
              "  0.021752726286649704,\n",
              "  0.020986713469028473,\n",
              "  0.021884290501475334,\n",
              "  0.02495473064482212,\n",
              "  0.023809297010302544,\n",
              "  0.020356208086013794,\n",
              "  0.019684145227074623,\n",
              "  0.018863793462514877,\n",
              "  0.019545510411262512,\n",
              "  0.019389761611819267,\n",
              "  0.021235818043351173,\n",
              "  0.018972251564264297,\n",
              "  0.019039437174797058,\n",
              "  0.027494285255670547,\n",
              "  0.02009475789964199,\n",
              "  0.019433435052633286,\n",
              "  0.019959276542067528,\n",
              "  0.019485758617520332,\n",
              "  0.018218019977211952,\n",
              "  0.017109107226133347,\n",
              "  0.01809612661600113,\n",
              "  0.018134020268917084,\n",
              "  0.017572686076164246,\n",
              "  0.019601503387093544,\n",
              "  0.0214804969727993,\n",
              "  0.02163024991750717,\n",
              "  0.017776062712073326,\n",
              "  0.0165069792419672,\n",
              "  0.01665165089070797,\n",
              "  0.017297616228461266,\n",
              "  0.01696711964905262,\n",
              "  0.01796276867389679,\n",
              "  0.017610307782888412,\n",
              "  0.01714305952191353,\n",
              "  0.016038350760936737,\n",
              "  0.017293013632297516,\n",
              "  0.018977969884872437,\n",
              "  0.01708281971514225,\n",
              "  0.01521348673850298,\n",
              "  0.01763404719531536,\n",
              "  0.016975050792098045,\n",
              "  0.015868475660681725,\n",
              "  0.017851781100034714,\n",
              "  0.019414911046624184,\n",
              "  0.014619662426412106,\n",
              "  0.01589006930589676,\n",
              "  0.01756230555474758,\n",
              "  0.01595226116478443,\n",
              "  0.015099454671144485,\n",
              "  0.015177723951637745,\n",
              "  0.014973674900829792,\n",
              "  0.0162226352840662,\n",
              "  0.015868857502937317,\n",
              "  0.016429563984274864,\n",
              "  0.01647551916539669,\n",
              "  0.01745452545583248,\n",
              "  0.015066866762936115,\n",
              "  0.016349582001566887,\n",
              "  0.015373988077044487,\n",
              "  0.01525211613625288,\n",
              "  0.015944913029670715,\n",
              "  0.02056758850812912,\n",
              "  0.01591419242322445,\n",
              "  0.016757911071181297,\n",
              "  0.01342854741960764,\n",
              "  0.014548344537615776,\n",
              "  0.013638987205922604,\n",
              "  0.014491870068013668,\n",
              "  0.013889491558074951,\n",
              "  0.015009541995823383,\n",
              "  0.013703803531825542,\n",
              "  0.013506338931620121,\n",
              "  0.014333384111523628,\n",
              "  0.014018581248819828,\n",
              "  0.014948512427508831,\n",
              "  0.01473443116992712,\n",
              "  0.013232974335551262,\n",
              "  0.013274981640279293,\n",
              "  0.013218618929386139,\n",
              "  0.01426562201231718,\n",
              "  0.013462158851325512,\n",
              "  0.013106070458889008,\n",
              "  0.012662438675761223,\n",
              "  0.013228568248450756,\n",
              "  0.01510500069707632,\n",
              "  0.013711674138903618,\n",
              "  0.013996900990605354,\n",
              "  0.013941401615738869,\n",
              "  0.020202402025461197,\n",
              "  0.019995853304862976,\n",
              "  0.015171007253229618,\n",
              "  0.016834869980812073,\n",
              "  0.015924233943223953,\n",
              "  0.01655687391757965,\n",
              "  0.015425167977809906,\n",
              "  0.014564803801476955,\n",
              "  0.012542418204247952,\n",
              "  0.013609115965664387,\n",
              "  0.012531550601124763,\n",
              "  0.01260561216622591,\n",
              "  0.013261502608656883,\n",
              "  0.012129410170018673,\n",
              "  0.015238399617373943,\n",
              "  0.018526719883084297,\n",
              "  0.015580594539642334,\n",
              "  0.012346494942903519,\n",
              "  0.012971293181180954,\n",
              "  0.013157224282622337,\n",
              "  0.01234220340847969,\n",
              "  0.012718940153717995,\n",
              "  0.011910801753401756,\n",
              "  0.014502898789942265,\n",
              "  0.012790677137672901,\n",
              "  0.013816486112773418,\n",
              "  0.011910608969628811,\n",
              "  0.013125019147992134,\n",
              "  0.012754857540130615,\n",
              "  0.012624082155525684,\n",
              "  0.017359763383865356,\n",
              "  0.013006286695599556,\n",
              "  0.013094796799123287,\n",
              "  0.01174608152359724,\n",
              "  0.011443940922617912,\n",
              "  0.013419156894087791,\n",
              "  0.012277086265385151,\n",
              "  0.011796501465141773,\n",
              "  0.012294964864850044,\n",
              "  0.012815686874091625,\n",
              "  0.01327821146696806,\n",
              "  0.011661874130368233,\n",
              "  0.010555113665759563,\n",
              "  0.011475213803350925,\n",
              "  0.011830349452793598,\n",
              "  0.011198288761079311,\n",
              "  0.012079832144081593,\n",
              "  0.012045921757817268,\n",
              "  0.011160070076584816,\n",
              "  0.012000929564237595,\n",
              "  0.011853114701807499,\n",
              "  0.012688301503658295,\n",
              "  0.012479908764362335,\n",
              "  0.01150329876691103,\n",
              "  0.011675608344376087,\n",
              "  0.011417744681239128,\n",
              "  0.01129714585840702,\n",
              "  0.011765910312533379,\n",
              "  0.010383657179772854,\n",
              "  0.011698399670422077,\n",
              "  0.011580240912735462,\n",
              "  0.01062744576483965,\n",
              "  0.012067685835063457,\n",
              "  0.012640519998967648,\n",
              "  0.01640249416232109,\n",
              "  0.012087677605450153,\n",
              "  0.02012345753610134,\n",
              "  0.013826580718159676,\n",
              "  0.010752154514193535,\n",
              "  0.011280183680355549,\n",
              "  0.010337291285395622,\n",
              "  0.010748242028057575,\n",
              "  0.010617196559906006,\n",
              "  0.010633496567606926,\n",
              "  0.011071284301578999,\n",
              "  0.010303536430001259,\n",
              "  0.0130853820592165,\n",
              "  0.011033228598535061,\n",
              "  0.01016911119222641,\n",
              "  0.01122465543448925,\n",
              "  0.014159739017486572,\n",
              "  0.010769300162792206,\n",
              "  0.010856134817004204,\n",
              "  0.011509357020258904,\n",
              "  0.011406159028410912,\n",
              "  0.011099758557975292,\n",
              "  0.011345084756612778,\n",
              "  0.011363185942173004,\n",
              "  0.010458337143063545,\n",
              "  0.01014967355877161,\n",
              "  0.011752738617360592,\n",
              "  0.011294062249362469,\n",
              "  0.011892227455973625,\n",
              "  0.010307552292943,\n",
              "  0.011121292598545551,\n",
              "  0.010585627518594265,\n",
              "  0.009996703825891018,\n",
              "  0.01136547327041626,\n",
              "  0.01197844184935093,\n",
              "  0.009913651272654533,\n",
              "  0.01072530634701252,\n",
              "  0.010536166839301586,\n",
              "  0.010022659786045551,\n",
              "  0.01043944526463747,\n",
              "  0.012284436263144016,\n",
              "  0.010706435889005661,\n",
              "  0.01590840518474579,\n",
              "  0.013933610171079636,\n",
              "  0.012149142101407051,\n",
              "  0.010331510566174984,\n",
              "  0.010163730010390282,\n",
              "  0.010625557973980904,\n",
              "  0.010062181390821934,\n",
              "  0.01091372687369585,\n",
              "  0.010028920136392117,\n",
              "  0.009917467832565308,\n",
              "  0.00969666987657547,\n",
              "  0.009986603632569313,\n",
              "  0.009911228902637959,\n",
              "  0.010368130169808865,\n",
              "  0.009937671013176441,\n",
              "  0.009736611507833004,\n",
              "  0.009992758743464947,\n",
              "  0.009619949385523796,\n",
              "  0.01209293957799673,\n",
              "  0.010051424615085125,\n",
              "  0.009690733626484871,\n",
              "  0.010384880006313324,\n",
              "  0.010385323315858841,\n",
              "  0.009684891439974308,\n",
              "  0.010385924950242043,\n",
              "  0.011245946399867535,\n",
              "  0.011429421603679657,\n",
              "  0.009932726621627808,\n",
              "  0.011277441866695881,\n",
              "  0.010427804663777351,\n",
              "  0.009998701512813568,\n",
              "  0.010168333537876606,\n",
              "  0.010021512396633625,\n",
              "  0.009862287901341915,\n",
              "  0.009347720071673393,\n",
              "  0.00992739386856556,\n",
              "  0.009629135951399803,\n",
              "  0.009416542015969753,\n",
              "  0.009308965876698494,\n",
              "  0.008934704586863518,\n",
              "  0.010222728364169598,\n",
              "  0.009035374037921429,\n",
              "  0.009835727512836456,\n",
              "  0.010719593614339828,\n",
              "  0.009299132041633129,\n",
              "  0.010119487531483173,\n",
              "  0.009429964236915112,\n",
              "  0.010958867147564888,\n",
              "  0.011646215803921223,\n",
              "  0.010846826247870922,\n",
              "  0.010238848626613617,\n",
              "  0.00996784120798111,\n",
              "  0.009480123408138752,\n",
              "  0.00908846128731966,\n",
              "  0.011168120428919792,\n",
              "  0.00969899632036686,\n",
              "  0.009840264916419983,\n",
              "  0.010006322525441647,\n",
              "  0.009184002876281738,\n",
              "  0.009861554019153118,\n",
              "  0.00962017010897398,\n",
              "  0.009471416473388672,\n",
              "  0.01024188008159399,\n",
              "  0.011410115286707878,\n",
              "  0.009178023785352707,\n",
              "  0.009984694421291351,\n",
              "  0.011243847198784351,\n",
              "  0.00958176888525486,\n",
              "  0.009193823672831059,\n",
              "  0.010325183160603046,\n",
              "  0.009302975609898567,\n",
              "  0.008686738088726997,\n",
              "  0.010833810083568096,\n",
              "  0.011398566886782646,\n",
              "  0.009561861865222454,\n",
              "  0.010052783414721489,\n",
              "  0.009724871255457401,\n",
              "  0.009935393929481506,\n",
              "  0.00873543880879879,\n",
              "  0.00971895270049572,\n",
              "  0.010477420873939991,\n",
              "  0.009822512045502663,\n",
              "  0.009006815031170845,\n",
              "  0.009428436867892742,\n",
              "  0.008644011802971363,\n",
              "  0.008947418071329594,\n",
              "  0.009472979232668877,\n",
              "  0.008165281265974045,\n",
              "  0.009239446371793747,\n",
              "  0.009108909405767918,\n",
              "  0.008189204148948193,\n",
              "  0.008646092377603054,\n",
              "  0.009916975162923336,\n",
              "  0.008515829220414162,\n",
              "  0.009332885034382343,\n",
              "  0.007967337034642696,\n",
              "  0.009631647728383541,\n",
              "  0.0088496720418334,\n",
              "  0.011526189744472504,\n",
              "  0.011155226267874241,\n",
              "  0.010000603273510933,\n",
              "  0.008780574426054955,\n",
              "  0.010220871306955814,\n",
              "  0.008680133149027824,\n",
              "  0.00924625527113676,\n",
              "  0.008416548371315002,\n",
              "  0.00925862044095993,\n",
              "  0.008973626419901848,\n",
              "  0.010067514143884182,\n",
              "  0.009069315157830715,\n",
              "  0.00871147122234106,\n",
              "  0.009664860554039478,\n",
              "  0.0092720165848732,\n",
              "  0.009756444953382015,\n",
              "  0.008582724258303642,\n",
              "  0.009647401049733162,\n",
              "  0.008775259368121624,\n",
              "  0.010247255675494671,\n",
              "  0.008809613063931465,\n",
              "  0.008505179546773434,\n",
              "  0.008881465531885624,\n",
              "  0.009922303259372711,\n",
              "  0.008837352506816387,\n",
              "  0.009714498184621334,\n",
              "  0.0084867337718606,\n",
              "  0.00828145258128643,\n",
              "  0.012533650733530521,\n",
              "  0.009988357312977314,\n",
              "  0.008342335931956768,\n",
              "  0.010234780609607697,\n",
              "  0.008998697623610497,\n",
              "  0.009418781846761703,\n",
              "  0.008947665803134441,\n",
              "  0.008677572011947632,\n",
              "  0.008648614399135113,\n",
              "  0.008357950486242771,\n",
              "  0.008561909198760986,\n",
              "  0.008469630964100361,\n",
              "  0.00853407196700573,\n",
              "  0.008750097826123238,\n",
              "  0.009024374186992645,\n",
              "  0.008175134658813477,\n",
              "  0.007528840098530054,\n",
              "  0.008893297985196114,\n",
              "  0.008296564221382141,\n",
              "  0.00913920532912016,\n",
              "  0.007670230697840452,\n",
              "  0.007673677988350391,\n",
              "  0.00887508224695921,\n",
              "  0.008115457370877266,\n",
              "  0.010466599836945534,\n",
              "  0.00951508991420269,\n",
              "  0.008709256537258625,\n",
              "  0.008959862403571606,\n",
              "  0.00859109964221716,\n",
              "  0.008398795500397682,\n",
              "  0.00856472086161375,\n",
              "  0.008913545869290829,\n",
              "  0.009100408293306828,\n",
              "  0.008819552138447762,\n",
              "  0.008278148248791695,\n",
              "  0.00872466154396534,\n",
              "  0.008691729977726936,\n",
              "  0.008265912532806396],\n",
              " 'val_mean_absolute_error': [0.10916096717119217,\n",
              "  0.09725876897573471,\n",
              "  0.09775549173355103,\n",
              "  0.09268775582313538,\n",
              "  0.0864609032869339,\n",
              "  0.0833868756890297,\n",
              "  0.07814236730337143,\n",
              "  0.07551151514053345,\n",
              "  0.07353466004133224,\n",
              "  0.06115015223622322,\n",
              "  0.06002417206764221,\n",
              "  0.057430777698755264,\n",
              "  0.053508974611759186,\n",
              "  0.04303254187107086,\n",
              "  0.04733540490269661,\n",
              "  0.03572798892855644,\n",
              "  0.0331697054207325,\n",
              "  0.03470978885889053,\n",
              "  0.032357316464185715,\n",
              "  0.04035990685224533,\n",
              "  0.03231307119131088,\n",
              "  0.03424827754497528,\n",
              "  0.03066101297736168,\n",
              "  0.03180523216724396,\n",
              "  0.025787044316530228,\n",
              "  0.030826997011899948,\n",
              "  0.024856334552168846,\n",
              "  0.02398422174155712,\n",
              "  0.02414293773472309,\n",
              "  0.02324756607413292,\n",
              "  0.027176888659596443,\n",
              "  0.02601531520485878,\n",
              "  0.023763535544276237,\n",
              "  0.024326447397470474,\n",
              "  0.02451302297413349,\n",
              "  0.024998236447572708,\n",
              "  0.022637179121375084,\n",
              "  0.023679327219724655,\n",
              "  0.021797453984618187,\n",
              "  0.0216493159532547,\n",
              "  0.02065511792898178,\n",
              "  0.020769765600562096,\n",
              "  0.021752726286649704,\n",
              "  0.020986715331673622,\n",
              "  0.021884290501475334,\n",
              "  0.02495473064482212,\n",
              "  0.023809297010302544,\n",
              "  0.020356208086013794,\n",
              "  0.019684145227074623,\n",
              "  0.018863793462514877,\n",
              "  0.019545510411262512,\n",
              "  0.019389761611819267,\n",
              "  0.021235818043351173,\n",
              "  0.018972251564264297,\n",
              "  0.01903943531215191,\n",
              "  0.027494285255670547,\n",
              "  0.02009475789964199,\n",
              "  0.019433435052633286,\n",
              "  0.019959276542067528,\n",
              "  0.019485758617520332,\n",
              "  0.018218019977211952,\n",
              "  0.017109107226133347,\n",
              "  0.01809612661600113,\n",
              "  0.018134020268917084,\n",
              "  0.017572686076164246,\n",
              "  0.019601503387093544,\n",
              "  0.0214804969727993,\n",
              "  0.02163024991750717,\n",
              "  0.017776062712073326,\n",
              "  0.0165069792419672,\n",
              "  0.01665165089070797,\n",
              "  0.017297616228461266,\n",
              "  0.01696711964905262,\n",
              "  0.01796276867389679,\n",
              "  0.017610307782888412,\n",
              "  0.01714305952191353,\n",
              "  0.016038350760936737,\n",
              "  0.017293013632297516,\n",
              "  0.018977969884872437,\n",
              "  0.01708281971514225,\n",
              "  0.01521348673850298,\n",
              "  0.01763404719531536,\n",
              "  0.016975050792098045,\n",
              "  0.015868475660681725,\n",
              "  0.017851781100034714,\n",
              "  0.019414912909269333,\n",
              "  0.014619662426412106,\n",
              "  0.01589006930589676,\n",
              "  0.017562303692102432,\n",
              "  0.01595226116478443,\n",
              "  0.015099454671144485,\n",
              "  0.015177723951637745,\n",
              "  0.014973674900829792,\n",
              "  0.0162226352840662,\n",
              "  0.015868857502937317,\n",
              "  0.016429563984274864,\n",
              "  0.01647551730275154,\n",
              "  0.01745452731847763,\n",
              "  0.015066866762936115,\n",
              "  0.016349582001566887,\n",
              "  0.015373988077044487,\n",
              "  0.01525211613625288,\n",
              "  0.015944913029670715,\n",
              "  0.02056758850812912,\n",
              "  0.01591419242322445,\n",
              "  0.016757911071181297,\n",
              "  0.01342854741960764,\n",
              "  0.014548344537615776,\n",
              "  0.013638987205922604,\n",
              "  0.014491870068013668,\n",
              "  0.0138894934207201,\n",
              "  0.015009541995823383,\n",
              "  0.013703803531825542,\n",
              "  0.013506339862942696,\n",
              "  0.014333384111523628,\n",
              "  0.014018581248819828,\n",
              "  0.014948512427508831,\n",
              "  0.01473443116992712,\n",
              "  0.013232974335551262,\n",
              "  0.013274983502924442,\n",
              "  0.013218618929386139,\n",
              "  0.01426562201231718,\n",
              "  0.013462158851325512,\n",
              "  0.013106070458889008,\n",
              "  0.012662438675761223,\n",
              "  0.013228568248450756,\n",
              "  0.01510500069707632,\n",
              "  0.013711674138903618,\n",
              "  0.013996900990605354,\n",
              "  0.013941401615738869,\n",
              "  0.020202402025461197,\n",
              "  0.019995853304862976,\n",
              "  0.015171007253229618,\n",
              "  0.016834869980812073,\n",
              "  0.015924233943223953,\n",
              "  0.01655687391757965,\n",
              "  0.015425167977809906,\n",
              "  0.014564803801476955,\n",
              "  0.012542418204247952,\n",
              "  0.013609115965664387,\n",
              "  0.012531550601124763,\n",
              "  0.01260561216622591,\n",
              "  0.013261502608656883,\n",
              "  0.012129410170018673,\n",
              "  0.015238399617373943,\n",
              "  0.018526719883084297,\n",
              "  0.015580594539642334,\n",
              "  0.012346494942903519,\n",
              "  0.012971293181180954,\n",
              "  0.013157224282622337,\n",
              "  0.01234220340847969,\n",
              "  0.012718940153717995,\n",
              "  0.011910801753401756,\n",
              "  0.014502898789942265,\n",
              "  0.012790677137672901,\n",
              "  0.013816486112773418,\n",
              "  0.011910608969628811,\n",
              "  0.013125019147992134,\n",
              "  0.012754857540130615,\n",
              "  0.012624082155525684,\n",
              "  0.017359763383865356,\n",
              "  0.013006286695599556,\n",
              "  0.013094796799123287,\n",
              "  0.01174608152359724,\n",
              "  0.011443940922617912,\n",
              "  0.013419156894087791,\n",
              "  0.012277086265385151,\n",
              "  0.011796501465141773,\n",
              "  0.012294964864850044,\n",
              "  0.012815686874091625,\n",
              "  0.013278210535645485,\n",
              "  0.011661874130368233,\n",
              "  0.010555113665759563,\n",
              "  0.011475213803350925,\n",
              "  0.011830349452793598,\n",
              "  0.011198288761079311,\n",
              "  0.012079832144081593,\n",
              "  0.012045921757817268,\n",
              "  0.011160070076584816,\n",
              "  0.012000929564237595,\n",
              "  0.011853114701807499,\n",
              "  0.012688301503658295,\n",
              "  0.012479908764362335,\n",
              "  0.01150329876691103,\n",
              "  0.011675608344376087,\n",
              "  0.011417744681239128,\n",
              "  0.01129714585840702,\n",
              "  0.011765910312533379,\n",
              "  0.010383657179772854,\n",
              "  0.011698399670422077,\n",
              "  0.011580240912735462,\n",
              "  0.01062744576483965,\n",
              "  0.012067685835063457,\n",
              "  0.012640519998967648,\n",
              "  0.01640249416232109,\n",
              "  0.012087677605450153,\n",
              "  0.02012345753610134,\n",
              "  0.013826580718159676,\n",
              "  0.010752154514193535,\n",
              "  0.011280183680355549,\n",
              "  0.010337291285395622,\n",
              "  0.010748242028057575,\n",
              "  0.010617196559906006,\n",
              "  0.010633496567606926,\n",
              "  0.011071284301578999,\n",
              "  0.010303536430001259,\n",
              "  0.0130853820592165,\n",
              "  0.011033228598535061,\n",
              "  0.01016911119222641,\n",
              "  0.01122465543448925,\n",
              "  0.014159739017486572,\n",
              "  0.010769300162792206,\n",
              "  0.010856134817004204,\n",
              "  0.011509357020258904,\n",
              "  0.011406159028410912,\n",
              "  0.011099758557975292,\n",
              "  0.011345083825290203,\n",
              "  0.011363185942173004,\n",
              "  0.010458337143063545,\n",
              "  0.01014967355877161,\n",
              "  0.011752738617360592,\n",
              "  0.011294062249362469,\n",
              "  0.011892227455973625,\n",
              "  0.010307552292943,\n",
              "  0.011121292598545551,\n",
              "  0.010585627518594265,\n",
              "  0.009996703825891018,\n",
              "  0.01136547327041626,\n",
              "  0.01197844184935093,\n",
              "  0.009913650341331959,\n",
              "  0.01072530634701252,\n",
              "  0.010536166839301586,\n",
              "  0.010022659786045551,\n",
              "  0.01043944526463747,\n",
              "  0.012284436263144016,\n",
              "  0.010706435889005661,\n",
              "  0.01590840518474579,\n",
              "  0.013933610171079636,\n",
              "  0.012149142101407051,\n",
              "  0.010331510566174984,\n",
              "  0.010163730010390282,\n",
              "  0.010625557973980904,\n",
              "  0.010062181390821934,\n",
              "  0.01091372687369585,\n",
              "  0.010028920136392117,\n",
              "  0.009917467832565308,\n",
              "  0.00969666987657547,\n",
              "  0.009986603632569313,\n",
              "  0.009911228902637959,\n",
              "  0.010368130169808865,\n",
              "  0.009937671013176441,\n",
              "  0.009736611507833004,\n",
              "  0.009992758743464947,\n",
              "  0.009619949385523796,\n",
              "  0.01209293957799673,\n",
              "  0.010051424615085125,\n",
              "  0.009690733626484871,\n",
              "  0.010384880006313324,\n",
              "  0.010385323315858841,\n",
              "  0.009684891439974308,\n",
              "  0.010385924950242043,\n",
              "  0.011245946399867535,\n",
              "  0.011429421603679657,\n",
              "  0.009932726621627808,\n",
              "  0.011277441866695881,\n",
              "  0.010427804663777351,\n",
              "  0.009998701512813568,\n",
              "  0.010168333537876606,\n",
              "  0.010021512396633625,\n",
              "  0.009862287901341915,\n",
              "  0.009347720071673393,\n",
              "  0.00992739386856556,\n",
              "  0.009629135951399803,\n",
              "  0.009416542015969753,\n",
              "  0.009308965876698494,\n",
              "  0.008934704586863518,\n",
              "  0.010222728364169598,\n",
              "  0.009035374037921429,\n",
              "  0.009835728444159031,\n",
              "  0.010719593614339828,\n",
              "  0.009299132041633129,\n",
              "  0.010119487531483173,\n",
              "  0.009429964236915112,\n",
              "  0.010958867147564888,\n",
              "  0.011646216735243797,\n",
              "  0.010846826247870922,\n",
              "  0.010238848626613617,\n",
              "  0.009967842139303684,\n",
              "  0.009480123408138752,\n",
              "  0.00908846128731966,\n",
              "  0.011168120428919792,\n",
              "  0.00969899632036686,\n",
              "  0.009840264916419983,\n",
              "  0.010006322525441647,\n",
              "  0.009184002876281738,\n",
              "  0.009861554019153118,\n",
              "  0.00962017010897398,\n",
              "  0.009471416473388672,\n",
              "  0.01024188008159399,\n",
              "  0.011410115286707878,\n",
              "  0.009178023785352707,\n",
              "  0.009984694421291351,\n",
              "  0.011243847198784351,\n",
              "  0.00958176888525486,\n",
              "  0.009193823672831059,\n",
              "  0.010325183160603046,\n",
              "  0.009302975609898567,\n",
              "  0.008686738088726997,\n",
              "  0.010833810083568096,\n",
              "  0.011398566886782646,\n",
              "  0.009561861865222454,\n",
              "  0.010052783414721489,\n",
              "  0.009724871255457401,\n",
              "  0.009935393929481506,\n",
              "  0.008735439740121365,\n",
              "  0.00971895270049572,\n",
              "  0.010477420873939991,\n",
              "  0.009822512045502663,\n",
              "  0.009006815031170845,\n",
              "  0.009428436867892742,\n",
              "  0.008644011802971363,\n",
              "  0.008947418071329594,\n",
              "  0.009472980163991451,\n",
              "  0.008165281265974045,\n",
              "  0.009239446371793747,\n",
              "  0.009108909405767918,\n",
              "  0.008189204148948193,\n",
              "  0.008646092377603054,\n",
              "  0.009916975162923336,\n",
              "  0.008515829220414162,\n",
              "  0.009332885034382343,\n",
              "  0.007967337034642696,\n",
              "  0.009631647728383541,\n",
              "  0.0088496720418334,\n",
              "  0.011526189744472504,\n",
              "  0.011155226267874241,\n",
              "  0.010000603273510933,\n",
              "  0.008780574426054955,\n",
              "  0.010220871306955814,\n",
              "  0.00868013221770525,\n",
              "  0.00924625527113676,\n",
              "  0.008416548371315002,\n",
              "  0.00925862044095993,\n",
              "  0.008973626419901848,\n",
              "  0.010067514143884182,\n",
              "  0.00906931422650814,\n",
              "  0.00871147122234106,\n",
              "  0.009664860554039478,\n",
              "  0.0092720165848732,\n",
              "  0.009756444953382015,\n",
              "  0.008582724258303642,\n",
              "  0.009647401049733162,\n",
              "  0.008775259368121624,\n",
              "  0.010247255675494671,\n",
              "  0.008809613063931465,\n",
              "  0.008505179546773434,\n",
              "  0.008881465531885624,\n",
              "  0.009922303259372711,\n",
              "  0.008837352506816387,\n",
              "  0.009714498184621334,\n",
              "  0.0084867337718606,\n",
              "  0.00828145258128643,\n",
              "  0.012533650733530521,\n",
              "  0.009988357312977314,\n",
              "  0.008342335931956768,\n",
              "  0.010234780609607697,\n",
              "  0.008998697623610497,\n",
              "  0.009418781846761703,\n",
              "  0.008947665803134441,\n",
              "  0.008677572011947632,\n",
              "  0.008648614399135113,\n",
              "  0.008357950486242771,\n",
              "  0.008561909198760986,\n",
              "  0.008469630032777786,\n",
              "  0.00853407196700573,\n",
              "  0.008750097826123238,\n",
              "  0.009024374186992645,\n",
              "  0.008175134658813477,\n",
              "  0.007528840098530054,\n",
              "  0.008893297985196114,\n",
              "  0.008296564221382141,\n",
              "  0.00913920532912016,\n",
              "  0.007670230697840452,\n",
              "  0.007673677988350391,\n",
              "  0.00887508224695921,\n",
              "  0.008115457370877266,\n",
              "  0.010466599836945534,\n",
              "  0.00951508991420269,\n",
              "  0.008709256537258625,\n",
              "  0.008959862403571606,\n",
              "  0.00859109964221716,\n",
              "  0.008398795500397682,\n",
              "  0.00856472086161375,\n",
              "  0.008913544937968254,\n",
              "  0.009100408293306828,\n",
              "  0.008819552138447762,\n",
              "  0.008278148248791695,\n",
              "  0.00872466154396534,\n",
              "  0.008691729977726936,\n",
              "  0.008265911601483822]}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "ObGrOy0LHu3x",
        "outputId": "48bf3eb6-0054-40e8-caf5-f1a4805b83bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1=model.get_layer(\"dense\")"
      ],
      "metadata": {
        "id": "Z0aNWNfsSHtz"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1.activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu1aFN96coWC",
        "outputId": "c20cd1ae-63d7-4ad8-e719-aca95b77797e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function keras.activations.sigmoid>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o1=model.get_layer(\"vektor\")"
      ],
      "metadata": {
        "id": "EEaadwGCa_H9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "mNNzsG6yhAvc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "hidden_states_fun = K.function([i1.input],[o1.output])"
      ],
      "metadata": {
        "id": "rzxk7T8HhQOn"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(X, index):\n",
        "    \"\"\"Return the hidden state associated with an input at the given timestep.\n",
        "    \"\"\"\n",
        "    hidden_states = hidden_states_fun(X.iloc[index:index+1,:].values)\n",
        "    return hidden_states"
      ],
      "metadata": {
        "id": "vw9pCVighoUh"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_minmax.iloc[0:1,:].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q6HBgVOiCgB",
        "outputId": "f4a53725-590d-4dc4-c840-a04a052bf6fb"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oo_list=[]\n",
        "for i in range( len(df_X_minmax)):\n",
        "    xo=get_embedding(df_X_minmax,i)\n",
        "    oo_list.append(xo[0].tolist()[0])"
      ],
      "metadata": {
        "id": "zA2s-WuVhCce"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oo_list[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBcmU0HGqhs1",
        "outputId": "73eba2cf-e7b1-4034-c3ee-497bd63165c5"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9945401549339294,\n",
              " 0.9449493288993835,\n",
              " 0.9998797178268433,\n",
              " 0.9280192255973816,\n",
              " 0.3235584795475006,\n",
              " 0.008531337603926659,\n",
              " 0.011019857600331306,\n",
              " 2.9472486858139746e-05,\n",
              " 0.13943292200565338,\n",
              " 0.9911682605743408,\n",
              " 1.4799631571804639e-05,\n",
              " 0.04932885989546776,\n",
              " 0.24038082361221313,\n",
              " 0.0012519300216808915,\n",
              " 5.27960728504695e-05,\n",
              " 0.9856116771697998,\n",
              " 0.997418999671936,\n",
              " 0.0004754349065478891,\n",
              " 0.8775749206542969,\n",
              " 0.0020824256353080273,\n",
              " 1.6999662788563796e-09,\n",
              " 0.0061145140789449215,\n",
              " 0.9998944997787476,\n",
              " 0.9999854564666748,\n",
              " 0.4216068685054779,\n",
              " 5.073061402072199e-05,\n",
              " 0.029871614649891853,\n",
              " 0.22542732954025269,\n",
              " 0.03987743332982063,\n",
              " 3.1711180781712756e-05,\n",
              " 0.008698394522070885,\n",
              " 0.4248214662075043,\n",
              " 0.2812606692314148,\n",
              " 7.236806154331532e-10,\n",
              " 0.26865047216415405,\n",
              " 0.0077888863161206245,\n",
              " 0.0029708941001445055,\n",
              " 0.09937344491481781,\n",
              " 0.015062452293932438,\n",
              " 0.0010229971958324313,\n",
              " 0.9998217225074768,\n",
              " 0.8248856663703918,\n",
              " 0.12483494728803635,\n",
              " 8.23273637706734e-07,\n",
              " 0.018959419801831245,\n",
              " 0.002224889351055026,\n",
              " 0.9878070950508118,\n",
              " 0.002278971718624234,\n",
              " 0.7766640186309814,\n",
              " 0.9999991655349731,\n",
              " 0.09208393841981888,\n",
              " 0.33880776166915894,\n",
              " 0.015569192357361317,\n",
              " 0.009516438469290733,\n",
              " 0.10357288271188736,\n",
              " 0.19256074726581573,\n",
              " 0.05830764025449753,\n",
              " 0.9999970197677612,\n",
              " 0.9999957084655762,\n",
              " 0.7901324033737183,\n",
              " 0.0010043737711384892,\n",
              " 0.9747968912124634,\n",
              " 0.9999988079071045,\n",
              " 0.013457654975354671,\n",
              " 2.656694414326921e-05,\n",
              " 0.9999442100524902,\n",
              " 1.4242080226267717e-07,\n",
              " 0.9652854800224304,\n",
              " 0.9995114803314209,\n",
              " 2.7701966246240772e-05,\n",
              " 4.942189661960583e-06,\n",
              " 0.00013340753503143787,\n",
              " 0.9709733724594116,\n",
              " 0.07868915051221848,\n",
              " 9.81728565108142e-09,\n",
              " 0.9680832624435425,\n",
              " 0.17219887673854828,\n",
              " 0.02475764788687229,\n",
              " 6.700807686499388e-10,\n",
              " 1.1897240881353355e-07,\n",
              " 0.004764357581734657,\n",
              " 0.025447823107242584,\n",
              " 0.4415504038333893,\n",
              " 0.0008459910750389099,\n",
              " 1.4586148608941585e-05,\n",
              " 0.9301227927207947,\n",
              " 0.16588972508907318,\n",
              " 0.40703409910202026,\n",
              " 0.024575259536504745,\n",
              " 0.9630509614944458,\n",
              " 0.001961465459316969,\n",
              " 0.870108962059021,\n",
              " 0.9999427795410156,\n",
              " 0.16086092591285706,\n",
              " 0.7244166135787964,\n",
              " 0.9999852180480957,\n",
              " 0.33829495310783386,\n",
              " 0.9805976748466492,\n",
              " 0.9682907462120056,\n",
              " 0.00035015877801924944,\n",
              " 0.2272041290998459,\n",
              " 0.9985901713371277,\n",
              " 4.995632025384111e-06,\n",
              " 0.0025650702882558107,\n",
              " 0.9999771118164062,\n",
              " 0.9999492168426514,\n",
              " 2.1529700461542234e-05,\n",
              " 0.7396885752677917,\n",
              " 0.18398132920265198,\n",
              " 0.008247115649282932,\n",
              " 0.0009128372184932232,\n",
              " 0.005295601207762957,\n",
              " 0.0033456948585808277,\n",
              " 0.861564576625824,\n",
              " 2.070598270620394e-07,\n",
              " 2.5047949492318367e-08,\n",
              " 0.0001609533792361617,\n",
              " 0.977711021900177,\n",
              " 0.8632375001907349,\n",
              " 0.9715782999992371,\n",
              " 4.2408773879287764e-06,\n",
              " 0.9724982976913452,\n",
              " 0.6489490866661072,\n",
              " 2.190353853848137e-08,\n",
              " 0.00658388389274478,\n",
              " 0.0006113097188062966,\n",
              " 0.016398662701249123,\n",
              " 0.3186552822589874,\n",
              " 3.8953479020165105e-07,\n",
              " 0.0008090659393928945,\n",
              " 0.9976412057876587,\n",
              " 0.00047908988199196756,\n",
              " 0.8855105638504028,\n",
              " 0.0010566128185018897,\n",
              " 0.11730342358350754,\n",
              " 0.005566178355365992,\n",
              " 0.9999984502792358,\n",
              " 9.820761988521554e-06,\n",
              " 0.19876757264137268,\n",
              " 1.8505910702515393e-05,\n",
              " 0.9999825954437256,\n",
              " 0.992143988609314,\n",
              " 7.776331661935032e-11,\n",
              " 0.9461272954940796,\n",
              " 0.7747242450714111,\n",
              " 0.059077419340610504,\n",
              " 0.060221076011657715,\n",
              " 0.15386073291301727,\n",
              " 0.9999563694000244,\n",
              " 0.6671364903450012]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PTXd7qn4qhrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(xo[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZGTg1q2jhub",
        "outputId": "a0694838-4ca9-4318-bef9-ddf70a5a692f"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xo[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwBOkXcnkXSY",
        "outputId": "235c5ae7-15a1-473d-ebde-65a2bb03cc58"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.8000597953796387,\n",
              "  0.8815061450004578,\n",
              "  0.9996716976165771,\n",
              "  0.8157748579978943,\n",
              "  0.981206476688385,\n",
              "  0.25905904173851013,\n",
              "  0.05861975625157356,\n",
              "  0.00019873295968864113,\n",
              "  0.8472177386283875,\n",
              "  0.9246665239334106,\n",
              "  2.020833335336647e-06,\n",
              "  0.5772954821586609,\n",
              "  0.047245610505342484,\n",
              "  0.0037986140232533216,\n",
              "  3.530822823449853e-06,\n",
              "  0.9996250867843628,\n",
              "  0.999975323677063,\n",
              "  7.134862971724942e-05,\n",
              "  0.7587500810623169,\n",
              "  0.001573936897329986,\n",
              "  4.3187835530034135e-09,\n",
              "  0.07836626470088959,\n",
              "  0.9885867238044739,\n",
              "  0.9999954700469971,\n",
              "  0.07075563073158264,\n",
              "  7.376825669780374e-05,\n",
              "  0.0027085826732218266,\n",
              "  0.007707920391112566,\n",
              "  0.792831301689148,\n",
              "  4.737873314297758e-05,\n",
              "  0.03709573298692703,\n",
              "  0.40343940258026123,\n",
              "  0.5808010697364807,\n",
              "  4.184232516024622e-09,\n",
              "  0.07477693259716034,\n",
              "  0.15050607919692993,\n",
              "  0.0012069201329723,\n",
              "  0.7145284414291382,\n",
              "  0.030998174101114273,\n",
              "  0.013758105225861073,\n",
              "  0.999940037727356,\n",
              "  0.7943576574325562,\n",
              "  0.1403360217809677,\n",
              "  1.4296472272690153e-07,\n",
              "  0.007843018509447575,\n",
              "  0.0008205902413465083,\n",
              "  0.5468271374702454,\n",
              "  0.13996092975139618,\n",
              "  0.5501651763916016,\n",
              "  0.9999977350234985,\n",
              "  0.8407254219055176,\n",
              "  0.006739257834851742,\n",
              "  0.19774894416332245,\n",
              "  0.012222020886838436,\n",
              "  0.48510822653770447,\n",
              "  0.6500834822654724,\n",
              "  0.7890853881835938,\n",
              "  0.9999176263809204,\n",
              "  0.9999639987945557,\n",
              "  0.4830746054649353,\n",
              "  0.03199423849582672,\n",
              "  0.9491472244262695,\n",
              "  0.999993085861206,\n",
              "  0.03995903208851814,\n",
              "  0.00038127644802443683,\n",
              "  0.9999880790710449,\n",
              "  1.0080293044723021e-08,\n",
              "  0.9284048676490784,\n",
              "  0.9996415376663208,\n",
              "  1.031819647323573e-05,\n",
              "  1.2371414186418406e-06,\n",
              "  0.0002693638380151242,\n",
              "  0.7077626585960388,\n",
              "  0.7579114437103271,\n",
              "  6.766997984186673e-08,\n",
              "  0.39310723543167114,\n",
              "  0.021320190280675888,\n",
              "  0.08597417920827866,\n",
              "  3.037660079030502e-08,\n",
              "  2.5322847818642913e-07,\n",
              "  0.16804707050323486,\n",
              "  0.03919237479567528,\n",
              "  0.9836628437042236,\n",
              "  0.008354727178812027,\n",
              "  4.0263721530209295e-06,\n",
              "  0.8731123208999634,\n",
              "  0.7497512698173523,\n",
              "  0.081596240401268,\n",
              "  0.010195134207606316,\n",
              "  0.8679699301719666,\n",
              "  0.03279731422662735,\n",
              "  0.8162083029747009,\n",
              "  0.9995633959770203,\n",
              "  0.1944364607334137,\n",
              "  0.8420625925064087,\n",
              "  0.9981774091720581,\n",
              "  0.5932146310806274,\n",
              "  0.9687314629554749,\n",
              "  0.893016517162323,\n",
              "  4.751484993903432e-06,\n",
              "  0.48566287755966187,\n",
              "  0.9709073901176453,\n",
              "  8.598893828093424e-07,\n",
              "  5.006479113944806e-05,\n",
              "  0.9984639883041382,\n",
              "  0.9999836683273315,\n",
              "  0.0017191603546962142,\n",
              "  0.04819531366229057,\n",
              "  0.9420687556266785,\n",
              "  0.027482088655233383,\n",
              "  6.657316589553375e-06,\n",
              "  0.013737752102315426,\n",
              "  0.03909672796726227,\n",
              "  0.5046782493591309,\n",
              "  1.350276761513669e-06,\n",
              "  1.416068789694691e-05,\n",
              "  2.3447197236237116e-05,\n",
              "  0.5197430849075317,\n",
              "  0.8241085410118103,\n",
              "  0.9863184690475464,\n",
              "  1.3475891137204599e-05,\n",
              "  0.8878957033157349,\n",
              "  0.01619143597781658,\n",
              "  1.1306410385714116e-07,\n",
              "  0.12075337022542953,\n",
              "  7.352996180998161e-05,\n",
              "  0.009567515924572945,\n",
              "  0.2343396246433258,\n",
              "  0.00035806657979264855,\n",
              "  0.0007975613116286695,\n",
              "  0.9126730561256409,\n",
              "  0.00023221010633278638,\n",
              "  0.17548957467079163,\n",
              "  0.045371320098638535,\n",
              "  0.7912659645080566,\n",
              "  0.005512830335646868,\n",
              "  0.9998561143875122,\n",
              "  0.0002913492207881063,\n",
              "  0.1677822470664978,\n",
              "  0.007262530270963907,\n",
              "  0.9999921321868896,\n",
              "  0.9743072390556335,\n",
              "  7.156817499742729e-11,\n",
              "  0.8078588843345642,\n",
              "  0.3182015120983124,\n",
              "  0.14623264968395233,\n",
              "  0.7024182081222534,\n",
              "  0.011671586893498898,\n",
              "  0.9999213218688965,\n",
              "  0.7952758073806763]]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "YrGI_BRDQrVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82AeWLEG1vsQ"
      },
      "outputs": [],
      "source": [
        "_MODE_=\"Tesztel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I132le63oAkN"
      },
      "outputs": [],
      "source": [
        "fname=\"https://github.com/sipocz/LSTM_HPLC/raw/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\"\n",
        "if _MODE_==\"Tesztel\":\n",
        "    #!rm XXXX*\n",
        "    !wget $fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el4jLBvoybTN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05jpd7BjeYez"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrAfG02Lj1wr"
      },
      "outputs": [],
      "source": [
        "fname=\"XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0mdKjZBjb7L"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk87VBSQndA5"
      },
      "outputs": [],
      "source": [
        "model.load_weights(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1ks758inc_Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_CHGgYja_bB"
      },
      "outputs": [],
      "source": [
        "pred=model.predict(df_X_minmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loCl2-qmA6SL"
      },
      "outputs": [],
      "source": [
        "def maxpos(alist):\n",
        "    temp = max(alist)\n",
        "    res = [i for i, j in enumerate(alist) if j == temp]\n",
        "    return res[0]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIFRZ753bMNi"
      },
      "outputs": [],
      "source": [
        "#from IPython.lib.display import YouTubeVideo\n",
        "def show_difference(pred,ytrain,verbose=False, only_wrong=True, to_table=False):\n",
        "    ecounter=0\n",
        "    o=[]\n",
        "    for i in range(len(pred)):\n",
        "        predi=pred[i]\n",
        "        traini=ytrain[i]\n",
        "        if abs(predi-traini)>0.5:\n",
        "            ecounter+=1\n",
        "            o.append(i)\n",
        "        if verbose:\n",
        "            if only_wrong:\n",
        "                \n",
        "                if abs(predi-traini)>0.5: \n",
        "                    if to_table:\n",
        "                        print(f\"|{i}|{predi}|{traini}|\")\n",
        "                    else:\n",
        "                        print(f\"{i}, {predi}, {traini}\")\n",
        "                \n",
        "            else:\n",
        "                print(f\"{i}, {predi}, {traini}\")\n",
        "\n",
        "                \n",
        "    print(f\"Hiba szám: {ecounter:6}, arány: {ecounter/len(ytrain)*100:3.2f}% \")\n",
        "    return(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7akwJMQ8Oza"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FCxVKUDhery"
      },
      "outputs": [],
      "source": [
        "df.iloc[188]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWKt2DCcFbKc"
      },
      "outputs": [],
      "source": [
        "wrong_prediction_list=show_difference(pred,df_y2,verbose=True,to_table=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwUpz9KG73GH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UF3Gzmf7vFt"
      },
      "outputs": [],
      "source": [
        "def show_batchID(df,index_list):\n",
        "    for i in index_list:\n",
        "        print(f\"{i:4}. --> {df.iloc[i].time.split('_')[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV04pFpA8_dj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo65biL-8djO"
      },
      "outputs": [],
      "source": [
        "show_batchID(df,wrong_prediction_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x39xe3TmGHJ_"
      },
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsQ2eUE8GZc_"
      },
      "outputs": [],
      "source": [
        "!pip install pypng\n",
        "!pip install pyqrcode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzKez2bME9eD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pyqrcode\n",
        "link_1 = pyqrcode.create('https://www.linkedin.com/in/36204746473/')\n",
        "link_1.png(\"p1.png\",scale=3,module_color=(20,20,20,255), background=(200,200,200,255))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlKyrRqfG6GP"
      },
      "source": [
        "### Model usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvmmKvBIF4n8"
      },
      "outputs": [],
      "source": [
        "test_df=pd.read_csv(\"test_200.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc0vyTmcHRvY"
      },
      "outputs": [],
      "source": [
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5utaqxGNHU7y"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykk17oAEoPwM"
      },
      "outputs": [],
      "source": [
        "__MAXWORD__=200\n",
        "test_word_list=list(test_df.Words_in_Numbers)\n",
        "x_test=create_x(test_word_list,maxword=__MAXWORD__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg0DqmUWoj5s"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itw7lX_hqO5m"
      },
      "outputs": [],
      "source": [
        "o=[\"kitaifa\",\"michezo\",\"biashara\",\"kamataifa\",\"burudani\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOQFlEC2oj2i"
      },
      "outputs": [],
      "source": [
        "def data_generator(y_pred):\n",
        "    out=[]\n",
        "    for pred in y_pred:\n",
        "        t1=[0,0,0,0,0]\n",
        "        ox=pred.argmax()\n",
        "        t1[ox]=1\n",
        "        out.append(t1) \n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSRQc0kWojzb"
      },
      "outputs": [],
      "source": [
        "output=data_generator(y_pred)\n",
        "\n",
        "output0=[x[0] for x in output]\n",
        "output1=[x[1] for x in output]\n",
        "output2=[x[2] for x in output]\n",
        "output3=[x[3] for x in output]\n",
        "output4=[x[4] for x in output]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G88pQ7jYojvu"
      },
      "outputs": [],
      "source": [
        "df_csv=pd.DataFrame()\n",
        "df_csv[\"test_id\"]=test_df[\"swahili_id\"]\n",
        "df_csv[o[0]]=output0\n",
        "df_csv[o[1]]=output1\n",
        "df_csv[o[2]]=output2\n",
        "df_csv[o[3]]=output3\n",
        "df_csv[o[4]]=output4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NSaY0zkyu1t"
      },
      "outputs": [],
      "source": [
        "df_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw4CUPobbH7A"
      },
      "outputs": [],
      "source": [
        "from datetime  import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAbhqmcPdF6U"
      },
      "outputs": [],
      "source": [
        "a=datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiOwO9UTdeUt"
      },
      "outputs": [],
      "source": [
        "fname=\"submission_\"+a+\".csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELciPC_OpWA"
      },
      "outputs": [],
      "source": [
        "df_csv.to_csv(fname,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvmaQZGBYM_7"
      },
      "outputs": [],
      "source": [
        "!head $fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrtPBTgcYwtg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HPLC_dense.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}