{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/LSTM_HPLC/blob/main/HPLC_dense_encoder_decoder_20220414000800.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VoDnJFuApPY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lS8T-dxnwfjO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xFjap2PTfYbv"
      },
      "outputs": [],
      "source": [
        "#emoji_url=\"https://github.com/sipocz/Twitter-Sentiment-Analysis-AI-Challenge/raw/5e7d7d29f58be438f33fa0c0bc2fd251dea0b453/emoji.py\"\n",
        "#!rm emoji.py\n",
        "#!wget $emoji_url\n",
        "#import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDUZMjUkgIIe",
        "outputId": "46f968fc-5d48-49a2-d567-764d0a5d093f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*Nmer2*': No such file or directory\n",
            "--2022-04-13 21:22:10--  https://github.com/sipocz/LSTM_HPLC/raw/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/LSTM_HPLC/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv [following]\n",
            "--2022-04-13 21:22:10--  https://raw.githubusercontent.com/sipocz/LSTM_HPLC/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15772588 (15M) [text/plain]\n",
            "Saving to: ‘Nmer2.csv’\n",
            "\n",
            "Nmer2.csv           100%[===================>]  15.04M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-13 21:22:11 (138 MB/s) - ‘Nmer2.csv’ saved [15772588/15772588]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!rm *Nmer2*\n",
        "\n",
        "!wget https://github.com/sipocz/LSTM_HPLC/raw/3a447ab5d1d37cdcbcb4d931ce7d73518359c8b2/orig/Nmer2.csv\n",
        "#!wget https://github.com/sipocz/LSTM_HPLC/raw/4371b1ef331c9d0f304e25117f160b979bb39661/orig/N_m3.csv\n",
        "\n",
        "_MODE_=\"Tesztel\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NFiNZuPMkfGl"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"Nmer2.csv\",sep=\",\",skiprows=1,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nCjVAcb2iJZy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "YDtEcJB5haQF",
        "outputId": "108c6750-09fa-4fb5-fc73-a22684c12e33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 time        0  0.00333333  0.00666667      0.01  0.0133333  \\\n",
              "0  M13127N_detect3020  1.68793     1.41315    0.755643  0.480864   0.255152   \n",
              "1  M13144N_detect3020  1.92345     1.32483    0.863592  0.559372   0.323847   \n",
              "2  M13153N_detect3020  1.88420     1.27576    0.853778  0.569185   0.333661   \n",
              "3  M14028N_detect3020  0.00000     1.42296    0.942100  0.598626   0.382728   \n",
              "4  M14049N_detect3020  1.59961     1.07949    0.706575  0.441609   0.235525   \n",
              "\n",
              "   0.0166667      0.02  0.0233333  0.0266667  ...  47.9167  47.92  47.9233  \\\n",
              "0   0.107949  0.000000        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "1   0.176644  0.039254        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "2   0.166830  0.039254        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "3   0.206084  0.058881        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "4   0.117762  0.000000        0.0        0.0  ...      0.0    0.0      0.0   \n",
              "\n",
              "   47.9267  47.93  47.9333  47.9367  47.94  Unnamed: 14384  Unnamed: 14385  \n",
              "0      0.0    0.0      0.0      0.0    0.0     2524.709798        megfelel  \n",
              "1      0.0    0.0      0.0      0.0    0.0     2422.244040        megfelel  \n",
              "2      0.0    0.0      0.0      0.0    0.0     2529.530277        megfelel  \n",
              "3      0.0    0.0      0.0      0.0    0.0     2433.146990        megfelel  \n",
              "4      0.0    0.0      0.0      0.0    0.0     2652.605677        megfelel  \n",
              "\n",
              "[5 rows x 14386 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f43fdde9-4da8-4c3c-ab80-6d2a58f4801c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>0</th>\n",
              "      <th>0.00333333</th>\n",
              "      <th>0.00666667</th>\n",
              "      <th>0.01</th>\n",
              "      <th>0.0133333</th>\n",
              "      <th>0.0166667</th>\n",
              "      <th>0.02</th>\n",
              "      <th>0.0233333</th>\n",
              "      <th>0.0266667</th>\n",
              "      <th>...</th>\n",
              "      <th>47.9167</th>\n",
              "      <th>47.92</th>\n",
              "      <th>47.9233</th>\n",
              "      <th>47.9267</th>\n",
              "      <th>47.93</th>\n",
              "      <th>47.9333</th>\n",
              "      <th>47.9367</th>\n",
              "      <th>47.94</th>\n",
              "      <th>Unnamed: 14384</th>\n",
              "      <th>Unnamed: 14385</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M13127N_detect3020</td>\n",
              "      <td>1.68793</td>\n",
              "      <td>1.41315</td>\n",
              "      <td>0.755643</td>\n",
              "      <td>0.480864</td>\n",
              "      <td>0.255152</td>\n",
              "      <td>0.107949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2524.709798</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M13144N_detect3020</td>\n",
              "      <td>1.92345</td>\n",
              "      <td>1.32483</td>\n",
              "      <td>0.863592</td>\n",
              "      <td>0.559372</td>\n",
              "      <td>0.323847</td>\n",
              "      <td>0.176644</td>\n",
              "      <td>0.039254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2422.244040</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M13153N_detect3020</td>\n",
              "      <td>1.88420</td>\n",
              "      <td>1.27576</td>\n",
              "      <td>0.853778</td>\n",
              "      <td>0.569185</td>\n",
              "      <td>0.333661</td>\n",
              "      <td>0.166830</td>\n",
              "      <td>0.039254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2529.530277</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M14028N_detect3020</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.42296</td>\n",
              "      <td>0.942100</td>\n",
              "      <td>0.598626</td>\n",
              "      <td>0.382728</td>\n",
              "      <td>0.206084</td>\n",
              "      <td>0.058881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2433.146990</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M14049N_detect3020</td>\n",
              "      <td>1.59961</td>\n",
              "      <td>1.07949</td>\n",
              "      <td>0.706575</td>\n",
              "      <td>0.441609</td>\n",
              "      <td>0.235525</td>\n",
              "      <td>0.117762</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2652.605677</td>\n",
              "      <td>megfelel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 14386 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f43fdde9-4da8-4c3c-ab80-6d2a58f4801c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f43fdde9-4da8-4c3c-ab80-6d2a58f4801c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f43fdde9-4da8-4c3c-ab80-6d2a58f4801c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"time\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4Vg8riI4e9G",
        "outputId": "bc45d422-5039-4eae-f8df-7c8fce9edb2e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       M13127N_detect3020\n",
              "1       M13144N_detect3020\n",
              "2       M13153N_detect3020\n",
              "3       M14028N_detect3020\n",
              "4       M14049N_detect3020\n",
              "              ...         \n",
              "200     M07114N_detect3020\n",
              "201     M09124N_detect3020\n",
              "202     M0B014N_detect3020\n",
              "203     M06018N_detect3020\n",
              "204    M06018N2_detect3020\n",
              "Name: time, Length: 205, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGJM5jz-elNX",
        "outputId": "c8be909b-6815-4d4c-c4fe-2cd5ce1304be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time              M5C011N_detect3020\n",
              "0                                0.0\n",
              "0.00333333                   1.21688\n",
              "0.00666667                  0.755643\n",
              "0.01                         0.47105\n",
              "                         ...        \n",
              "47.9333                          0.0\n",
              "47.9367                          0.0\n",
              "47.94                            0.0\n",
              "Unnamed: 14384            2190.83243\n",
              "Unnamed: 14385              megfelel\n",
              "Name: 31, Length: 14386, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.iloc[31]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4arFXoEAuVVh"
      },
      "outputs": [],
      "source": [
        "col=df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DxpDNE_3TgCM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dA8Mk878pUdU"
      },
      "outputs": [],
      "source": [
        "df_X=df[col[11000:-2]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "fkrSKZswuyF1",
        "outputId": "37b764f0-6810-4a09-9e59-5617d8dfa2ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   36.6633  36.6667  36.67  36.6733  36.6767  36.68  36.6833  36.6867  36.69  \\\n",
              "0      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   36.6933  ...  47.91  47.9133  47.9167  47.92  47.9233  47.9267  47.93  \\\n",
              "0      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   47.9333  47.9367  47.94  \n",
              "0      0.0      0.0    0.0  \n",
              "1      0.0      0.0    0.0  \n",
              "2      0.0      0.0    0.0  \n",
              "3      0.0      0.0    0.0  \n",
              "4      0.0      0.0    0.0  \n",
              "\n",
              "[5 rows x 3384 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68645cf4-298a-4c03-8a2c-85bcbcb5e1f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>36.6633</th>\n",
              "      <th>36.6667</th>\n",
              "      <th>36.67</th>\n",
              "      <th>36.6733</th>\n",
              "      <th>36.6767</th>\n",
              "      <th>36.68</th>\n",
              "      <th>36.6833</th>\n",
              "      <th>36.6867</th>\n",
              "      <th>36.69</th>\n",
              "      <th>36.6933</th>\n",
              "      <th>...</th>\n",
              "      <th>47.91</th>\n",
              "      <th>47.9133</th>\n",
              "      <th>47.9167</th>\n",
              "      <th>47.92</th>\n",
              "      <th>47.9233</th>\n",
              "      <th>47.9267</th>\n",
              "      <th>47.93</th>\n",
              "      <th>47.9333</th>\n",
              "      <th>47.9367</th>\n",
              "      <th>47.94</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3384 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68645cf4-298a-4c03-8a2c-85bcbcb5e1f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68645cf4-298a-4c03-8a2c-85bcbcb5e1f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68645cf4-298a-4c03-8a2c-85bcbcb5e1f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x0YQyPvuTa_9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OczaGqbTqKgz"
      },
      "outputs": [],
      "source": [
        "df_y=df[col[-2:-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dLG_fYjUiUSU"
      },
      "outputs": [],
      "source": [
        "df_y2=df[col[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCzr89LSiaM8",
        "outputId": "299eb2ff-d355-419f-daa3-80dd28ea26bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      megfelel\n",
              "1      megfelel\n",
              "2      megfelel\n",
              "3      megfelel\n",
              "4      megfelel\n",
              "         ...   \n",
              "200    megfelel\n",
              "201    megfelel\n",
              "202    megfelel\n",
              "203    megfelel\n",
              "204    megfelel\n",
              "Name: Unnamed: 14385, Length: 205, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K9oEj5UHjCg_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "df[\"coded\"] = LE.fit_transform(df_y2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3B5dWWjxB2yN"
      },
      "outputs": [],
      "source": [
        "df_y.columns=[\"Value\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_b6_UVJN64t",
        "outputId": "fa7c1636-b632-4ccd-993d-7889173ec8ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     205.000000\n",
              "mean     2185.517049\n",
              "std       370.014482\n",
              "min       231.169975\n",
              "25%      2054.040078\n",
              "50%      2243.486861\n",
              "75%      2422.116922\n",
              "max      2739.643728\n",
              "Name: Value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_y.Value.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LDFiWnELjilf"
      },
      "outputs": [],
      "source": [
        "df_y2=df[\"coded\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fo6w0xCWjij9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KOBK9cMiBw2h"
      },
      "outputs": [],
      "source": [
        "def min_max_scaling(series):\n",
        "    return (series - series.min()) / (series.max() - series.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TflhyEeDCnWk",
        "outputId": "b61ae81f-7694-444e-8968-093f3ba190b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231.1699753"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df_y.Value.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "d-MBR9tevMFW"
      },
      "outputs": [],
      "source": [
        "df_y_scaled=min_max_scaling(df_y.Value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vGSDdwGqneG",
        "outputId": "70a0e753-f75e-47f4-c4f2-601de922a34e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.914317\n",
              "1    0.873469\n",
              "2    0.916239\n",
              "3    0.877815\n",
              "4    0.965302\n",
              "Name: Value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df_y_scaled.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UwyEcsOxmiC1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMOTKbN2G3R"
      },
      "source": [
        "##Mentés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t8zcn0gafG-W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fsQObqfegZ7M"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LshFLrqKMVuR"
      },
      "outputs": [],
      "source": [
        "#df_train=pd.read_csv(\"train_200.csv\")\n",
        "#df_test=pd.read_csv(\"test_200.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "x3HJcQ35MWAn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XomafEhUMWAp"
      },
      "source": [
        "### adatbetöltés mondatok betöltése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9w60OcdVMWAp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqbe8kFxgJav"
      },
      "source": [
        "##Tanulás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tRf3_wt9Bekb"
      },
      "outputs": [],
      "source": [
        "__MAXWORD__=len(df_X.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UwmBdEkijzv",
        "outputId": "daecb602-5bd2-47ba-ea0c-5b00f7d2884d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3384"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "__MAXWORD__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "59MTn_Fs3SvW"
      },
      "outputs": [],
      "source": [
        "lstm_size=25   #25 90% körül teljesített \n",
        "max_input_length=__MAXWORD__\n",
        "\n",
        "\n",
        "n_out=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bD2F2akc1TJ5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS2bWgXsz2zc",
        "outputId": "7579cf80-1806-4636-ab27-dd33eb6c0170"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "\n",
        "#print(lens1)\n",
        "n_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "VKzA6vcSCEm-",
        "outputId": "c45e0027-f2c6-4478-bc22-709053582ebd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Value\n",
              "count   205.000000\n",
              "mean   2185.517049\n",
              "std     370.014482\n",
              "min     231.169975\n",
              "25%    2054.040078\n",
              "50%    2243.486861\n",
              "75%    2422.116922\n",
              "max    2739.643728"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbae5934-ea27-44a5-9a38-f0bd4e5d7cf1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>205.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2185.517049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>370.014482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>231.169975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2054.040078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2243.486861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2422.116922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2739.643728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbae5934-ea27-44a5-9a38-f0bd4e5d7cf1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cbae5934-ea27-44a5-9a38-f0bd4e5d7cf1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cbae5934-ea27-44a5-9a38-f0bd4e5d7cf1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df_y.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "B7UMww7mFieO"
      },
      "outputs": [],
      "source": [
        "bins_list=[i*20 for i in range(150)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8u2USy7wPTrB",
        "outputId": "b37b6554-0954-4917-e8d8-07753d3468e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Value\n",
              "0  2524.709798\n",
              "1  2422.244040\n",
              "2  2529.530277\n",
              "3  2433.146990\n",
              "4  2652.605677"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85d3a30f-9b80-4cae-8337-f6c57423a0b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2524.709798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2422.244040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2529.530277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2433.146990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2652.605677</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85d3a30f-9b80-4cae-8337-f6c57423a0b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85d3a30f-9b80-4cae-8337-f6c57423a0b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85d3a30f-9b80-4cae-8337-f6c57423a0b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "df_y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dXmx6l3eFicN"
      },
      "outputs": [],
      "source": [
        "to_big=df_y.Value[df_y.Value>3000].index.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGwptS6DR5d1",
        "outputId": "b56a8734-b0bb-4942-a401-9d66241812f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "to_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "cVrXRp4UE5Qk",
        "outputId": "1b836c82-88a7-4160-f672-00d085b4f0aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fae2c66a5d0>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df_y.hist(bins=bins_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "JMwhuw-yQRZy"
      },
      "outputs": [],
      "source": [
        "for i in to_big:\n",
        "    df_X=df_X.drop(index=i)\n",
        "    df_y=df_y.drop(index=i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9XgbsvsWRhX9"
      },
      "outputs": [],
      "source": [
        "max_y=df_y.Value.max()\n",
        "min_y=df_y.Value.min()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iyuQlwKRw72",
        "outputId": "a6825d74-1023-4a9d-cd56-96a28edf1830"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2739.643728, 231.1699753)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "(max_y,min_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "V3ujsNSQTYe4"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "df_y_minmax=min_max_scaler.fit_transform(pd.array(df_y.Value).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "N2AHvh8HU0JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "q019xqnl-vmH"
      },
      "outputs": [],
      "source": [
        "df_X_minmax=df_X/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "C01xxvPgJ1wP"
      },
      "outputs": [],
      "source": [
        "# start learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "26U6UXRhmSUE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model_name=\"HPLC\"\n",
        "def scheduler(epoch, lr):\n",
        "       #return 0.00003\n",
        "\n",
        "    '''    \n",
        "    maxx=0.00001\n",
        "    minn=0.001\n",
        "    frekvency=3\n",
        "    o=(epoch % frekvency)/frekvency * (maxx-minn)+minn\n",
        "    return o\n",
        "    '''\n",
        "        \n",
        "    if epoch<4000:\n",
        "        return 0.0005  \n",
        "    elif epoch <80:\n",
        "        return 0.0001\n",
        "    elif epoch <85:\n",
        "        return 0.0005\n",
        "    elif epoch <190:\n",
        "        return 0.0001\n",
        "    \n",
        "    \n",
        "    return 0.00001\n",
        "    \n",
        "callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_loss_{loss:.4f}.hdf5\", monitor='loss',\n",
        "                            verbose=1, save_best_only=False, mode='min')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uvvPuVNSxNa7",
        "outputId": "37c8e7d8-6d88-49f6-a1bc-466e39a0f28c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional, Conv1D, MaxPooling1D, Dropout\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.optimizers import Adadelta,Adam,SGD,Adamax,RMSprop\\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error, mean_absolute_error, binary_crossentropy\\nfrom tensorflow.keras import backend as K\\n\\nK.clear_session\\nx= Input(shape=(max_input_length,1,))\\n\\n#conv1=Conv1D(filters=8, kernel_size=2, padding=\\'same\\', activation=\\'relu\\')(embedded_x)\\n#MP=MaxPooling1D(pool_size=1)(conv1)\\nlstm_output= Bidirectional(LSTM(units=lstm_size*3,return_sequences=True,dropout=0.1))(x)  #,return_state=True\\nlstm_output= Bidirectional(LSTM(units=lstm_size*2,return_sequences=True,dropout=0.1))(lstm_output)\\nlstm_output= Bidirectional(LSTM(units=lstm_size,return_sequences=True,dropout=0.1))(lstm_output)\\n\\n\\nlstm_output=Dropout(0.1)(lstm_output)\\n\\nDense_out=Flatten()(lstm_output)\\n#Dense_out= Dense(50, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\\n\\npredictions= Dense(n_out, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\\nmodel=Model(inputs=x, outputs=predictions)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "'''\n",
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional, Conv1D, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD,Adamax,RMSprop\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error, mean_absolute_error, binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,1,))\n",
        "\n",
        "#conv1=Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(embedded_x)\n",
        "#MP=MaxPooling1D(pool_size=1)(conv1)\n",
        "lstm_output= Bidirectional(LSTM(units=lstm_size*3,return_sequences=True,dropout=0.1))(x)  #,return_state=True\n",
        "lstm_output= Bidirectional(LSTM(units=lstm_size*2,return_sequences=True,dropout=0.1))(lstm_output)\n",
        "lstm_output= Bidirectional(LSTM(units=lstm_size,return_sequences=True,dropout=0.1))(lstm_output)\n",
        "\n",
        "\n",
        "lstm_output=Dropout(0.1)(lstm_output)\n",
        "\n",
        "Dense_out=Flatten()(lstm_output)\n",
        "#Dense_out= Dense(50, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\n",
        "\n",
        "predictions= Dense(n_out, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(Dense_out)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW5XdP2VBiiU",
        "outputId": "27720221-292a-4478-a6f6-c2e57903ee75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3384\n"
          ]
        }
      ],
      "source": [
        "print(max_input_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kSk7_6Tt_ssM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input,Dense,Embedding,LSTM,TimeDistributed, Flatten, Bidirectional, Conv1D, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta,Adam,SGD,Adamax,RMSprop\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy,categorical_crossentropy, mean_squared_error, mean_absolute_error, binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session\n",
        "x= Input(shape=(max_input_length,))\n",
        "\n",
        "#conv1=Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(embedded_x)\n",
        "#MP=MaxPooling1D(pool_size=1)(conv1)\n",
        "dense_o=Dense(units=1000,activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(x)\n",
        "dense_o=Dropout(0.02)(dense_o)\n",
        "dense_o=Dense(units=150,activation=\"sigmoid\",kernel_initializer=\"HeNormal\", name=\"vektor\")(dense_o)  # unit = 100 esetén : 0.0042\n",
        "dense_o=Dropout(0.02)(dense_o)\n",
        "dense_o=Dense(units=1000,activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(dense_o)\n",
        "dense_o=Dropout(0.02)(dense_o)\n",
        "\n",
        "\n",
        "\n",
        "predictions= Dense(units=max_input_length, activation=\"sigmoid\",kernel_initializer=\"HeNormal\")(dense_o)\n",
        "model=Model(inputs=x, outputs=predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OPg96E_EHxew"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rK1gsmAC8Wm5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQpADS6gMHmd",
        "outputId": "83579917-57f7-419b-a95d-b48ba588bf56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3384)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              3385000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " vektor (Dense)              (None, 150)               150150    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1000)              151000    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3384)              3387384   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,073,534\n",
            "Trainable params: 7,073,534\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SzJYM8QeJyvI"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP22GO0AoZkD",
        "outputId": "544e0690-857a-4725-da40-b74c61dd4019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'HPLC*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm  HPLC*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JssV4hq6oZih"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ACrVf-TbJYDe"
      },
      "outputs": [],
      "source": [
        "#train_x = np.asarray(xtrain)\n",
        "#train_y = np.asarray(ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jcUtrkpY_Ny-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "RWIM4DP36S2L"
      },
      "outputs": [],
      "source": [
        "# Loss \n",
        "\n",
        "loss =mean_absolute_error #binary_crossentropy  #mean_absolute_error #categorical_crossentropy \n",
        "# Optimizer\n",
        "optimizer = Adam(learning_rate=0.1) #Ízlés szerint...\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=loss,metrics=[\"mean_absolute_error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FK7a0SH9oYM3"
      },
      "outputs": [],
      "source": [
        "def save_model():\n",
        "    _MODEL_TYPE_=\"lstm_hplc\"\n",
        "\n",
        "    model_name=_MODEL_TYPE_.upper()\n",
        "    callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_LOSS_{loss:.5f}_VLOSS_{val_loss:.5f}_ACC_{accuracy:.4f}_VACC_{val_accuracy:.4f}_.hdf5\", monitor='val_binary_accuracy',\n",
        "                            verbose=1, save_best_only=True, mode='max')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ONvuYqbc-8qV",
        "outputId": "24382fb5-94c5-4f28-f4d1-5d02c0835772"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   36.6633  36.6667  36.67  36.6733  36.6767  36.68  36.6833  36.6867  36.69  \\\n",
              "0      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0      0.0    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   36.6933  ...  47.91  47.9133  47.9167  47.92  47.9233  47.9267  47.93  \\\n",
              "0      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "1      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "2      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "3      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "4      0.0  ...    0.0      0.0      0.0    0.0      0.0      0.0    0.0   \n",
              "\n",
              "   47.9333  47.9367  47.94  \n",
              "0      0.0      0.0    0.0  \n",
              "1      0.0      0.0    0.0  \n",
              "2      0.0      0.0    0.0  \n",
              "3      0.0      0.0    0.0  \n",
              "4      0.0      0.0    0.0  \n",
              "\n",
              "[5 rows x 3384 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-180aa0da-8a5f-4280-956c-31e130f621a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>36.6633</th>\n",
              "      <th>36.6667</th>\n",
              "      <th>36.67</th>\n",
              "      <th>36.6733</th>\n",
              "      <th>36.6767</th>\n",
              "      <th>36.68</th>\n",
              "      <th>36.6833</th>\n",
              "      <th>36.6867</th>\n",
              "      <th>36.69</th>\n",
              "      <th>36.6933</th>\n",
              "      <th>...</th>\n",
              "      <th>47.91</th>\n",
              "      <th>47.9133</th>\n",
              "      <th>47.9167</th>\n",
              "      <th>47.92</th>\n",
              "      <th>47.9233</th>\n",
              "      <th>47.9267</th>\n",
              "      <th>47.93</th>\n",
              "      <th>47.9333</th>\n",
              "      <th>47.9367</th>\n",
              "      <th>47.94</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3384 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-180aa0da-8a5f-4280-956c-31e130f621a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-180aa0da-8a5f-4280-956c-31e130f621a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-180aa0da-8a5f-4280-956c-31e130f621a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "df_X_minmax.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wI1SqakKjNO6"
      },
      "outputs": [],
      "source": [
        "#model.load_weights(\"XXXX_0.06212.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yQ8xl2wAVOV",
        "outputId": "131ce65b-7cf0-47c3-aada-6f6df2dcb537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "df_y2[188]=0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuuuxuvLAozz",
        "outputId": "e78cd74e-fa3a-4cdb-e3be-6d53f3099b8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "df_y2[188]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "cN_KulIeAoyW"
      },
      "outputs": [],
      "source": [
        "_MODE_=\"Tanul\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDs1Ey1HAZnw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SYe1udAjRRnM"
      },
      "outputs": [],
      "source": [
        "#_MODE_=\"load\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hAO4BBOFRHpR"
      },
      "outputs": [],
      "source": [
        "if _MODE_==\"load\":\n",
        "    url=\"https://github.com/sipocz/LSTM_HPLC/raw/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\"\n",
        "    !wget $url\n",
        "    fname=\"XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\"\n",
        "    model.load_weights(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0CeUxTwZRr7U"
      },
      "outputs": [],
      "source": [
        "_MODE_=\"Tanul\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op-I834SFpUX",
        "outputId": "a4eab588-41fc-43d5-b10f-24cf45864290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2218 - mean_absolute_error: 0.2218\n",
            "Epoch 1: saving model to HPLC_loss_0.2218.hdf5\n",
            "16/16 [==============================] - 7s 79ms/step - loss: 0.2218 - mean_absolute_error: 0.2218 - val_loss: 0.1097 - val_mean_absolute_error: 0.1097 - lr: 5.0000e-04\n",
            "Epoch 2/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.1116 - mean_absolute_error: 0.1116\n",
            "Epoch 2: saving model to HPLC_loss_0.1104.hdf5\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.1104 - mean_absolute_error: 0.1104 - val_loss: 0.0953 - val_mean_absolute_error: 0.0953 - lr: 5.0000e-04\n",
            "Epoch 3/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1069 - mean_absolute_error: 0.1069\n",
            "Epoch 3: saving model to HPLC_loss_0.1069.hdf5\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.1069 - mean_absolute_error: 0.1069 - val_loss: 0.0977 - val_mean_absolute_error: 0.0977 - lr: 5.0000e-04\n",
            "Epoch 4/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.1021 - mean_absolute_error: 0.1021\n",
            "Epoch 4: saving model to HPLC_loss_0.1026.hdf5\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.1026 - mean_absolute_error: 0.1026 - val_loss: 0.0920 - val_mean_absolute_error: 0.0920 - lr: 5.0000e-04\n",
            "Epoch 5/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0934 - mean_absolute_error: 0.0934\n",
            "Epoch 5: saving model to HPLC_loss_0.0934.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0934 - mean_absolute_error: 0.0934 - val_loss: 0.0907 - val_mean_absolute_error: 0.0907 - lr: 5.0000e-04\n",
            "Epoch 6/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0810 - mean_absolute_error: 0.0810\n",
            "Epoch 6: saving model to HPLC_loss_0.0810.hdf5\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.0810 - mean_absolute_error: 0.0810 - val_loss: 0.0764 - val_mean_absolute_error: 0.0764 - lr: 5.0000e-04\n",
            "Epoch 7/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0735 - mean_absolute_error: 0.0735\n",
            "Epoch 7: saving model to HPLC_loss_0.0726.hdf5\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0726 - mean_absolute_error: 0.0726 - val_loss: 0.0755 - val_mean_absolute_error: 0.0755 - lr: 5.0000e-04\n",
            "Epoch 8/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0669 - mean_absolute_error: 0.0669\n",
            "Epoch 8: saving model to HPLC_loss_0.0669.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0669 - mean_absolute_error: 0.0669 - val_loss: 0.0760 - val_mean_absolute_error: 0.0760 - lr: 5.0000e-04\n",
            "Epoch 9/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0636 - mean_absolute_error: 0.0636\n",
            "Epoch 9: saving model to HPLC_loss_0.0631.hdf5\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0631 - mean_absolute_error: 0.0631 - val_loss: 0.0684 - val_mean_absolute_error: 0.0684 - lr: 5.0000e-04\n",
            "Epoch 10/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0626 - mean_absolute_error: 0.0626\n",
            "Epoch 10: saving model to HPLC_loss_0.0600.hdf5\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0600 - mean_absolute_error: 0.0600 - val_loss: 0.0641 - val_mean_absolute_error: 0.0641 - lr: 5.0000e-04\n",
            "Epoch 11/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0572 - mean_absolute_error: 0.0572\n",
            "Epoch 11: saving model to HPLC_loss_0.0572.hdf5\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.0572 - mean_absolute_error: 0.0572 - val_loss: 0.0625 - val_mean_absolute_error: 0.0625 - lr: 5.0000e-04\n",
            "Epoch 12/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0538 - mean_absolute_error: 0.0538\n",
            "Epoch 12: saving model to HPLC_loss_0.0541.hdf5\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.0541 - mean_absolute_error: 0.0541 - val_loss: 0.0642 - val_mean_absolute_error: 0.0642 - lr: 5.0000e-04\n",
            "Epoch 13/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0514 - mean_absolute_error: 0.0514\n",
            "Epoch 13: saving model to HPLC_loss_0.0513.hdf5\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0513 - mean_absolute_error: 0.0513 - val_loss: 0.0593 - val_mean_absolute_error: 0.0593 - lr: 5.0000e-04\n",
            "Epoch 14/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0480 - mean_absolute_error: 0.0480\n",
            "Epoch 14: saving model to HPLC_loss_0.0474.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0474 - mean_absolute_error: 0.0474 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431 - lr: 5.0000e-04\n",
            "Epoch 15/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0439 - mean_absolute_error: 0.0439\n",
            "Epoch 15: saving model to HPLC_loss_0.0439.hdf5\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0439 - mean_absolute_error: 0.0439 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 5.0000e-04\n",
            "Epoch 16/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0412 - mean_absolute_error: 0.0412\n",
            "Epoch 16: saving model to HPLC_loss_0.0412.hdf5\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 0.0412 - mean_absolute_error: 0.0412 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461 - lr: 5.0000e-04\n",
            "Epoch 17/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0402 - mean_absolute_error: 0.0402\n",
            "Epoch 17: saving model to HPLC_loss_0.0402.hdf5\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0402 - mean_absolute_error: 0.0402 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330 - lr: 5.0000e-04\n",
            "Epoch 18/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0385 - mean_absolute_error: 0.0385\n",
            "Epoch 18: saving model to HPLC_loss_0.0385.hdf5\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0395 - val_mean_absolute_error: 0.0395 - lr: 5.0000e-04\n",
            "Epoch 19/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0378 - mean_absolute_error: 0.0378\n",
            "Epoch 19: saving model to HPLC_loss_0.0380.hdf5\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330 - lr: 5.0000e-04\n",
            "Epoch 20/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
            "Epoch 20: saving model to HPLC_loss_0.0360.hdf5\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0305 - val_mean_absolute_error: 0.0305 - lr: 5.0000e-04\n",
            "Epoch 21/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
            "Epoch 21: saving model to HPLC_loss_0.0356.hdf5\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0277 - val_mean_absolute_error: 0.0277 - lr: 5.0000e-04\n",
            "Epoch 22/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0343 - mean_absolute_error: 0.0343\n",
            "Epoch 22: saving model to HPLC_loss_0.0339.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0331 - val_mean_absolute_error: 0.0331 - lr: 5.0000e-04\n",
            "Epoch 23/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
            "Epoch 23: saving model to HPLC_loss_0.0332.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0271 - val_mean_absolute_error: 0.0271 - lr: 5.0000e-04\n",
            "Epoch 24/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
            "Epoch 24: saving model to HPLC_loss_0.0323.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0323 - mean_absolute_error: 0.0323 - val_loss: 0.0252 - val_mean_absolute_error: 0.0252 - lr: 5.0000e-04\n",
            "Epoch 25/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
            "Epoch 25: saving model to HPLC_loss_0.0318.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0253 - val_mean_absolute_error: 0.0253 - lr: 5.0000e-04\n",
            "Epoch 26/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
            "Epoch 26: saving model to HPLC_loss_0.0321.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0259 - val_mean_absolute_error: 0.0259 - lr: 5.0000e-04\n",
            "Epoch 27/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
            "Epoch 27: saving model to HPLC_loss_0.0309.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0244 - val_mean_absolute_error: 0.0244 - lr: 5.0000e-04\n",
            "Epoch 28/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0300 - mean_absolute_error: 0.0300\n",
            "Epoch 28: saving model to HPLC_loss_0.0304.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0304 - mean_absolute_error: 0.0304 - val_loss: 0.0247 - val_mean_absolute_error: 0.0247 - lr: 5.0000e-04\n",
            "Epoch 29/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
            "Epoch 29: saving model to HPLC_loss_0.0295.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0236 - val_mean_absolute_error: 0.0236 - lr: 5.0000e-04\n",
            "Epoch 30/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
            "Epoch 30: saving model to HPLC_loss_0.0297.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0219 - val_mean_absolute_error: 0.0219 - lr: 5.0000e-04\n",
            "Epoch 31/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
            "Epoch 31: saving model to HPLC_loss_0.0298.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0233 - val_mean_absolute_error: 0.0233 - lr: 5.0000e-04\n",
            "Epoch 32/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0279 - mean_absolute_error: 0.0279\n",
            "Epoch 32: saving model to HPLC_loss_0.0285.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0285 - mean_absolute_error: 0.0285 - val_loss: 0.0221 - val_mean_absolute_error: 0.0221 - lr: 5.0000e-04\n",
            "Epoch 33/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0267 - mean_absolute_error: 0.0267\n",
            "Epoch 33: saving model to HPLC_loss_0.0279.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0279 - mean_absolute_error: 0.0279 - val_loss: 0.0216 - val_mean_absolute_error: 0.0216 - lr: 5.0000e-04\n",
            "Epoch 34/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0275 - mean_absolute_error: 0.0275\n",
            "Epoch 34: saving model to HPLC_loss_0.0279.hdf5\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0279 - mean_absolute_error: 0.0279 - val_loss: 0.0219 - val_mean_absolute_error: 0.0219 - lr: 5.0000e-04\n",
            "Epoch 35/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0281 - mean_absolute_error: 0.0281\n",
            "Epoch 35: saving model to HPLC_loss_0.0274.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0274 - mean_absolute_error: 0.0274 - val_loss: 0.0210 - val_mean_absolute_error: 0.0210 - lr: 5.0000e-04\n",
            "Epoch 36/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0270 - mean_absolute_error: 0.0270\n",
            "Epoch 36: saving model to HPLC_loss_0.0271.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0271 - mean_absolute_error: 0.0271 - val_loss: 0.0220 - val_mean_absolute_error: 0.0220 - lr: 5.0000e-04\n",
            "Epoch 37/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0269 - mean_absolute_error: 0.0269\n",
            "Epoch 37: saving model to HPLC_loss_0.0270.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0270 - mean_absolute_error: 0.0270 - val_loss: 0.0208 - val_mean_absolute_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 38/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0267 - mean_absolute_error: 0.0267\n",
            "Epoch 38: saving model to HPLC_loss_0.0270.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0270 - mean_absolute_error: 0.0270 - val_loss: 0.0215 - val_mean_absolute_error: 0.0215 - lr: 5.0000e-04\n",
            "Epoch 39/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0275 - mean_absolute_error: 0.0275\n",
            "Epoch 39: saving model to HPLC_loss_0.0268.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0268 - mean_absolute_error: 0.0268 - val_loss: 0.0208 - val_mean_absolute_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 40/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0257 - mean_absolute_error: 0.0257\n",
            "Epoch 40: saving model to HPLC_loss_0.0263.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - val_loss: 0.0207 - val_mean_absolute_error: 0.0207 - lr: 5.0000e-04\n",
            "Epoch 41/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0252 - mean_absolute_error: 0.0252\n",
            "Epoch 41: saving model to HPLC_loss_0.0256.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0256 - mean_absolute_error: 0.0256 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201 - lr: 5.0000e-04\n",
            "Epoch 42/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0254 - mean_absolute_error: 0.0254\n",
            "Epoch 42: saving model to HPLC_loss_0.0254.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0254 - mean_absolute_error: 0.0254 - val_loss: 0.0214 - val_mean_absolute_error: 0.0214 - lr: 5.0000e-04\n",
            "Epoch 43/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244\n",
            "Epoch 43: saving model to HPLC_loss_0.0254.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0254 - mean_absolute_error: 0.0254 - val_loss: 0.0228 - val_mean_absolute_error: 0.0228 - lr: 5.0000e-04\n",
            "Epoch 44/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0255 - mean_absolute_error: 0.0255\n",
            "Epoch 44: saving model to HPLC_loss_0.0257.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0257 - mean_absolute_error: 0.0257 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201 - lr: 5.0000e-04\n",
            "Epoch 45/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243\n",
            "Epoch 45: saving model to HPLC_loss_0.0248.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0248 - mean_absolute_error: 0.0248 - val_loss: 0.0197 - val_mean_absolute_error: 0.0197 - lr: 5.0000e-04\n",
            "Epoch 46/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0251 - mean_absolute_error: 0.0251\n",
            "Epoch 46: saving model to HPLC_loss_0.0247.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0247 - mean_absolute_error: 0.0247 - val_loss: 0.0199 - val_mean_absolute_error: 0.0199 - lr: 5.0000e-04\n",
            "Epoch 47/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244\n",
            "Epoch 47: saving model to HPLC_loss_0.0244.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - val_loss: 0.0189 - val_mean_absolute_error: 0.0189 - lr: 5.0000e-04\n",
            "Epoch 48/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0248 - mean_absolute_error: 0.0248\n",
            "Epoch 48: saving model to HPLC_loss_0.0240.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0240 - mean_absolute_error: 0.0240 - val_loss: 0.0206 - val_mean_absolute_error: 0.0206 - lr: 5.0000e-04\n",
            "Epoch 49/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246\n",
            "Epoch 49: saving model to HPLC_loss_0.0245.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - val_loss: 0.0220 - val_mean_absolute_error: 0.0220 - lr: 5.0000e-04\n",
            "Epoch 50/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0232 - mean_absolute_error: 0.0232\n",
            "Epoch 50: saving model to HPLC_loss_0.0247.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0247 - mean_absolute_error: 0.0247 - val_loss: 0.0215 - val_mean_absolute_error: 0.0215 - lr: 5.0000e-04\n",
            "Epoch 51/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0239 - mean_absolute_error: 0.0239\n",
            "Epoch 51: saving model to HPLC_loss_0.0244.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - val_loss: 0.0212 - val_mean_absolute_error: 0.0212 - lr: 5.0000e-04\n",
            "Epoch 52/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0240 - mean_absolute_error: 0.0240\n",
            "Epoch 52: saving model to HPLC_loss_0.0237.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0237 - mean_absolute_error: 0.0237 - val_loss: 0.0206 - val_mean_absolute_error: 0.0206 - lr: 5.0000e-04\n",
            "Epoch 53/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0239 - mean_absolute_error: 0.0239\n",
            "Epoch 53: saving model to HPLC_loss_0.0230.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0230 - mean_absolute_error: 0.0230 - val_loss: 0.0185 - val_mean_absolute_error: 0.0185 - lr: 5.0000e-04\n",
            "Epoch 54/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0240 - mean_absolute_error: 0.0240\n",
            "Epoch 54: saving model to HPLC_loss_0.0237.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0237 - mean_absolute_error: 0.0237 - val_loss: 0.0199 - val_mean_absolute_error: 0.0199 - lr: 5.0000e-04\n",
            "Epoch 55/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0234 - mean_absolute_error: 0.0234\n",
            "Epoch 55: saving model to HPLC_loss_0.0238.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0238 - mean_absolute_error: 0.0238 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243 - lr: 5.0000e-04\n",
            "Epoch 56/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0224 - mean_absolute_error: 0.0224\n",
            "Epoch 56: saving model to HPLC_loss_0.0227.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0227 - mean_absolute_error: 0.0227 - val_loss: 0.0202 - val_mean_absolute_error: 0.0202 - lr: 5.0000e-04\n",
            "Epoch 57/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0221 - mean_absolute_error: 0.0221\n",
            "Epoch 57: saving model to HPLC_loss_0.0229.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0229 - mean_absolute_error: 0.0229 - val_loss: 0.0191 - val_mean_absolute_error: 0.0191 - lr: 5.0000e-04\n",
            "Epoch 58/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0227 - mean_absolute_error: 0.0227\n",
            "Epoch 58: saving model to HPLC_loss_0.0226.hdf5\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0226 - mean_absolute_error: 0.0226 - val_loss: 0.0204 - val_mean_absolute_error: 0.0204 - lr: 5.0000e-04\n",
            "Epoch 59/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0234 - mean_absolute_error: 0.0234\n",
            "Epoch 59: saving model to HPLC_loss_0.0227.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0227 - mean_absolute_error: 0.0227 - val_loss: 0.0201 - val_mean_absolute_error: 0.0201 - lr: 5.0000e-04\n",
            "Epoch 60/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0229 - mean_absolute_error: 0.0229\n",
            "Epoch 60: saving model to HPLC_loss_0.0227.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0227 - mean_absolute_error: 0.0227 - val_loss: 0.0182 - val_mean_absolute_error: 0.0182 - lr: 5.0000e-04\n",
            "Epoch 61/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0228 - mean_absolute_error: 0.0228\n",
            "Epoch 61: saving model to HPLC_loss_0.0225.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0225 - mean_absolute_error: 0.0225 - val_loss: 0.0191 - val_mean_absolute_error: 0.0191 - lr: 5.0000e-04\n",
            "Epoch 62/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0227 - mean_absolute_error: 0.0227\n",
            "Epoch 62: saving model to HPLC_loss_0.0221.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0221 - mean_absolute_error: 0.0221 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178 - lr: 5.0000e-04\n",
            "Epoch 63/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0221 - mean_absolute_error: 0.0221\n",
            "Epoch 63: saving model to HPLC_loss_0.0218.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0218 - mean_absolute_error: 0.0218 - val_loss: 0.0186 - val_mean_absolute_error: 0.0186 - lr: 5.0000e-04\n",
            "Epoch 64/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0218 - mean_absolute_error: 0.0218\n",
            "Epoch 64: saving model to HPLC_loss_0.0221.hdf5\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0221 - mean_absolute_error: 0.0221 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256 - lr: 5.0000e-04\n",
            "Epoch 65/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0241 - mean_absolute_error: 0.0241\n",
            "Epoch 65: saving model to HPLC_loss_0.0239.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0239 - mean_absolute_error: 0.0239 - val_loss: 0.0204 - val_mean_absolute_error: 0.0204 - lr: 5.0000e-04\n",
            "Epoch 66/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0234 - mean_absolute_error: 0.0234\n",
            "Epoch 66: saving model to HPLC_loss_0.0227.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0227 - mean_absolute_error: 0.0227 - val_loss: 0.0193 - val_mean_absolute_error: 0.0193 - lr: 5.0000e-04\n",
            "Epoch 67/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0219 - mean_absolute_error: 0.0219\n",
            "Epoch 67: saving model to HPLC_loss_0.0219.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0219 - mean_absolute_error: 0.0219 - val_loss: 0.0173 - val_mean_absolute_error: 0.0173 - lr: 5.0000e-04\n",
            "Epoch 68/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0215 - mean_absolute_error: 0.0215\n",
            "Epoch 68: saving model to HPLC_loss_0.0216.hdf5\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.0216 - mean_absolute_error: 0.0216 - val_loss: 0.0185 - val_mean_absolute_error: 0.0185 - lr: 5.0000e-04\n",
            "Epoch 69/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0206 - mean_absolute_error: 0.0206\n",
            "Epoch 69: saving model to HPLC_loss_0.0215.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0215 - mean_absolute_error: 0.0215 - val_loss: 0.0170 - val_mean_absolute_error: 0.0170 - lr: 5.0000e-04\n",
            "Epoch 70/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0213 - mean_absolute_error: 0.0213\n",
            "Epoch 70: saving model to HPLC_loss_0.0209.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0209 - mean_absolute_error: 0.0209 - val_loss: 0.0205 - val_mean_absolute_error: 0.0205 - lr: 5.0000e-04\n",
            "Epoch 71/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0202 - mean_absolute_error: 0.0202\n",
            "Epoch 71: saving model to HPLC_loss_0.0210.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0210 - mean_absolute_error: 0.0210 - val_loss: 0.0161 - val_mean_absolute_error: 0.0161 - lr: 5.0000e-04\n",
            "Epoch 72/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0190 - mean_absolute_error: 0.0190\n",
            "Epoch 72: saving model to HPLC_loss_0.0202.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0202 - mean_absolute_error: 0.0202 - val_loss: 0.0174 - val_mean_absolute_error: 0.0174 - lr: 5.0000e-04\n",
            "Epoch 73/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0210 - mean_absolute_error: 0.0210\n",
            "Epoch 73: saving model to HPLC_loss_0.0210.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0210 - mean_absolute_error: 0.0210 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178 - lr: 5.0000e-04\n",
            "Epoch 74/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0205 - mean_absolute_error: 0.0205\n",
            "Epoch 74: saving model to HPLC_loss_0.0213.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0213 - mean_absolute_error: 0.0213 - val_loss: 0.0163 - val_mean_absolute_error: 0.0163 - lr: 5.0000e-04\n",
            "Epoch 75/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0211 - mean_absolute_error: 0.0211\n",
            "Epoch 75: saving model to HPLC_loss_0.0208.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0208 - mean_absolute_error: 0.0208 - val_loss: 0.0210 - val_mean_absolute_error: 0.0210 - lr: 5.0000e-04\n",
            "Epoch 76/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0213 - mean_absolute_error: 0.0213\n",
            "Epoch 76: saving model to HPLC_loss_0.0211.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0211 - mean_absolute_error: 0.0211 - val_loss: 0.0203 - val_mean_absolute_error: 0.0203 - lr: 5.0000e-04\n",
            "Epoch 77/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0211 - mean_absolute_error: 0.0211\n",
            "Epoch 77: saving model to HPLC_loss_0.0211.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0211 - mean_absolute_error: 0.0211 - val_loss: 0.0198 - val_mean_absolute_error: 0.0198 - lr: 5.0000e-04\n",
            "Epoch 78/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0222 - mean_absolute_error: 0.0222\n",
            "Epoch 78: saving model to HPLC_loss_0.0219.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0219 - mean_absolute_error: 0.0219 - val_loss: 0.0176 - val_mean_absolute_error: 0.0176 - lr: 5.0000e-04\n",
            "Epoch 79/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0207 - mean_absolute_error: 0.0207\n",
            "Epoch 79: saving model to HPLC_loss_0.0207.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0207 - mean_absolute_error: 0.0207 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154 - lr: 5.0000e-04\n",
            "Epoch 80/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0204 - mean_absolute_error: 0.0204\n",
            "Epoch 80: saving model to HPLC_loss_0.0199.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0199 - mean_absolute_error: 0.0199 - val_loss: 0.0163 - val_mean_absolute_error: 0.0163 - lr: 5.0000e-04\n",
            "Epoch 81/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0199 - mean_absolute_error: 0.0199\n",
            "Epoch 81: saving model to HPLC_loss_0.0195.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0195 - mean_absolute_error: 0.0195 - val_loss: 0.0166 - val_mean_absolute_error: 0.0166 - lr: 5.0000e-04\n",
            "Epoch 82/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0191 - mean_absolute_error: 0.0191\n",
            "Epoch 82: saving model to HPLC_loss_0.0194.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0194 - mean_absolute_error: 0.0194 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148 - lr: 5.0000e-04\n",
            "Epoch 83/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0200 - mean_absolute_error: 0.0200\n",
            "Epoch 83: saving model to HPLC_loss_0.0200.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0200 - mean_absolute_error: 0.0200 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178 - lr: 5.0000e-04\n",
            "Epoch 84/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0198 - mean_absolute_error: 0.0198\n",
            "Epoch 84: saving model to HPLC_loss_0.0198.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0198 - mean_absolute_error: 0.0198 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154 - lr: 5.0000e-04\n",
            "Epoch 85/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0191 - mean_absolute_error: 0.0191\n",
            "Epoch 85: saving model to HPLC_loss_0.0189.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0189 - mean_absolute_error: 0.0189 - val_loss: 0.0153 - val_mean_absolute_error: 0.0153 - lr: 5.0000e-04\n",
            "Epoch 86/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0196 - mean_absolute_error: 0.0196\n",
            "Epoch 86: saving model to HPLC_loss_0.0195.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0195 - mean_absolute_error: 0.0195 - val_loss: 0.0199 - val_mean_absolute_error: 0.0199 - lr: 5.0000e-04\n",
            "Epoch 87/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0203 - mean_absolute_error: 0.0203\n",
            "Epoch 87: saving model to HPLC_loss_0.0199.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0199 - mean_absolute_error: 0.0199 - val_loss: 0.0179 - val_mean_absolute_error: 0.0179 - lr: 5.0000e-04\n",
            "Epoch 88/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0187 - mean_absolute_error: 0.0187\n",
            "Epoch 88: saving model to HPLC_loss_0.0189.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0189 - mean_absolute_error: 0.0189 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - lr: 5.0000e-04\n",
            "Epoch 89/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0196 - mean_absolute_error: 0.0196\n",
            "Epoch 89: saving model to HPLC_loss_0.0199.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0199 - mean_absolute_error: 0.0199 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - lr: 5.0000e-04\n",
            "Epoch 90/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0185 - mean_absolute_error: 0.0185\n",
            "Epoch 90: saving model to HPLC_loss_0.0184.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0184 - mean_absolute_error: 0.0184 - val_loss: 0.0161 - val_mean_absolute_error: 0.0161 - lr: 5.0000e-04\n",
            "Epoch 91/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0190 - mean_absolute_error: 0.0190\n",
            "Epoch 91: saving model to HPLC_loss_0.0189.hdf5\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.0189 - mean_absolute_error: 0.0189 - val_loss: 0.0149 - val_mean_absolute_error: 0.0149 - lr: 5.0000e-04\n",
            "Epoch 92/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0179 - mean_absolute_error: 0.0179\n",
            "Epoch 92: saving model to HPLC_loss_0.0189.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0189 - mean_absolute_error: 0.0189 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147 - lr: 5.0000e-04\n",
            "Epoch 93/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0196 - mean_absolute_error: 0.0196\n",
            "Epoch 93: saving model to HPLC_loss_0.0192.hdf5\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0192 - mean_absolute_error: 0.0192 - val_loss: 0.0192 - val_mean_absolute_error: 0.0192 - lr: 5.0000e-04\n",
            "Epoch 94/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0189 - mean_absolute_error: 0.0189\n",
            "Epoch 94: saving model to HPLC_loss_0.0190.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0190 - mean_absolute_error: 0.0190 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164 - lr: 5.0000e-04\n",
            "Epoch 95/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0179 - mean_absolute_error: 0.0179\n",
            "Epoch 95: saving model to HPLC_loss_0.0179.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - lr: 5.0000e-04\n",
            "Epoch 96/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0178 - mean_absolute_error: 0.0178\n",
            "Epoch 96: saving model to HPLC_loss_0.0179.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - lr: 5.0000e-04\n",
            "Epoch 97/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0180 - mean_absolute_error: 0.0180\n",
            "Epoch 97: saving model to HPLC_loss_0.0178.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0178 - mean_absolute_error: 0.0178 - val_loss: 0.0156 - val_mean_absolute_error: 0.0156 - lr: 5.0000e-04\n",
            "Epoch 98/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0179 - mean_absolute_error: 0.0179\n",
            "Epoch 98: saving model to HPLC_loss_0.0179.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0157 - val_mean_absolute_error: 0.0157 - lr: 5.0000e-04\n",
            "Epoch 99/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0176 - mean_absolute_error: 0.0176\n",
            "Epoch 99: saving model to HPLC_loss_0.0176.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0149 - val_mean_absolute_error: 0.0149 - lr: 5.0000e-04\n",
            "Epoch 100/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0170 - mean_absolute_error: 0.0170\n",
            "Epoch 100: saving model to HPLC_loss_0.0177.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0177 - mean_absolute_error: 0.0177 - val_loss: 0.0173 - val_mean_absolute_error: 0.0173 - lr: 5.0000e-04\n",
            "Epoch 101/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0180 - mean_absolute_error: 0.0180\n",
            "Epoch 101: saving model to HPLC_loss_0.0176.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 102/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0176 - mean_absolute_error: 0.0176\n",
            "Epoch 102: saving model to HPLC_loss_0.0176.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - lr: 5.0000e-04\n",
            "Epoch 103/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0175 - mean_absolute_error: 0.0175\n",
            "Epoch 103: saving model to HPLC_loss_0.0175.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0175 - mean_absolute_error: 0.0175 - val_loss: 0.0161 - val_mean_absolute_error: 0.0161 - lr: 5.0000e-04\n",
            "Epoch 104/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0177 - mean_absolute_error: 0.0177\n",
            "Epoch 104: saving model to HPLC_loss_0.0176.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0176 - mean_absolute_error: 0.0176 - val_loss: 0.0159 - val_mean_absolute_error: 0.0159 - lr: 5.0000e-04\n",
            "Epoch 105/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0172 - mean_absolute_error: 0.0172\n",
            "Epoch 105: saving model to HPLC_loss_0.0180.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0180 - mean_absolute_error: 0.0180 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142 - lr: 5.0000e-04\n",
            "Epoch 106/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0174 - mean_absolute_error: 0.0174\n",
            "Epoch 106: saving model to HPLC_loss_0.0170.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - lr: 5.0000e-04\n",
            "Epoch 107/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0169 - mean_absolute_error: 0.0169\n",
            "Epoch 107: saving model to HPLC_loss_0.0170.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0170 - mean_absolute_error: 0.0170 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - lr: 5.0000e-04\n",
            "Epoch 108/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0168 - mean_absolute_error: 0.0168\n",
            "Epoch 108: saving model to HPLC_loss_0.0164.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0164 - mean_absolute_error: 0.0164 - val_loss: 0.0169 - val_mean_absolute_error: 0.0169 - lr: 5.0000e-04\n",
            "Epoch 109/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0161 - mean_absolute_error: 0.0161\n",
            "Epoch 109: saving model to HPLC_loss_0.0163.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0163 - mean_absolute_error: 0.0163 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - lr: 5.0000e-04\n",
            "Epoch 110/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0169 - mean_absolute_error: 0.0169\n",
            "Epoch 110: saving model to HPLC_loss_0.0165.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0165 - mean_absolute_error: 0.0165 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142 - lr: 5.0000e-04\n",
            "Epoch 111/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0161 - mean_absolute_error: 0.0161\n",
            "Epoch 111: saving model to HPLC_loss_0.0161.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0161 - mean_absolute_error: 0.0161 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - lr: 5.0000e-04\n",
            "Epoch 112/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0170 - mean_absolute_error: 0.0170\n",
            "Epoch 112: saving model to HPLC_loss_0.0166.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0166 - mean_absolute_error: 0.0166 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152 - lr: 5.0000e-04\n",
            "Epoch 113/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0158 - mean_absolute_error: 0.0158\n",
            "Epoch 113: saving model to HPLC_loss_0.0164.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0164 - mean_absolute_error: 0.0164 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 114/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.0159\n",
            "Epoch 114: saving model to HPLC_loss_0.0161.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0161 - mean_absolute_error: 0.0161 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - lr: 5.0000e-04\n",
            "Epoch 115/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.0159\n",
            "Epoch 115: saving model to HPLC_loss_0.0157.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - lr: 5.0000e-04\n",
            "Epoch 116/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0157 - mean_absolute_error: 0.0157\n",
            "Epoch 116: saving model to HPLC_loss_0.0157.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142 - lr: 5.0000e-04\n",
            "Epoch 117/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0160 - mean_absolute_error: 0.0160\n",
            "Epoch 117: saving model to HPLC_loss_0.0160.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0160 - mean_absolute_error: 0.0160 - val_loss: 0.0149 - val_mean_absolute_error: 0.0149 - lr: 5.0000e-04\n",
            "Epoch 118/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.0159\n",
            "Epoch 118: saving model to HPLC_loss_0.0155.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 119/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0151 - mean_absolute_error: 0.0151\n",
            "Epoch 119: saving model to HPLC_loss_0.0151.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0151 - mean_absolute_error: 0.0151 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - lr: 5.0000e-04\n",
            "Epoch 120/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0144 - mean_absolute_error: 0.0144\n",
            "Epoch 120: saving model to HPLC_loss_0.0149.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - lr: 5.0000e-04\n",
            "Epoch 121/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0148 - mean_absolute_error: 0.0148\n",
            "Epoch 121: saving model to HPLC_loss_0.0146.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0146 - mean_absolute_error: 0.0146 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - lr: 5.0000e-04\n",
            "Epoch 122/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0147 - mean_absolute_error: 0.0147\n",
            "Epoch 122: saving model to HPLC_loss_0.0148.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0148 - mean_absolute_error: 0.0148 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 123/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 123: saving model to HPLC_loss_0.0147.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0147 - mean_absolute_error: 0.0147 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148 - lr: 5.0000e-04\n",
            "Epoch 124/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0152 - mean_absolute_error: 0.0152\n",
            "Epoch 124: saving model to HPLC_loss_0.0152.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0152 - mean_absolute_error: 0.0152 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 125/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0168 - mean_absolute_error: 0.0168\n",
            "Epoch 125: saving model to HPLC_loss_0.0160.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0160 - mean_absolute_error: 0.0160 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - lr: 5.0000e-04\n",
            "Epoch 126/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0155 - mean_absolute_error: 0.0155\n",
            "Epoch 126: saving model to HPLC_loss_0.0150.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0150 - mean_absolute_error: 0.0150 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - lr: 5.0000e-04\n",
            "Epoch 127/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0149 - mean_absolute_error: 0.0149\n",
            "Epoch 127: saving model to HPLC_loss_0.0150.hdf5\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0150 - mean_absolute_error: 0.0150 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 128/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
            "Epoch 128: saving model to HPLC_loss_0.0141.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0141 - mean_absolute_error: 0.0141 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 129/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 129: saving model to HPLC_loss_0.0143.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 130/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 130: saving model to HPLC_loss_0.0142.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 131/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0138 - mean_absolute_error: 0.0138\n",
            "Epoch 131: saving model to HPLC_loss_0.0142.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - lr: 5.0000e-04\n",
            "Epoch 132/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0150 - mean_absolute_error: 0.0150\n",
            "Epoch 132: saving model to HPLC_loss_0.0150.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0150 - mean_absolute_error: 0.0150 - val_loss: 0.0158 - val_mean_absolute_error: 0.0158 - lr: 5.0000e-04\n",
            "Epoch 133/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0151 - mean_absolute_error: 0.0151\n",
            "Epoch 133: saving model to HPLC_loss_0.0148.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0148 - mean_absolute_error: 0.0148 - val_loss: 0.0203 - val_mean_absolute_error: 0.0203 - lr: 5.0000e-04\n",
            "Epoch 134/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0160 - mean_absolute_error: 0.0160\n",
            "Epoch 134: saving model to HPLC_loss_0.0157.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0142 - val_mean_absolute_error: 0.0142 - lr: 5.0000e-04\n",
            "Epoch 135/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0144 - mean_absolute_error: 0.0144\n",
            "Epoch 135: saving model to HPLC_loss_0.0142.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 136/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0139 - mean_absolute_error: 0.0139\n",
            "Epoch 136: saving model to HPLC_loss_0.0136.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148 - lr: 5.0000e-04\n",
            "Epoch 137/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0144 - mean_absolute_error: 0.0144\n",
            "Epoch 137: saving model to HPLC_loss_0.0143.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - lr: 5.0000e-04\n",
            "Epoch 138/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 138: saving model to HPLC_loss_0.0139.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0139 - mean_absolute_error: 0.0139 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 139/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
            "Epoch 139: saving model to HPLC_loss_0.0143.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147 - lr: 5.0000e-04\n",
            "Epoch 140/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
            "Epoch 140: saving model to HPLC_loss_0.0142.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 141/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
            "Epoch 141: saving model to HPLC_loss_0.0136.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - lr: 5.0000e-04\n",
            "Epoch 142/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0132 - mean_absolute_error: 0.0132\n",
            "Epoch 142: saving model to HPLC_loss_0.0135.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0135 - mean_absolute_error: 0.0135 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 143/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0135 - mean_absolute_error: 0.0135\n",
            "Epoch 143: saving model to HPLC_loss_0.0136.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - lr: 5.0000e-04\n",
            "Epoch 144/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
            "Epoch 144: saving model to HPLC_loss_0.0135.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0135 - mean_absolute_error: 0.0135 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - lr: 5.0000e-04\n",
            "Epoch 145/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0134 - mean_absolute_error: 0.0134\n",
            "Epoch 145: saving model to HPLC_loss_0.0136.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 146/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
            "Epoch 146: saving model to HPLC_loss_0.0137.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 147/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0136 - mean_absolute_error: 0.0136\n",
            "Epoch 147: saving model to HPLC_loss_0.0131.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0131 - mean_absolute_error: 0.0131 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - lr: 5.0000e-04\n",
            "Epoch 148/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0132 - mean_absolute_error: 0.0132\n",
            "Epoch 148: saving model to HPLC_loss_0.0132.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0132 - mean_absolute_error: 0.0132 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 149/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0136 - mean_absolute_error: 0.0136\n",
            "Epoch 149: saving model to HPLC_loss_0.0133.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 150/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0127 - mean_absolute_error: 0.0127\n",
            "Epoch 150: saving model to HPLC_loss_0.0129.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0129 - mean_absolute_error: 0.0129 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 151/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0130 - mean_absolute_error: 0.0130\n",
            "Epoch 151: saving model to HPLC_loss_0.0128.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 152/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0128\n",
            "Epoch 152: saving model to HPLC_loss_0.0128.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - lr: 5.0000e-04\n",
            "Epoch 153/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0133 - mean_absolute_error: 0.0133\n",
            "Epoch 153: saving model to HPLC_loss_0.0132.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0132 - mean_absolute_error: 0.0132 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - lr: 5.0000e-04\n",
            "Epoch 154/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0132 - mean_absolute_error: 0.0132\n",
            "Epoch 154: saving model to HPLC_loss_0.0135.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0135 - mean_absolute_error: 0.0135 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160 - lr: 5.0000e-04\n",
            "Epoch 155/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0129\n",
            "Epoch 155: saving model to HPLC_loss_0.0129.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0129 - mean_absolute_error: 0.0129 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 156/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0125 - mean_absolute_error: 0.0125\n",
            "Epoch 156: saving model to HPLC_loss_0.0126.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - lr: 5.0000e-04\n",
            "Epoch 157/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0128\n",
            "Epoch 157: saving model to HPLC_loss_0.0128.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 158/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0129\n",
            "Epoch 158: saving model to HPLC_loss_0.0135.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0135 - mean_absolute_error: 0.0135 - val_loss: 0.0171 - val_mean_absolute_error: 0.0171 - lr: 5.0000e-04\n",
            "Epoch 159/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
            "Epoch 159: saving model to HPLC_loss_0.0136.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0136 - mean_absolute_error: 0.0136 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 160/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0131 - mean_absolute_error: 0.0131\n",
            "Epoch 160: saving model to HPLC_loss_0.0133.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 161/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0138 - mean_absolute_error: 0.0138\n",
            "Epoch 161: saving model to HPLC_loss_0.0133.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - lr: 5.0000e-04\n",
            "Epoch 162/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0125 - mean_absolute_error: 0.0125\n",
            "Epoch 162: saving model to HPLC_loss_0.0125.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0125 - mean_absolute_error: 0.0125 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 163/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 163: saving model to HPLC_loss_0.0121.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - lr: 5.0000e-04\n",
            "Epoch 164/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 164: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - lr: 5.0000e-04\n",
            "Epoch 165/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 165: saving model to HPLC_loss_0.0121.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 166/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 166: saving model to HPLC_loss_0.0121.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - lr: 5.0000e-04\n",
            "Epoch 167/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 167: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - lr: 5.0000e-04\n",
            "Epoch 168/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
            "Epoch 168: saving model to HPLC_loss_0.0122.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150 - lr: 5.0000e-04\n",
            "Epoch 169/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0129\n",
            "Epoch 169: saving model to HPLC_loss_0.0128.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 170/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 170: saving model to HPLC_loss_0.0122.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - lr: 5.0000e-04\n",
            "Epoch 171/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 171: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 172/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 172: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - lr: 5.0000e-04\n",
            "Epoch 173/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 173: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 174/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 174: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 175/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 175: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 176/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
            "Epoch 176: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 177/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 177: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 178/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
            "Epoch 178: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 179/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 179: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - lr: 5.0000e-04\n",
            "Epoch 180/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 180: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 181/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 181: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 182/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 182: saving model to HPLC_loss_0.0114.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0114 - mean_absolute_error: 0.0114 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - lr: 5.0000e-04\n",
            "Epoch 183/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 183: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 184/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0128 - mean_absolute_error: 0.0128\n",
            "Epoch 184: saving model to HPLC_loss_0.0126.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0126 - mean_absolute_error: 0.0126 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - lr: 5.0000e-04\n",
            "Epoch 185/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 185: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - lr: 5.0000e-04\n",
            "Epoch 186/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
            "Epoch 186: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 187/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
            "Epoch 187: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - lr: 5.0000e-04\n",
            "Epoch 188/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0116 - mean_absolute_error: 0.0116\n",
            "Epoch 188: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 189/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 189: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 190/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 190: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - lr: 5.0000e-04\n",
            "Epoch 191/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
            "Epoch 191: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 192/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 192: saving model to HPLC_loss_0.0120.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 193/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 193: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - lr: 5.0000e-04\n",
            "Epoch 194/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 194: saving model to HPLC_loss_0.0122.hdf5\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - lr: 5.0000e-04\n",
            "Epoch 195/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0116 - mean_absolute_error: 0.0116\n",
            "Epoch 195: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 196/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0116 - mean_absolute_error: 0.0116\n",
            "Epoch 196: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 197/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 197: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 198/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 198: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 199/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
            "Epoch 199: saving model to HPLC_loss_0.0117.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0117 - mean_absolute_error: 0.0117 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 200/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0114\n",
            "Epoch 200: saving model to HPLC_loss_0.0115.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - lr: 5.0000e-04\n",
            "Epoch 201/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 201: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - lr: 5.0000e-04\n",
            "Epoch 202/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 202: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 203/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 203: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - lr: 5.0000e-04\n",
            "Epoch 204/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 204: saving model to HPLC_loss_0.0115.hdf5\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 205/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 205: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - lr: 5.0000e-04\n",
            "Epoch 206/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 206: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - lr: 5.0000e-04\n",
            "Epoch 207/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
            "Epoch 207: saving model to HPLC_loss_0.0121.hdf5\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - lr: 5.0000e-04\n",
            "Epoch 208/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 208: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 209/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 209: saving model to HPLC_loss_0.0111.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0111 - mean_absolute_error: 0.0111 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 210/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 210: saving model to HPLC_loss_0.0110.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0110 - mean_absolute_error: 0.0110 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 211/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 211: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - lr: 5.0000e-04\n",
            "Epoch 212/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 212: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - lr: 5.0000e-04\n",
            "Epoch 213/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 213: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - lr: 5.0000e-04\n",
            "Epoch 214/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 214: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - lr: 5.0000e-04\n",
            "Epoch 215/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
            "Epoch 215: saving model to HPLC_loss_0.0118.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0118 - mean_absolute_error: 0.0118 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - lr: 5.0000e-04\n",
            "Epoch 216/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
            "Epoch 216: saving model to HPLC_loss_0.0119.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - lr: 5.0000e-04\n",
            "Epoch 217/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0116 - mean_absolute_error: 0.0116\n",
            "Epoch 217: saving model to HPLC_loss_0.0115.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0115 - mean_absolute_error: 0.0115 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 218/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0113 - mean_absolute_error: 0.0113\n",
            "Epoch 218: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 219/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 219: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - lr: 5.0000e-04\n",
            "Epoch 220/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
            "Epoch 220: saving model to HPLC_loss_0.0124.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 221/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 221: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 222/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0109 - mean_absolute_error: 0.0109\n",
            "Epoch 222: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 223/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0109 - mean_absolute_error: 0.0109\n",
            "Epoch 223: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 224/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0112\n",
            "Epoch 224: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 225/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 225: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 226/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 226: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 227/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 227: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 228/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 228: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 229/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 229: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 230/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 230: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 231/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0110 - mean_absolute_error: 0.0110\n",
            "Epoch 231: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 232/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 232: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 233/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0107\n",
            "Epoch 233: saving model to HPLC_loss_0.0107.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - lr: 5.0000e-04\n",
            "Epoch 234/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
            "Epoch 234: saving model to HPLC_loss_0.0106.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - lr: 5.0000e-04\n",
            "Epoch 235/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0108 - mean_absolute_error: 0.0108\n",
            "Epoch 235: saving model to HPLC_loss_0.0107.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 236/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 236: saving model to HPLC_loss_0.0107.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0107 - mean_absolute_error: 0.0107 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 237/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 237: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 238/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 238: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099 - lr: 5.0000e-04\n",
            "Epoch 239/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 239: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 240/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 240: saving model to HPLC_loss_0.0108.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0108 - mean_absolute_error: 0.0108 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 241/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0109 - mean_absolute_error: 0.0109\n",
            "Epoch 241: saving model to HPLC_loss_0.0109.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 242/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 242: saving model to HPLC_loss_0.0103.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 243/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 243: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 244/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 244: saving model to HPLC_loss_0.0103.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - lr: 5.0000e-04\n",
            "Epoch 245/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
            "Epoch 245: saving model to HPLC_loss_0.0123.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 246/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 246: saving model to HPLC_loss_0.0116.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 247/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 247: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 248/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0106 - mean_absolute_error: 0.0106\n",
            "Epoch 248: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - lr: 5.0000e-04\n",
            "Epoch 249/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 249: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 250/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
            "Epoch 250: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - lr: 5.0000e-04\n",
            "Epoch 251/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 251: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 252/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 252: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0107 - val_mean_absolute_error: 0.0107 - lr: 5.0000e-04\n",
            "Epoch 253/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 253: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 254/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0111\n",
            "Epoch 254: saving model to HPLC_loss_0.0112.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 255/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0115\n",
            "Epoch 255: saving model to HPLC_loss_0.0113.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0113 - mean_absolute_error: 0.0113 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 256/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 256: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 257/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0107\n",
            "Epoch 257: saving model to HPLC_loss_0.0103.hdf5\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 258/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 258: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 259/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 259: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 260/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 260: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 261/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
            "Epoch 261: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 262/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0104\n",
            "Epoch 262: saving model to HPLC_loss_0.0104.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0104 - mean_absolute_error: 0.0104 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 263/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 263: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 264/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 264: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 265/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 265: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 266/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 266: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - lr: 5.0000e-04\n",
            "Epoch 267/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 267: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 268/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 268: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 269/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 269: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - lr: 5.0000e-04\n",
            "Epoch 270/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 270: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 271/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 271: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 272/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 272: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 273/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0102\n",
            "Epoch 273: saving model to HPLC_loss_0.0101.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0101 - mean_absolute_error: 0.0101 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 274/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 274: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 275/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 275: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 276/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 276: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 277/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 277: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 278/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 278: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 279/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 279: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 280/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 280: saving model to HPLC_loss_0.0096.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0096 - mean_absolute_error: 0.0096 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 281/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 281: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 282/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 282: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 283/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0105\n",
            "Epoch 283: saving model to HPLC_loss_0.0105.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0105 - mean_absolute_error: 0.0105 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 284/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 284: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 285/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 285: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 286/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 286: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 287/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 287: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 288/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0097\n",
            "Epoch 288: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 1s 49ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 289/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 289: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 290/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 290: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 291/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 291: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 292/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 292: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 293/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 293: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 294/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 294: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 295/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 295: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 296/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 296: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 297/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 297: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 298/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 298: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 299/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 299: saving model to HPLC_loss_0.0100.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0100 - mean_absolute_error: 0.0100 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 300/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 300: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 301/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0098\n",
            "Epoch 301: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 302/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 302: saving model to HPLC_loss_0.0099.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 303/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.0100\n",
            "Epoch 303: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 304/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 304: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 305/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 305: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 306/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 306: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 307/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 307: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 308/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 308: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 309/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 309: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 310/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 310: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - lr: 5.0000e-04\n",
            "Epoch 311/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 311: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - lr: 5.0000e-04\n",
            "Epoch 312/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 312: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 313/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 313: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 314/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 314: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 315/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0099\n",
            "Epoch 315: saving model to HPLC_loss_0.0098.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0098 - mean_absolute_error: 0.0098 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 316/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 316: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 317/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0096\n",
            "Epoch 317: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 318/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 318: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 319/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 319: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 320/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 320: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - lr: 5.0000e-04\n",
            "Epoch 321/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 321: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 322/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 322: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 323/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 323: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 324/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 324: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 325/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 325: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 326/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 326: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 327/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 327: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 328/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 328: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 329/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 329: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 330/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 330: saving model to HPLC_loss_0.0091.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 331/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 331: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 332/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 332: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 333/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 333: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 334/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0095\n",
            "Epoch 334: saving model to HPLC_loss_0.0095.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0095 - mean_absolute_error: 0.0095 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 335/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 335: saving model to HPLC_loss_0.0102.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 336/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0101\n",
            "Epoch 336: saving model to HPLC_loss_0.0097.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 337/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 337: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - lr: 5.0000e-04\n",
            "Epoch 338/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 338: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 339/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 339: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 340/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 340: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 341/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 341: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 342/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 342: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 343/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0093\n",
            "Epoch 343: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 344/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 344: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 345/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 345: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - lr: 5.0000e-04\n",
            "Epoch 346/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 346: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 347/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 347: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082 - lr: 5.0000e-04\n",
            "Epoch 348/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 348: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080 - lr: 5.0000e-04\n",
            "Epoch 349/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 349: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 350/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 350: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 351/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 351: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090 - lr: 5.0000e-04\n",
            "Epoch 352/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
            "Epoch 352: saving model to HPLC_loss_0.0093.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0093 - mean_absolute_error: 0.0093 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 353/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0094\n",
            "Epoch 353: saving model to HPLC_loss_0.0094.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 354/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
            "Epoch 354: saving model to HPLC_loss_0.0092.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 355/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 355: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0086 - val_mean_absolute_error: 0.0086 - lr: 5.0000e-04\n",
            "Epoch 356/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 356: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - lr: 5.0000e-04\n",
            "Epoch 357/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 357: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 358/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 358: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087 - lr: 5.0000e-04\n",
            "Epoch 359/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 359: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 360/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 360: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 361/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 361: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 362/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 362: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 363/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 363: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 364/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 364: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 365/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 365: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 366/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 366: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082 - lr: 5.0000e-04\n",
            "Epoch 367/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 367: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n",
            "Epoch 368/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 368: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - lr: 5.0000e-04\n",
            "Epoch 369/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 369: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 370/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0089 - mean_absolute_error: 0.0089\n",
            "Epoch 370: saving model to HPLC_loss_0.0089.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 371/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 371: saving model to HPLC_loss_0.0086.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081 - lr: 5.0000e-04\n",
            "Epoch 372/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 372: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 373/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 373: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 374/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 374: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080 - lr: 5.0000e-04\n",
            "Epoch 375/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0079 - mean_absolute_error: 0.0079\n",
            "Epoch 375: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081 - lr: 5.0000e-04\n",
            "Epoch 376/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 376: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 377/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 377: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - lr: 5.0000e-04\n",
            "Epoch 378/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0085\n",
            "Epoch 378: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 1s 30ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 379/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 379: saving model to HPLC_loss_0.0087.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0087 - mean_absolute_error: 0.0087 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - lr: 5.0000e-04\n",
            "Epoch 380/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0088\n",
            "Epoch 380: saving model to HPLC_loss_0.0090.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0090 - mean_absolute_error: 0.0090 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 381/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 381: saving model to HPLC_loss_0.0083.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0083 - mean_absolute_error: 0.0083 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 382/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 382: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 383/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 383: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0083 - val_mean_absolute_error: 0.0083 - lr: 5.0000e-04\n",
            "Epoch 384/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0078 - mean_absolute_error: 0.0078\n",
            "Epoch 384: saving model to HPLC_loss_0.0078.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - lr: 5.0000e-04\n",
            "Epoch 385/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 385: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 386/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 386: saving model to HPLC_loss_0.0085.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0085 - mean_absolute_error: 0.0085 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082 - lr: 5.0000e-04\n",
            "Epoch 387/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0076 - mean_absolute_error: 0.0076\n",
            "Epoch 387: saving model to HPLC_loss_0.0078.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0078 - mean_absolute_error: 0.0078 - val_loss: 0.0088 - val_mean_absolute_error: 0.0088 - lr: 5.0000e-04\n",
            "Epoch 388/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0081 - mean_absolute_error: 0.0081\n",
            "Epoch 388: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 32ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0081 - val_mean_absolute_error: 0.0081 - lr: 5.0000e-04\n",
            "Epoch 389/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0080\n",
            "Epoch 389: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0079 - val_mean_absolute_error: 0.0079 - lr: 5.0000e-04\n",
            "Epoch 390/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0077 - mean_absolute_error: 0.0077\n",
            "Epoch 390: saving model to HPLC_loss_0.0077.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 391/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0078 - mean_absolute_error: 0.0078\n",
            "Epoch 391: saving model to HPLC_loss_0.0080.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0080 - mean_absolute_error: 0.0080 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 392/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0086\n",
            "Epoch 392: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - lr: 5.0000e-04\n",
            "Epoch 393/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 393: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 394/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0082\n",
            "Epoch 394: saving model to HPLC_loss_0.0082.hdf5\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0082 - mean_absolute_error: 0.0082 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 395/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0090\n",
            "Epoch 395: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n",
            "Epoch 396/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0084\n",
            "Epoch 396: saving model to HPLC_loss_0.0084.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0084 - val_mean_absolute_error: 0.0084 - lr: 5.0000e-04\n",
            "Epoch 397/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0087\n",
            "Epoch 397: saving model to HPLC_loss_0.0088.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0088 - mean_absolute_error: 0.0088 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 398/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0083\n",
            "Epoch 398: saving model to HPLC_loss_0.0081.hdf5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0089 - val_mean_absolute_error: 0.0089 - lr: 5.0000e-04\n",
            "Epoch 399/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0077 - mean_absolute_error: 0.0077\n",
            "Epoch 399: saving model to HPLC_loss_0.0077.hdf5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080 - lr: 5.0000e-04\n",
            "Epoch 400/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0078 - mean_absolute_error: 0.0078\n",
            "Epoch 400: saving model to HPLC_loss_0.0079.hdf5\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Illesszük az adatra a modellt\n",
        "if _MODE_==\"Tanul\":\n",
        "    history=model.fit(\n",
        "            x=df_X_minmax,\n",
        "            y=df_X_minmax, \n",
        "            epochs=400, \n",
        "            batch_size=12,\n",
        "            validation_split=0.1,         \n",
        "            callbacks=[callbacks]          \n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rFyo3DvKCD0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Y3yWoor3Hyt4"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    import matplotlib.pyplot as plt\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4eRDUJximCb",
        "outputId": "16b25288-69fa-493e-8658-98d770a928f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.22180095314979553,\n",
              "  0.11041713505983353,\n",
              "  0.10688105970621109,\n",
              "  0.10262878984212875,\n",
              "  0.09344843029975891,\n",
              "  0.08099886029958725,\n",
              "  0.07258384674787521,\n",
              "  0.06686024367809296,\n",
              "  0.06305158138275146,\n",
              "  0.06003473699092865,\n",
              "  0.05722194164991379,\n",
              "  0.05412333086133003,\n",
              "  0.05134054273366928,\n",
              "  0.0473659448325634,\n",
              "  0.04389996454119682,\n",
              "  0.04115842655301094,\n",
              "  0.04017351567745209,\n",
              "  0.03854726627469063,\n",
              "  0.03798629343509674,\n",
              "  0.035996947437524796,\n",
              "  0.0355597622692585,\n",
              "  0.03394671902060509,\n",
              "  0.033201538026332855,\n",
              "  0.03233080729842186,\n",
              "  0.03180932626128197,\n",
              "  0.03206578642129898,\n",
              "  0.03087078221142292,\n",
              "  0.03043411858379841,\n",
              "  0.029524175450205803,\n",
              "  0.02974359318614006,\n",
              "  0.029765691608190536,\n",
              "  0.028464846312999725,\n",
              "  0.027927901595830917,\n",
              "  0.027929197996854782,\n",
              "  0.027396315708756447,\n",
              "  0.027145644649863243,\n",
              "  0.026995891705155373,\n",
              "  0.02699100412428379,\n",
              "  0.026788843795657158,\n",
              "  0.026343882083892822,\n",
              "  0.02556360326707363,\n",
              "  0.025437327101826668,\n",
              "  0.02538718841969967,\n",
              "  0.025683851912617683,\n",
              "  0.024844679981470108,\n",
              "  0.0246971994638443,\n",
              "  0.024358976632356644,\n",
              "  0.024022802710533142,\n",
              "  0.024462807923555374,\n",
              "  0.02470896765589714,\n",
              "  0.024409180507063866,\n",
              "  0.02372067980468273,\n",
              "  0.02303389646112919,\n",
              "  0.023668045178055763,\n",
              "  0.023759841918945312,\n",
              "  0.022696062922477722,\n",
              "  0.02294139936566353,\n",
              "  0.022629069164395332,\n",
              "  0.02273895964026451,\n",
              "  0.02272103913128376,\n",
              "  0.022512877359986305,\n",
              "  0.02210676111280918,\n",
              "  0.021793384104967117,\n",
              "  0.022093545645475388,\n",
              "  0.023885488510131836,\n",
              "  0.02270885370671749,\n",
              "  0.02185283973813057,\n",
              "  0.021604003384709358,\n",
              "  0.021516738459467888,\n",
              "  0.020905612036585808,\n",
              "  0.02098781242966652,\n",
              "  0.02023131586611271,\n",
              "  0.020952964201569557,\n",
              "  0.021260548382997513,\n",
              "  0.02081763930618763,\n",
              "  0.02110731042921543,\n",
              "  0.021117286756634712,\n",
              "  0.02185690402984619,\n",
              "  0.02066170983016491,\n",
              "  0.019891582429409027,\n",
              "  0.019496168941259384,\n",
              "  0.019381891936063766,\n",
              "  0.019964681938290596,\n",
              "  0.019800281152129173,\n",
              "  0.018925784155726433,\n",
              "  0.01945216953754425,\n",
              "  0.019866742193698883,\n",
              "  0.018917612731456757,\n",
              "  0.019920792430639267,\n",
              "  0.01837415061891079,\n",
              "  0.018890073522925377,\n",
              "  0.018863683566451073,\n",
              "  0.019238289445638657,\n",
              "  0.01899099349975586,\n",
              "  0.017862845212221146,\n",
              "  0.017932036891579628,\n",
              "  0.017841029912233353,\n",
              "  0.017875242978334427,\n",
              "  0.017588723450899124,\n",
              "  0.017666161060333252,\n",
              "  0.017639482393860817,\n",
              "  0.017633158713579178,\n",
              "  0.017504585906863213,\n",
              "  0.017578985542058945,\n",
              "  0.01797333173453808,\n",
              "  0.01703876256942749,\n",
              "  0.016971424221992493,\n",
              "  0.01639733836054802,\n",
              "  0.016335612162947655,\n",
              "  0.016542328521609306,\n",
              "  0.016130398958921432,\n",
              "  0.016611307859420776,\n",
              "  0.01641751267015934,\n",
              "  0.016093015670776367,\n",
              "  0.01568274386227131,\n",
              "  0.015718085691332817,\n",
              "  0.015967456623911858,\n",
              "  0.015547964721918106,\n",
              "  0.015101774595677853,\n",
              "  0.014914085157215595,\n",
              "  0.01462546270340681,\n",
              "  0.014819891192018986,\n",
              "  0.014666939154267311,\n",
              "  0.01522743795067072,\n",
              "  0.01603078655898571,\n",
              "  0.015008225105702877,\n",
              "  0.014951424673199654,\n",
              "  0.014082470908761024,\n",
              "  0.014275831170380116,\n",
              "  0.014191689901053905,\n",
              "  0.01417659129947424,\n",
              "  0.015010545961558819,\n",
              "  0.014835039153695107,\n",
              "  0.01569012925028801,\n",
              "  0.014200485311448574,\n",
              "  0.013623549602925777,\n",
              "  0.014263828285038471,\n",
              "  0.013925410807132721,\n",
              "  0.014261968433856964,\n",
              "  0.014179825782775879,\n",
              "  0.013606296852231026,\n",
              "  0.013523402623832226,\n",
              "  0.013640383258461952,\n",
              "  0.01348789781332016,\n",
              "  0.013642923906445503,\n",
              "  0.013715005479753017,\n",
              "  0.013141615316271782,\n",
              "  0.013159831054508686,\n",
              "  0.013321982696652412,\n",
              "  0.012862218543887138,\n",
              "  0.012768222019076347,\n",
              "  0.01275094598531723,\n",
              "  0.013189581222832203,\n",
              "  0.013538127765059471,\n",
              "  0.01293498370796442,\n",
              "  0.012606607750058174,\n",
              "  0.012806838378310204,\n",
              "  0.01352321170270443,\n",
              "  0.0136034544557333,\n",
              "  0.013266372494399548,\n",
              "  0.013335112482309341,\n",
              "  0.012510011903941631,\n",
              "  0.012127396650612354,\n",
              "  0.012418849393725395,\n",
              "  0.012119749560952187,\n",
              "  0.012095307931303978,\n",
              "  0.012320359237492085,\n",
              "  0.012234367430210114,\n",
              "  0.01275542750954628,\n",
              "  0.012199871242046356,\n",
              "  0.011957836337387562,\n",
              "  0.012305207550525665,\n",
              "  0.011946807615458965,\n",
              "  0.011998114176094532,\n",
              "  0.011662094853818417,\n",
              "  0.012036926113069057,\n",
              "  0.011794069781899452,\n",
              "  0.011818988248705864,\n",
              "  0.011722548864781857,\n",
              "  0.011988484300673008,\n",
              "  0.011683880351483822,\n",
              "  0.011381878517568111,\n",
              "  0.011552704498171806,\n",
              "  0.012593725696206093,\n",
              "  0.012422647327184677,\n",
              "  0.012293434701859951,\n",
              "  0.011984053999185562,\n",
              "  0.01162825059145689,\n",
              "  0.011320776306092739,\n",
              "  0.011248363181948662,\n",
              "  0.011932339519262314,\n",
              "  0.01195449847728014,\n",
              "  0.011779004707932472,\n",
              "  0.012183241546154022,\n",
              "  0.011679220013320446,\n",
              "  0.011629951186478138,\n",
              "  0.0110617745667696,\n",
              "  0.011634895578026772,\n",
              "  0.011694937944412231,\n",
              "  0.01146254688501358,\n",
              "  0.01111813634634018,\n",
              "  0.011278891935944557,\n",
              "  0.011051095090806484,\n",
              "  0.011494812555611134,\n",
              "  0.011336938478052616,\n",
              "  0.011328404769301414,\n",
              "  0.012134367600083351,\n",
              "  0.011252615600824356,\n",
              "  0.011085208505392075,\n",
              "  0.01100902073085308,\n",
              "  0.010497870855033398,\n",
              "  0.011247619055211544,\n",
              "  0.011236574500799179,\n",
              "  0.012356801889836788,\n",
              "  0.011772261932492256,\n",
              "  0.011941039003431797,\n",
              "  0.011535047553479671,\n",
              "  0.011271247640252113,\n",
              "  0.011263268068432808,\n",
              "  0.012428770773112774,\n",
              "  0.011329935863614082,\n",
              "  0.010893639177083969,\n",
              "  0.01093000266700983,\n",
              "  0.011192328296601772,\n",
              "  0.01060166023671627,\n",
              "  0.010526303201913834,\n",
              "  0.01041188184171915,\n",
              "  0.010536829009652138,\n",
              "  0.010528760962188244,\n",
              "  0.010583082213997841,\n",
              "  0.01091093197464943,\n",
              "  0.011191400699317455,\n",
              "  0.010735735297203064,\n",
              "  0.010589881800115108,\n",
              "  0.010708126239478588,\n",
              "  0.010675564408302307,\n",
              "  0.010524864308536053,\n",
              "  0.010104511864483356,\n",
              "  0.010182172060012817,\n",
              "  0.01079193502664566,\n",
              "  0.010897275060415268,\n",
              "  0.010299782268702984,\n",
              "  0.010050475597381592,\n",
              "  0.010342089459300041,\n",
              "  0.012307140044867992,\n",
              "  0.01157444715499878,\n",
              "  0.010541399009525776,\n",
              "  0.010429736226797104,\n",
              "  0.010092792101204395,\n",
              "  0.01037603709846735,\n",
              "  0.010118400678038597,\n",
              "  0.01020605955272913,\n",
              "  0.010489329695701599,\n",
              "  0.011223768815398216,\n",
              "  0.011261933483183384,\n",
              "  0.010376526042819023,\n",
              "  0.010286438278853893,\n",
              "  0.010105079971253872,\n",
              "  0.010131360031664371,\n",
              "  0.010062169283628464,\n",
              "  0.01045070681720972,\n",
              "  0.010395831428468227,\n",
              "  0.010138330049812794,\n",
              "  0.009871872141957283,\n",
              "  0.010033946484327316,\n",
              "  0.009585423395037651,\n",
              "  0.009609632194042206,\n",
              "  0.009938386268913746,\n",
              "  0.009612051770091057,\n",
              "  0.009631427936255932,\n",
              "  0.00969899632036686,\n",
              "  0.009855466894805431,\n",
              "  0.01005470659583807,\n",
              "  0.009443648159503937,\n",
              "  0.00945188570767641,\n",
              "  0.009627032093703747,\n",
              "  0.009499575011432171,\n",
              "  0.009565271437168121,\n",
              "  0.009655200876295567,\n",
              "  0.00957200676202774,\n",
              "  0.010008282959461212,\n",
              "  0.01004463154822588,\n",
              "  0.0104757035151124,\n",
              "  0.009841447696089745,\n",
              "  0.009672272950410843,\n",
              "  0.009379609487950802,\n",
              "  0.009665071964263916,\n",
              "  0.009669332765042782,\n",
              "  0.009358690120279789,\n",
              "  0.009393343701958656,\n",
              "  0.008945625275373459,\n",
              "  0.009459398686885834,\n",
              "  0.009115366265177727,\n",
              "  0.009072311222553253,\n",
              "  0.009331698529422283,\n",
              "  0.008957923389971256,\n",
              "  0.00922236405313015,\n",
              "  0.009448418393731117,\n",
              "  0.010030957870185375,\n",
              "  0.009218696504831314,\n",
              "  0.00973012950271368,\n",
              "  0.009888356551527977,\n",
              "  0.009696167893707752,\n",
              "  0.009416562505066395,\n",
              "  0.009090432897210121,\n",
              "  0.00881993118673563,\n",
              "  0.008747891522943974,\n",
              "  0.00900097656995058,\n",
              "  0.009225791320204735,\n",
              "  0.009715053252875805,\n",
              "  0.009221062064170837,\n",
              "  0.009239371865987778,\n",
              "  0.008870585821568966,\n",
              "  0.009044813923537731,\n",
              "  0.009786027483642101,\n",
              "  0.009028903208673,\n",
              "  0.009257898665964603,\n",
              "  0.008703178726136684,\n",
              "  0.008658191189169884,\n",
              "  0.009063013829290867,\n",
              "  0.008912655524909496,\n",
              "  0.00869564525783062,\n",
              "  0.009238813072443008,\n",
              "  0.008910587057471275,\n",
              "  0.008437341079115868,\n",
              "  0.008536798879504204,\n",
              "  0.009245890192687511,\n",
              "  0.009310627356171608,\n",
              "  0.009458712302148342,\n",
              "  0.009065866470336914,\n",
              "  0.008953534997999668,\n",
              "  0.008796784095466137,\n",
              "  0.008646591566503048,\n",
              "  0.009521287865936756,\n",
              "  0.010153443552553654,\n",
              "  0.009749850258231163,\n",
              "  0.009042159654200077,\n",
              "  0.008530213497579098,\n",
              "  0.008898562751710415,\n",
              "  0.00860496424138546,\n",
              "  0.00835052877664566,\n",
              "  0.008859661407768726,\n",
              "  0.009183873422443867,\n",
              "  0.009026010520756245,\n",
              "  0.008779773488640785,\n",
              "  0.008948614820837975,\n",
              "  0.008554929867386818,\n",
              "  0.008404160849750042,\n",
              "  0.008281082846224308,\n",
              "  0.0086979940533638,\n",
              "  0.009219297207891941,\n",
              "  0.009256438352167606,\n",
              "  0.009368493221700191,\n",
              "  0.009209584444761276,\n",
              "  0.008783658966422081,\n",
              "  0.008444070816040039,\n",
              "  0.008550765924155712,\n",
              "  0.008996302261948586,\n",
              "  0.008497640490531921,\n",
              "  0.008191248401999474,\n",
              "  0.008197510614991188,\n",
              "  0.00857556238770485,\n",
              "  0.008259345777332783,\n",
              "  0.0083894869312644,\n",
              "  0.00825468823313713,\n",
              "  0.008195241913199425,\n",
              "  0.008198365569114685,\n",
              "  0.008326586335897446,\n",
              "  0.008269582875072956,\n",
              "  0.00887200515717268,\n",
              "  0.00857496913522482,\n",
              "  0.008460464887320995,\n",
              "  0.008420173078775406,\n",
              "  0.008147593587636948,\n",
              "  0.007977789267897606,\n",
              "  0.008184385485947132,\n",
              "  0.008285793475806713,\n",
              "  0.008427740074694157,\n",
              "  0.00872052926570177,\n",
              "  0.008956827223300934,\n",
              "  0.00830362644046545,\n",
              "  0.008142183534801006,\n",
              "  0.00811169482767582,\n",
              "  0.007847301661968231,\n",
              "  0.008167823776602745,\n",
              "  0.00852600671350956,\n",
              "  0.007761642336845398,\n",
              "  0.008108813315629959,\n",
              "  0.007980864495038986,\n",
              "  0.007730297278612852,\n",
              "  0.007983813993632793,\n",
              "  0.008417296223342419,\n",
              "  0.008176379837095737,\n",
              "  0.008206850849092007,\n",
              "  0.00882851891219616,\n",
              "  0.008402206003665924,\n",
              "  0.008777589537203312,\n",
              "  0.008100515231490135,\n",
              "  0.007698744535446167,\n",
              "  0.007901487872004509],\n",
              " 'lr': [0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005,\n",
              "  0.0005],\n",
              " 'mean_absolute_error': [0.22180095314979553,\n",
              "  0.11041713505983353,\n",
              "  0.10688105970621109,\n",
              "  0.10262878984212875,\n",
              "  0.09344843029975891,\n",
              "  0.08099886029958725,\n",
              "  0.07258384674787521,\n",
              "  0.06686024367809296,\n",
              "  0.06305158138275146,\n",
              "  0.06003473699092865,\n",
              "  0.05722194164991379,\n",
              "  0.05412333086133003,\n",
              "  0.05134054273366928,\n",
              "  0.0473659448325634,\n",
              "  0.04389996454119682,\n",
              "  0.04115842655301094,\n",
              "  0.04017351567745209,\n",
              "  0.03854726627469063,\n",
              "  0.03798629343509674,\n",
              "  0.035996947437524796,\n",
              "  0.0355597622692585,\n",
              "  0.03394671902060509,\n",
              "  0.033201538026332855,\n",
              "  0.03233080729842186,\n",
              "  0.03180932626128197,\n",
              "  0.03206578642129898,\n",
              "  0.03087078221142292,\n",
              "  0.03043411858379841,\n",
              "  0.029524175450205803,\n",
              "  0.02974359318614006,\n",
              "  0.029765691608190536,\n",
              "  0.028464846312999725,\n",
              "  0.027927901595830917,\n",
              "  0.027929197996854782,\n",
              "  0.027396315708756447,\n",
              "  0.027145644649863243,\n",
              "  0.026995891705155373,\n",
              "  0.02699100412428379,\n",
              "  0.026788843795657158,\n",
              "  0.02634388394653797,\n",
              "  0.02556360326707363,\n",
              "  0.025437327101826668,\n",
              "  0.02538718841969967,\n",
              "  0.025683851912617683,\n",
              "  0.024844679981470108,\n",
              "  0.0246971994638443,\n",
              "  0.024358976632356644,\n",
              "  0.024022802710533142,\n",
              "  0.024462807923555374,\n",
              "  0.02470896765589714,\n",
              "  0.024409180507063866,\n",
              "  0.02372067980468273,\n",
              "  0.02303389646112919,\n",
              "  0.023668045178055763,\n",
              "  0.023759841918945312,\n",
              "  0.022696062922477722,\n",
              "  0.02294139936566353,\n",
              "  0.022629069164395332,\n",
              "  0.02273895964026451,\n",
              "  0.02272103913128376,\n",
              "  0.022512877359986305,\n",
              "  0.02210676111280918,\n",
              "  0.021793384104967117,\n",
              "  0.022093545645475388,\n",
              "  0.023885488510131836,\n",
              "  0.02270885370671749,\n",
              "  0.02185283973813057,\n",
              "  0.021604003384709358,\n",
              "  0.021516738459467888,\n",
              "  0.020905612036585808,\n",
              "  0.02098781242966652,\n",
              "  0.02023131586611271,\n",
              "  0.020952964201569557,\n",
              "  0.021260548382997513,\n",
              "  0.02081763744354248,\n",
              "  0.02110731042921543,\n",
              "  0.021117286756634712,\n",
              "  0.02185690402984619,\n",
              "  0.02066170983016491,\n",
              "  0.019891582429409027,\n",
              "  0.019496168941259384,\n",
              "  0.019381891936063766,\n",
              "  0.019964681938290596,\n",
              "  0.019800281152129173,\n",
              "  0.018925784155726433,\n",
              "  0.01945216953754425,\n",
              "  0.019866742193698883,\n",
              "  0.018917612731456757,\n",
              "  0.019920792430639267,\n",
              "  0.01837415248155594,\n",
              "  0.018890073522925377,\n",
              "  0.018863683566451073,\n",
              "  0.019238289445638657,\n",
              "  0.01899099349975586,\n",
              "  0.017862845212221146,\n",
              "  0.017932036891579628,\n",
              "  0.017841029912233353,\n",
              "  0.017875242978334427,\n",
              "  0.017588723450899124,\n",
              "  0.017666161060333252,\n",
              "  0.017639482393860817,\n",
              "  0.01763315498828888,\n",
              "  0.017504585906863213,\n",
              "  0.017578985542058945,\n",
              "  0.01797333173453808,\n",
              "  0.01703876256942749,\n",
              "  0.016971424221992493,\n",
              "  0.01639733836054802,\n",
              "  0.016335612162947655,\n",
              "  0.016542328521609306,\n",
              "  0.01613040082156658,\n",
              "  0.016611307859420776,\n",
              "  0.01641751267015934,\n",
              "  0.016093015670776367,\n",
              "  0.01568274386227131,\n",
              "  0.015718085691332817,\n",
              "  0.015967456623911858,\n",
              "  0.015547964721918106,\n",
              "  0.015101774595677853,\n",
              "  0.014914085157215595,\n",
              "  0.01462546270340681,\n",
              "  0.014819891192018986,\n",
              "  0.014666939154267311,\n",
              "  0.01522743795067072,\n",
              "  0.01603078655898571,\n",
              "  0.015008225105702877,\n",
              "  0.014951424673199654,\n",
              "  0.014082470908761024,\n",
              "  0.014275831170380116,\n",
              "  0.014191689901053905,\n",
              "  0.01417659129947424,\n",
              "  0.015010545961558819,\n",
              "  0.014835039153695107,\n",
              "  0.01569012925028801,\n",
              "  0.014200485311448574,\n",
              "  0.013623549602925777,\n",
              "  0.014263828285038471,\n",
              "  0.01392541266977787,\n",
              "  0.014261968433856964,\n",
              "  0.014179825782775879,\n",
              "  0.013606296852231026,\n",
              "  0.013523402623832226,\n",
              "  0.013640383258461952,\n",
              "  0.01348789781332016,\n",
              "  0.013642923906445503,\n",
              "  0.013715005479753017,\n",
              "  0.013141615316271782,\n",
              "  0.013159831054508686,\n",
              "  0.013321982696652412,\n",
              "  0.012862218543887138,\n",
              "  0.012768222019076347,\n",
              "  0.01275094598531723,\n",
              "  0.013189581222832203,\n",
              "  0.013538127765059471,\n",
              "  0.012934982776641846,\n",
              "  0.012606607750058174,\n",
              "  0.012806838378310204,\n",
              "  0.01352321170270443,\n",
              "  0.0136034544557333,\n",
              "  0.013266372494399548,\n",
              "  0.013335112482309341,\n",
              "  0.012510011903941631,\n",
              "  0.012127396650612354,\n",
              "  0.012418849393725395,\n",
              "  0.012119749560952187,\n",
              "  0.012095307931303978,\n",
              "  0.012320359237492085,\n",
              "  0.012234367430210114,\n",
              "  0.01275542750954628,\n",
              "  0.012199871242046356,\n",
              "  0.011957836337387562,\n",
              "  0.012305207550525665,\n",
              "  0.011946807615458965,\n",
              "  0.011998114176094532,\n",
              "  0.011662094853818417,\n",
              "  0.012036926113069057,\n",
              "  0.011794069781899452,\n",
              "  0.011818988248705864,\n",
              "  0.011722548864781857,\n",
              "  0.011988484300673008,\n",
              "  0.011683880351483822,\n",
              "  0.011381878517568111,\n",
              "  0.011552704498171806,\n",
              "  0.012593725696206093,\n",
              "  0.012422647327184677,\n",
              "  0.012293434701859951,\n",
              "  0.011984053999185562,\n",
              "  0.01162825059145689,\n",
              "  0.011320776306092739,\n",
              "  0.011248363181948662,\n",
              "  0.011932339519262314,\n",
              "  0.01195449847728014,\n",
              "  0.011779004707932472,\n",
              "  0.012183241546154022,\n",
              "  0.011679220013320446,\n",
              "  0.011629951186478138,\n",
              "  0.0110617745667696,\n",
              "  0.011634895578026772,\n",
              "  0.011694937944412231,\n",
              "  0.01146254688501358,\n",
              "  0.01111813634634018,\n",
              "  0.011278891935944557,\n",
              "  0.011051095090806484,\n",
              "  0.011494812555611134,\n",
              "  0.011336938478052616,\n",
              "  0.011328404769301414,\n",
              "  0.012134367600083351,\n",
              "  0.011252615600824356,\n",
              "  0.011085208505392075,\n",
              "  0.01100902073085308,\n",
              "  0.010497870855033398,\n",
              "  0.011247619055211544,\n",
              "  0.011236574500799179,\n",
              "  0.012356801889836788,\n",
              "  0.011772261932492256,\n",
              "  0.011941039003431797,\n",
              "  0.011535047553479671,\n",
              "  0.011271247640252113,\n",
              "  0.011263268068432808,\n",
              "  0.012428770773112774,\n",
              "  0.011329935863614082,\n",
              "  0.010893639177083969,\n",
              "  0.01093000266700983,\n",
              "  0.011192328296601772,\n",
              "  0.010601659305393696,\n",
              "  0.010526303201913834,\n",
              "  0.01041188184171915,\n",
              "  0.010536829009652138,\n",
              "  0.010528760962188244,\n",
              "  0.010583082213997841,\n",
              "  0.01091093197464943,\n",
              "  0.011191400699317455,\n",
              "  0.010735735297203064,\n",
              "  0.010589881800115108,\n",
              "  0.010708126239478588,\n",
              "  0.010675564408302307,\n",
              "  0.010524864308536053,\n",
              "  0.010104511864483356,\n",
              "  0.010182172060012817,\n",
              "  0.01079193502664566,\n",
              "  0.010897275060415268,\n",
              "  0.010299782268702984,\n",
              "  0.010050475597381592,\n",
              "  0.010342089459300041,\n",
              "  0.012307140044867992,\n",
              "  0.01157444715499878,\n",
              "  0.010541399009525776,\n",
              "  0.010429736226797104,\n",
              "  0.010092792101204395,\n",
              "  0.01037603709846735,\n",
              "  0.010118400678038597,\n",
              "  0.01020605955272913,\n",
              "  0.010489328764379025,\n",
              "  0.011223768815398216,\n",
              "  0.011261933483183384,\n",
              "  0.010376526042819023,\n",
              "  0.010286438278853893,\n",
              "  0.010105079971253872,\n",
              "  0.010131360031664371,\n",
              "  0.010062169283628464,\n",
              "  0.01045070681720972,\n",
              "  0.010395831428468227,\n",
              "  0.010138330981135368,\n",
              "  0.009871872141957283,\n",
              "  0.010033946484327316,\n",
              "  0.009585423395037651,\n",
              "  0.009609632194042206,\n",
              "  0.009938386268913746,\n",
              "  0.009612051770091057,\n",
              "  0.009631427936255932,\n",
              "  0.00969899632036686,\n",
              "  0.009855466894805431,\n",
              "  0.01005470659583807,\n",
              "  0.009443648159503937,\n",
              "  0.00945188570767641,\n",
              "  0.009627032093703747,\n",
              "  0.009499575011432171,\n",
              "  0.009565272368490696,\n",
              "  0.009655200876295567,\n",
              "  0.00957200676202774,\n",
              "  0.010008282959461212,\n",
              "  0.01004463154822588,\n",
              "  0.0104757035151124,\n",
              "  0.009841447696089745,\n",
              "  0.009672272950410843,\n",
              "  0.009379609487950802,\n",
              "  0.009665071964263916,\n",
              "  0.009669332765042782,\n",
              "  0.009358690120279789,\n",
              "  0.009393343701958656,\n",
              "  0.008945625275373459,\n",
              "  0.009459398686885834,\n",
              "  0.009115366265177727,\n",
              "  0.009072311222553253,\n",
              "  0.009331698529422283,\n",
              "  0.008957923389971256,\n",
              "  0.009222363121807575,\n",
              "  0.009448418393731117,\n",
              "  0.010030957870185375,\n",
              "  0.009218696504831314,\n",
              "  0.00973012950271368,\n",
              "  0.009888356551527977,\n",
              "  0.009696167893707752,\n",
              "  0.009416562505066395,\n",
              "  0.009090432897210121,\n",
              "  0.008819930255413055,\n",
              "  0.008747891522943974,\n",
              "  0.00900097656995058,\n",
              "  0.009225791320204735,\n",
              "  0.009715053252875805,\n",
              "  0.009221062995493412,\n",
              "  0.009239371865987778,\n",
              "  0.008870584890246391,\n",
              "  0.009044813923537731,\n",
              "  0.009786027483642101,\n",
              "  0.009028902277350426,\n",
              "  0.009257898665964603,\n",
              "  0.008703178726136684,\n",
              "  0.008658191189169884,\n",
              "  0.009063013829290867,\n",
              "  0.008912655524909496,\n",
              "  0.00869564525783062,\n",
              "  0.009238813072443008,\n",
              "  0.008910587057471275,\n",
              "  0.008437341079115868,\n",
              "  0.008536798879504204,\n",
              "  0.009245890192687511,\n",
              "  0.009310627356171608,\n",
              "  0.009458712302148342,\n",
              "  0.00906586553901434,\n",
              "  0.008953534997999668,\n",
              "  0.008796784095466137,\n",
              "  0.008646591566503048,\n",
              "  0.009521286934614182,\n",
              "  0.010153443552553654,\n",
              "  0.009749850258231163,\n",
              "  0.009042159654200077,\n",
              "  0.008530213497579098,\n",
              "  0.008898562751710415,\n",
              "  0.00860496424138546,\n",
              "  0.00835052877664566,\n",
              "  0.008859661407768726,\n",
              "  0.009183873422443867,\n",
              "  0.009026010520756245,\n",
              "  0.008779773488640785,\n",
              "  0.008948614820837975,\n",
              "  0.008554929867386818,\n",
              "  0.008404160849750042,\n",
              "  0.008281082846224308,\n",
              "  0.0086979940533638,\n",
              "  0.009219298139214516,\n",
              "  0.009256438352167606,\n",
              "  0.009368493221700191,\n",
              "  0.009209584444761276,\n",
              "  0.008783658966422081,\n",
              "  0.008444070816040039,\n",
              "  0.008550765924155712,\n",
              "  0.008996302261948586,\n",
              "  0.008497640490531921,\n",
              "  0.008191248401999474,\n",
              "  0.008197510614991188,\n",
              "  0.00857556238770485,\n",
              "  0.008259345777332783,\n",
              "  0.0083894869312644,\n",
              "  0.00825468823313713,\n",
              "  0.008195241913199425,\n",
              "  0.008198365569114685,\n",
              "  0.008326585404574871,\n",
              "  0.008269582875072956,\n",
              "  0.00887200515717268,\n",
              "  0.00857496913522482,\n",
              "  0.008460464887320995,\n",
              "  0.008420173078775406,\n",
              "  0.008147593587636948,\n",
              "  0.007977789267897606,\n",
              "  0.008184385485947132,\n",
              "  0.008285793475806713,\n",
              "  0.008427740074694157,\n",
              "  0.008720527403056622,\n",
              "  0.008956827223300934,\n",
              "  0.00830362644046545,\n",
              "  0.008142183534801006,\n",
              "  0.00811169482767582,\n",
              "  0.007847301661968231,\n",
              "  0.008167823776602745,\n",
              "  0.00852600485086441,\n",
              "  0.007761642336845398,\n",
              "  0.008108813315629959,\n",
              "  0.007980864495038986,\n",
              "  0.007730297278612852,\n",
              "  0.007983813993632793,\n",
              "  0.008417296223342419,\n",
              "  0.008176379837095737,\n",
              "  0.008206850849092007,\n",
              "  0.00882851891219616,\n",
              "  0.008402206003665924,\n",
              "  0.008777589537203312,\n",
              "  0.008100515231490135,\n",
              "  0.007698744535446167,\n",
              "  0.007901487872004509],\n",
              " 'val_loss': [0.1097070574760437,\n",
              "  0.09530782699584961,\n",
              "  0.09772847592830658,\n",
              "  0.09195711463689804,\n",
              "  0.09067582339048386,\n",
              "  0.0763915404677391,\n",
              "  0.07548598945140839,\n",
              "  0.07598965615034103,\n",
              "  0.06840293854475021,\n",
              "  0.06405321508646011,\n",
              "  0.0624619796872139,\n",
              "  0.06418591737747192,\n",
              "  0.05931074172258377,\n",
              "  0.043113674968481064,\n",
              "  0.03809655085206032,\n",
              "  0.046107422560453415,\n",
              "  0.03304849937558174,\n",
              "  0.03954271599650383,\n",
              "  0.0330045148730278,\n",
              "  0.030516695231199265,\n",
              "  0.027682878077030182,\n",
              "  0.033148594200611115,\n",
              "  0.027081819251179695,\n",
              "  0.025214629247784615,\n",
              "  0.025267764925956726,\n",
              "  0.025851402431726456,\n",
              "  0.0244215726852417,\n",
              "  0.02465895190834999,\n",
              "  0.0235916581004858,\n",
              "  0.021868089213967323,\n",
              "  0.023269323632121086,\n",
              "  0.022106550633907318,\n",
              "  0.02162441797554493,\n",
              "  0.021908923983573914,\n",
              "  0.02104906737804413,\n",
              "  0.02197583205997944,\n",
              "  0.02084469422698021,\n",
              "  0.021546250209212303,\n",
              "  0.020833713933825493,\n",
              "  0.02074534073472023,\n",
              "  0.02006646990776062,\n",
              "  0.0214034803211689,\n",
              "  0.022751659154891968,\n",
              "  0.020059796050190926,\n",
              "  0.01968371495604515,\n",
              "  0.01985793188214302,\n",
              "  0.01889936439692974,\n",
              "  0.020619630813598633,\n",
              "  0.02199755236506462,\n",
              "  0.021545900031924248,\n",
              "  0.021230410784482956,\n",
              "  0.020591389387845993,\n",
              "  0.018467023968696594,\n",
              "  0.01991964876651764,\n",
              "  0.024320386350154877,\n",
              "  0.020201608538627625,\n",
              "  0.01912723481655121,\n",
              "  0.020373625680804253,\n",
              "  0.02009250782430172,\n",
              "  0.018164722248911858,\n",
              "  0.0191363375633955,\n",
              "  0.017751339823007584,\n",
              "  0.018577732145786285,\n",
              "  0.025569861754775047,\n",
              "  0.020419642329216003,\n",
              "  0.01929321326315403,\n",
              "  0.01732112653553486,\n",
              "  0.018547531217336655,\n",
              "  0.017012109979987144,\n",
              "  0.020459327846765518,\n",
              "  0.016081562265753746,\n",
              "  0.01743345335125923,\n",
              "  0.017836645245552063,\n",
              "  0.01628195308148861,\n",
              "  0.02098107896745205,\n",
              "  0.02031731978058815,\n",
              "  0.019778044894337654,\n",
              "  0.01759723760187626,\n",
              "  0.015428219921886921,\n",
              "  0.016286620870232582,\n",
              "  0.016575176268815994,\n",
              "  0.014789101667702198,\n",
              "  0.017815854400396347,\n",
              "  0.01538071222603321,\n",
              "  0.015289136208593845,\n",
              "  0.019917044788599014,\n",
              "  0.01787085272371769,\n",
              "  0.014345385134220123,\n",
              "  0.014290782622992992,\n",
              "  0.016113562509417534,\n",
              "  0.014898626133799553,\n",
              "  0.014747747220098972,\n",
              "  0.01915043219923973,\n",
              "  0.016430839896202087,\n",
              "  0.013569171540439129,\n",
              "  0.014301687479019165,\n",
              "  0.01564253307878971,\n",
              "  0.015656938776373863,\n",
              "  0.014891599304974079,\n",
              "  0.017321452498435974,\n",
              "  0.01448423694819212,\n",
              "  0.01337607391178608,\n",
              "  0.016139531508088112,\n",
              "  0.015877891331911087,\n",
              "  0.014242731034755707,\n",
              "  0.01512409932911396,\n",
              "  0.013954159803688526,\n",
              "  0.01685287617146969,\n",
              "  0.014096279628574848,\n",
              "  0.014193719252943993,\n",
              "  0.013681720942258835,\n",
              "  0.01522308774292469,\n",
              "  0.013321967795491219,\n",
              "  0.013002552092075348,\n",
              "  0.015135261230170727,\n",
              "  0.014185257256031036,\n",
              "  0.014902732335031033,\n",
              "  0.014487676322460175,\n",
              "  0.013550113886594772,\n",
              "  0.013366008177399635,\n",
              "  0.01380182709544897,\n",
              "  0.013300354592502117,\n",
              "  0.01480466965585947,\n",
              "  0.014494867995381355,\n",
              "  0.012740373611450195,\n",
              "  0.013755866326391697,\n",
              "  0.013257690705358982,\n",
              "  0.012632656842470169,\n",
              "  0.012598340399563313,\n",
              "  0.013242020271718502,\n",
              "  0.014091775752604008,\n",
              "  0.01582489162683487,\n",
              "  0.020308811217546463,\n",
              "  0.014222190715372562,\n",
              "  0.013083473779261112,\n",
              "  0.014766775071620941,\n",
              "  0.013009387999773026,\n",
              "  0.013295277021825314,\n",
              "  0.014683820307254791,\n",
              "  0.012126147747039795,\n",
              "  0.012866339646279812,\n",
              "  0.013134606182575226,\n",
              "  0.012435282580554485,\n",
              "  0.012360488064587116,\n",
              "  0.014537615701556206,\n",
              "  0.012538088485598564,\n",
              "  0.014016883447766304,\n",
              "  0.012060360051691532,\n",
              "  0.013140369206666946,\n",
              "  0.012529811821877956,\n",
              "  0.012516139075160027,\n",
              "  0.01222169864922762,\n",
              "  0.013376282528042793,\n",
              "  0.016046134755015373,\n",
              "  0.011934109032154083,\n",
              "  0.0134518938139081,\n",
              "  0.011773549020290375,\n",
              "  0.01708022505044937,\n",
              "  0.012648929841816425,\n",
              "  0.01183087658137083,\n",
              "  0.012812018394470215,\n",
              "  0.012542545795440674,\n",
              "  0.01296504307538271,\n",
              "  0.011953749693930149,\n",
              "  0.012642511166632175,\n",
              "  0.01292710192501545,\n",
              "  0.012513895519077778,\n",
              "  0.014957479201257229,\n",
              "  0.012088567018508911,\n",
              "  0.014063900336623192,\n",
              "  0.011309348046779633,\n",
              "  0.01237938366830349,\n",
              "  0.013218484818935394,\n",
              "  0.011425215750932693,\n",
              "  0.01116448175162077,\n",
              "  0.011552377603948116,\n",
              "  0.011922741308808327,\n",
              "  0.011551347561180592,\n",
              "  0.01203110534697771,\n",
              "  0.011699060909450054,\n",
              "  0.011889866553246975,\n",
              "  0.010951949283480644,\n",
              "  0.011118262074887753,\n",
              "  0.01378516387194395,\n",
              "  0.013454647734761238,\n",
              "  0.011868322268128395,\n",
              "  0.012554978020489216,\n",
              "  0.010554525069892406,\n",
              "  0.012091639451682568,\n",
              "  0.011760703288018703,\n",
              "  0.010703538544476032,\n",
              "  0.010444684885442257,\n",
              "  0.013095419853925705,\n",
              "  0.013007172383368015,\n",
              "  0.014508437365293503,\n",
              "  0.011624579317867756,\n",
              "  0.012107772752642632,\n",
              "  0.012327523902058601,\n",
              "  0.010770246386528015,\n",
              "  0.012875880114734173,\n",
              "  0.010943984612822533,\n",
              "  0.01131420023739338,\n",
              "  0.01101698074489832,\n",
              "  0.011095345951616764,\n",
              "  0.012361496686935425,\n",
              "  0.01186179555952549,\n",
              "  0.013196110725402832,\n",
              "  0.01214390154927969,\n",
              "  0.010195349343121052,\n",
              "  0.010584915988147259,\n",
              "  0.012239450588822365,\n",
              "  0.012927179224789143,\n",
              "  0.013306024484336376,\n",
              "  0.01369912177324295,\n",
              "  0.010915401391685009,\n",
              "  0.012059986591339111,\n",
              "  0.010488859377801418,\n",
              "  0.01045475248247385,\n",
              "  0.01289716549217701,\n",
              "  0.010340556502342224,\n",
              "  0.011603521183133125,\n",
              "  0.010600605048239231,\n",
              "  0.009825029410421848,\n",
              "  0.010800971649587154,\n",
              "  0.010241244919598103,\n",
              "  0.010656392201781273,\n",
              "  0.010012962855398655,\n",
              "  0.00990042369812727,\n",
              "  0.009573974646627903,\n",
              "  0.010840076953172684,\n",
              "  0.011173154227435589,\n",
              "  0.01143381092697382,\n",
              "  0.011469738557934761,\n",
              "  0.010925312526524067,\n",
              "  0.010235513560473919,\n",
              "  0.010036085732281208,\n",
              "  0.009999625384807587,\n",
              "  0.009877575561404228,\n",
              "  0.011570786125957966,\n",
              "  0.009652968496084213,\n",
              "  0.011159704066812992,\n",
              "  0.009489037096500397,\n",
              "  0.009754287078976631,\n",
              "  0.010862424038350582,\n",
              "  0.011339293792843819,\n",
              "  0.01155887357890606,\n",
              "  0.011646538972854614,\n",
              "  0.011630821973085403,\n",
              "  0.010243193246424198,\n",
              "  0.012264996767044067,\n",
              "  0.010000714100897312,\n",
              "  0.010723637416958809,\n",
              "  0.010799343697726727,\n",
              "  0.01083120983093977,\n",
              "  0.010481704026460648,\n",
              "  0.010067330673336983,\n",
              "  0.009999362751841545,\n",
              "  0.011357431299984455,\n",
              "  0.0093401949852705,\n",
              "  0.009754602797329426,\n",
              "  0.009499271400272846,\n",
              "  0.010419211350381374,\n",
              "  0.010530815459787846,\n",
              "  0.010395180433988571,\n",
              "  0.009556448087096214,\n",
              "  0.011007850058376789,\n",
              "  0.009560611099004745,\n",
              "  0.011067556217312813,\n",
              "  0.011353431269526482,\n",
              "  0.010167747735977173,\n",
              "  0.010014333762228489,\n",
              "  0.010545897297561169,\n",
              "  0.009703189134597778,\n",
              "  0.009025613777339458,\n",
              "  0.00973778311163187,\n",
              "  0.009410435333848,\n",
              "  0.009449574165046215,\n",
              "  0.008790780790150166,\n",
              "  0.010263131931424141,\n",
              "  0.010248268023133278,\n",
              "  0.010642723180353642,\n",
              "  0.011697238311171532,\n",
              "  0.010194255970418453,\n",
              "  0.010334155522286892,\n",
              "  0.010200153104960918,\n",
              "  0.010025489144027233,\n",
              "  0.009635671973228455,\n",
              "  0.010240448638796806,\n",
              "  0.010362522676587105,\n",
              "  0.009193369187414646,\n",
              "  0.010413112118840218,\n",
              "  0.009377890266478062,\n",
              "  0.00943430420011282,\n",
              "  0.009568639099597931,\n",
              "  0.0090865483507514,\n",
              "  0.008586691692471504,\n",
              "  0.008897458203136921,\n",
              "  0.010584798641502857,\n",
              "  0.00984339788556099,\n",
              "  0.009191981516778469,\n",
              "  0.010038275271654129,\n",
              "  0.009603639133274555,\n",
              "  0.009825695306062698,\n",
              "  0.009045724757015705,\n",
              "  0.009438318200409412,\n",
              "  0.008485172875225544,\n",
              "  0.009413521736860275,\n",
              "  0.010438889265060425,\n",
              "  0.008901202119886875,\n",
              "  0.011207816191017628,\n",
              "  0.011006251908838749,\n",
              "  0.008925427682697773,\n",
              "  0.00976327620446682,\n",
              "  0.009659083560109138,\n",
              "  0.01030929945409298,\n",
              "  0.010563580319285393,\n",
              "  0.008879488334059715,\n",
              "  0.008777575567364693,\n",
              "  0.009460659697651863,\n",
              "  0.010643519461154938,\n",
              "  0.008678828366100788,\n",
              "  0.00901964120566845,\n",
              "  0.009632925502955914,\n",
              "  0.009112901985645294,\n",
              "  0.0089365029707551,\n",
              "  0.008975350297987461,\n",
              "  0.009390145540237427,\n",
              "  0.009691007435321808,\n",
              "  0.009308798238635063,\n",
              "  0.010314829647541046,\n",
              "  0.008861665613949299,\n",
              "  0.008412620984017849,\n",
              "  0.008956052362918854,\n",
              "  0.010313163511455059,\n",
              "  0.010461184196174145,\n",
              "  0.009417247027158737,\n",
              "  0.012044649571180344,\n",
              "  0.009427404962480068,\n",
              "  0.008487281389534473,\n",
              "  0.009180136024951935,\n",
              "  0.010340324603021145,\n",
              "  0.009724048897624016,\n",
              "  0.00900751817971468,\n",
              "  0.010027975775301456,\n",
              "  0.010424508713185787,\n",
              "  0.008438591845333576,\n",
              "  0.008183149620890617,\n",
              "  0.008013321086764336,\n",
              "  0.009538915939629078,\n",
              "  0.008680078200995922,\n",
              "  0.009031515568494797,\n",
              "  0.009682093746960163,\n",
              "  0.009332229383289814,\n",
              "  0.009121192619204521,\n",
              "  0.008584542199969292,\n",
              "  0.010244525969028473,\n",
              "  0.008805091492831707,\n",
              "  0.008729951456189156,\n",
              "  0.009347555227577686,\n",
              "  0.009305985644459724,\n",
              "  0.009352932684123516,\n",
              "  0.009954373352229595,\n",
              "  0.009267140179872513,\n",
              "  0.009113138541579247,\n",
              "  0.009598053991794586,\n",
              "  0.008155189454555511,\n",
              "  0.008258168585598469,\n",
              "  0.011490888893604279,\n",
              "  0.00924499798566103,\n",
              "  0.009225227870047092,\n",
              "  0.008141341619193554,\n",
              "  0.008882488124072552,\n",
              "  0.00842575915157795,\n",
              "  0.00798317976295948,\n",
              "  0.008148367516696453,\n",
              "  0.009219111874699593,\n",
              "  0.009317034855484962,\n",
              "  0.01032982487231493,\n",
              "  0.009429516270756721,\n",
              "  0.008471347391605377,\n",
              "  0.009147646836936474,\n",
              "  0.008515813387930393,\n",
              "  0.008276334032416344,\n",
              "  0.009074111469089985,\n",
              "  0.00953639019280672,\n",
              "  0.00820816308259964,\n",
              "  0.008771845139563084,\n",
              "  0.008100779727101326,\n",
              "  0.007863704115152359,\n",
              "  0.009671434760093689,\n",
              "  0.009657698683440685,\n",
              "  0.009165912866592407,\n",
              "  0.008416123688220978,\n",
              "  0.010038761422038078,\n",
              "  0.008521763607859612,\n",
              "  0.008408986032009125,\n",
              "  0.011111771687865257,\n",
              "  0.008866716176271439,\n",
              "  0.007979813031852245,\n",
              "  0.008450500667095184],\n",
              " 'val_mean_absolute_error': [0.1097070574760437,\n",
              "  0.09530782699584961,\n",
              "  0.09772847592830658,\n",
              "  0.09195711463689804,\n",
              "  0.09067582339048386,\n",
              "  0.0763915404677391,\n",
              "  0.07548598945140839,\n",
              "  0.07598965615034103,\n",
              "  0.06840293854475021,\n",
              "  0.06405321508646011,\n",
              "  0.062461983412504196,\n",
              "  0.06418591737747192,\n",
              "  0.05931074172258377,\n",
              "  0.043113674968481064,\n",
              "  0.03809655085206032,\n",
              "  0.046107422560453415,\n",
              "  0.03304849937558174,\n",
              "  0.03954271599650383,\n",
              "  0.0330045148730278,\n",
              "  0.030516695231199265,\n",
              "  0.027682878077030182,\n",
              "  0.033148594200611115,\n",
              "  0.027081819251179695,\n",
              "  0.025214629247784615,\n",
              "  0.025267764925956726,\n",
              "  0.025851402431726456,\n",
              "  0.0244215726852417,\n",
              "  0.02465895190834999,\n",
              "  0.0235916581004858,\n",
              "  0.021868089213967323,\n",
              "  0.023269323632121086,\n",
              "  0.022106550633907318,\n",
              "  0.02162441797554493,\n",
              "  0.021908923983573914,\n",
              "  0.02104906737804413,\n",
              "  0.02197583205997944,\n",
              "  0.02084469422698021,\n",
              "  0.021546250209212303,\n",
              "  0.020833713933825493,\n",
              "  0.02074534073472023,\n",
              "  0.02006646990776062,\n",
              "  0.02140348218381405,\n",
              "  0.022751659154891968,\n",
              "  0.020059796050190926,\n",
              "  0.01968371495604515,\n",
              "  0.01985793188214302,\n",
              "  0.01889936253428459,\n",
              "  0.020619630813598633,\n",
              "  0.02199755236506462,\n",
              "  0.021545900031924248,\n",
              "  0.021230410784482956,\n",
              "  0.020591389387845993,\n",
              "  0.018467023968696594,\n",
              "  0.01991964876651764,\n",
              "  0.024320386350154877,\n",
              "  0.020201608538627625,\n",
              "  0.01912723481655121,\n",
              "  0.020373625680804253,\n",
              "  0.02009250782430172,\n",
              "  0.018164722248911858,\n",
              "  0.0191363375633955,\n",
              "  0.017751339823007584,\n",
              "  0.018577732145786285,\n",
              "  0.025569861754775047,\n",
              "  0.020419642329216003,\n",
              "  0.01929321326315403,\n",
              "  0.01732112653553486,\n",
              "  0.018547531217336655,\n",
              "  0.017012109979987144,\n",
              "  0.020459327846765518,\n",
              "  0.016081562265753746,\n",
              "  0.01743345335125923,\n",
              "  0.017836645245552063,\n",
              "  0.01628195308148861,\n",
              "  0.02098107896745205,\n",
              "  0.02031731978058815,\n",
              "  0.019778044894337654,\n",
              "  0.01759723760187626,\n",
              "  0.015428219921886921,\n",
              "  0.016286620870232582,\n",
              "  0.016575176268815994,\n",
              "  0.014789101667702198,\n",
              "  0.017815854400396347,\n",
              "  0.01538071222603321,\n",
              "  0.015289136208593845,\n",
              "  0.019917044788599014,\n",
              "  0.01787085272371769,\n",
              "  0.014345385134220123,\n",
              "  0.014290782622992992,\n",
              "  0.016113562509417534,\n",
              "  0.014898626133799553,\n",
              "  0.014747747220098972,\n",
              "  0.01915043219923973,\n",
              "  0.016430839896202087,\n",
              "  0.013569171540439129,\n",
              "  0.014301687479019165,\n",
              "  0.01564253307878971,\n",
              "  0.015656938776373863,\n",
              "  0.014891599304974079,\n",
              "  0.017321452498435974,\n",
              "  0.01448423694819212,\n",
              "  0.01337607391178608,\n",
              "  0.016139531508088112,\n",
              "  0.015877891331911087,\n",
              "  0.014242732897400856,\n",
              "  0.01512409932911396,\n",
              "  0.013954159803688526,\n",
              "  0.01685287617146969,\n",
              "  0.014096279628574848,\n",
              "  0.014193719252943993,\n",
              "  0.013681720942258835,\n",
              "  0.01522308774292469,\n",
              "  0.013321967795491219,\n",
              "  0.013002552092075348,\n",
              "  0.015135260298848152,\n",
              "  0.014185257256031036,\n",
              "  0.014902732335031033,\n",
              "  0.014487676322460175,\n",
              "  0.013550112955272198,\n",
              "  0.013366008177399635,\n",
              "  0.01380182709544897,\n",
              "  0.013300354592502117,\n",
              "  0.01480466965585947,\n",
              "  0.014494867995381355,\n",
              "  0.012740373611450195,\n",
              "  0.013755866326391697,\n",
              "  0.013257690705358982,\n",
              "  0.012632656842470169,\n",
              "  0.012598340399563313,\n",
              "  0.013242020271718502,\n",
              "  0.014091775752604008,\n",
              "  0.01582489162683487,\n",
              "  0.020308809354901314,\n",
              "  0.014222190715372562,\n",
              "  0.013083473779261112,\n",
              "  0.014766775071620941,\n",
              "  0.013009387999773026,\n",
              "  0.013295277021825314,\n",
              "  0.014683820307254791,\n",
              "  0.012126147747039795,\n",
              "  0.012866339646279812,\n",
              "  0.013134606182575226,\n",
              "  0.012435282580554485,\n",
              "  0.01236048899590969,\n",
              "  0.014537615701556206,\n",
              "  0.012538088485598564,\n",
              "  0.014016883447766304,\n",
              "  0.012060360051691532,\n",
              "  0.013140369206666946,\n",
              "  0.012529811821877956,\n",
              "  0.012516139075160027,\n",
              "  0.01222169864922762,\n",
              "  0.013376282528042793,\n",
              "  0.016046134755015373,\n",
              "  0.011934109032154083,\n",
              "  0.013451892882585526,\n",
              "  0.011773549020290375,\n",
              "  0.01708022505044937,\n",
              "  0.012648929841816425,\n",
              "  0.01183087658137083,\n",
              "  0.012812018394470215,\n",
              "  0.012542545795440674,\n",
              "  0.01296504307538271,\n",
              "  0.011953749693930149,\n",
              "  0.012642511166632175,\n",
              "  0.01292710192501545,\n",
              "  0.012513895519077778,\n",
              "  0.014957479201257229,\n",
              "  0.012088567018508911,\n",
              "  0.014063900336623192,\n",
              "  0.011309348046779633,\n",
              "  0.01237938366830349,\n",
              "  0.013218484818935394,\n",
              "  0.011425215750932693,\n",
              "  0.01116448175162077,\n",
              "  0.011552377603948116,\n",
              "  0.011922741308808327,\n",
              "  0.011551347561180592,\n",
              "  0.01203110534697771,\n",
              "  0.011699060909450054,\n",
              "  0.011889866553246975,\n",
              "  0.010951949283480644,\n",
              "  0.011118262074887753,\n",
              "  0.01378516387194395,\n",
              "  0.013454647734761238,\n",
              "  0.011868322268128395,\n",
              "  0.012554978020489216,\n",
              "  0.010554525069892406,\n",
              "  0.012091639451682568,\n",
              "  0.011760703288018703,\n",
              "  0.010703538544476032,\n",
              "  0.010444684885442257,\n",
              "  0.013095419853925705,\n",
              "  0.013007172383368015,\n",
              "  0.014508436433970928,\n",
              "  0.011624579317867756,\n",
              "  0.012107772752642632,\n",
              "  0.012327523902058601,\n",
              "  0.01077024731785059,\n",
              "  0.012875880114734173,\n",
              "  0.010943984612822533,\n",
              "  0.01131420023739338,\n",
              "  0.01101698074489832,\n",
              "  0.011095345951616764,\n",
              "  0.012361496686935425,\n",
              "  0.01186179555952549,\n",
              "  0.013196110725402832,\n",
              "  0.01214390154927969,\n",
              "  0.010195349343121052,\n",
              "  0.010584915988147259,\n",
              "  0.012239450588822365,\n",
              "  0.012927179224789143,\n",
              "  0.013306024484336376,\n",
              "  0.01369912177324295,\n",
              "  0.010915401391685009,\n",
              "  0.012059986591339111,\n",
              "  0.010488859377801418,\n",
              "  0.01045475248247385,\n",
              "  0.01289716549217701,\n",
              "  0.010340556502342224,\n",
              "  0.011603521183133125,\n",
              "  0.010600605048239231,\n",
              "  0.009825029410421848,\n",
              "  0.010800971649587154,\n",
              "  0.010241244919598103,\n",
              "  0.010656392201781273,\n",
              "  0.010012962855398655,\n",
              "  0.00990042369812727,\n",
              "  0.009573974646627903,\n",
              "  0.010840076953172684,\n",
              "  0.011173154227435589,\n",
              "  0.01143381092697382,\n",
              "  0.011469738557934761,\n",
              "  0.010925312526524067,\n",
              "  0.010235513560473919,\n",
              "  0.010036085732281208,\n",
              "  0.009999625384807587,\n",
              "  0.009877575561404228,\n",
              "  0.011570786125957966,\n",
              "  0.009652968496084213,\n",
              "  0.011159704066812992,\n",
              "  0.009489037096500397,\n",
              "  0.009754287078976631,\n",
              "  0.010862424038350582,\n",
              "  0.011339293792843819,\n",
              "  0.01155887357890606,\n",
              "  0.011646538972854614,\n",
              "  0.011630821973085403,\n",
              "  0.010243193246424198,\n",
              "  0.012264996767044067,\n",
              "  0.010000714100897312,\n",
              "  0.010723637416958809,\n",
              "  0.010799343697726727,\n",
              "  0.01083120983093977,\n",
              "  0.010481704026460648,\n",
              "  0.010067330673336983,\n",
              "  0.009999362751841545,\n",
              "  0.011357431299984455,\n",
              "  0.0093401949852705,\n",
              "  0.009754602797329426,\n",
              "  0.009499271400272846,\n",
              "  0.010419211350381374,\n",
              "  0.010530815459787846,\n",
              "  0.010395180433988571,\n",
              "  0.009556448087096214,\n",
              "  0.011007850058376789,\n",
              "  0.009560611099004745,\n",
              "  0.011067556217312813,\n",
              "  0.011353431269526482,\n",
              "  0.010167747735977173,\n",
              "  0.010014333762228489,\n",
              "  0.010545896366238594,\n",
              "  0.009703189134597778,\n",
              "  0.009025613777339458,\n",
              "  0.00973778311163187,\n",
              "  0.009410435333848,\n",
              "  0.009449574165046215,\n",
              "  0.008790780790150166,\n",
              "  0.010263131931424141,\n",
              "  0.010248268023133278,\n",
              "  0.010642723180353642,\n",
              "  0.011697238311171532,\n",
              "  0.010194255970418453,\n",
              "  0.010334155522286892,\n",
              "  0.010200153104960918,\n",
              "  0.010025489144027233,\n",
              "  0.009635671973228455,\n",
              "  0.010240448638796806,\n",
              "  0.01036252174526453,\n",
              "  0.009193369187414646,\n",
              "  0.010413112118840218,\n",
              "  0.009377890266478062,\n",
              "  0.00943430420011282,\n",
              "  0.009568639099597931,\n",
              "  0.0090865483507514,\n",
              "  0.008586691692471504,\n",
              "  0.008897458203136921,\n",
              "  0.010584798641502857,\n",
              "  0.00984339788556099,\n",
              "  0.009191981516778469,\n",
              "  0.010038275271654129,\n",
              "  0.009603639133274555,\n",
              "  0.009825695306062698,\n",
              "  0.00904572568833828,\n",
              "  0.009438318200409412,\n",
              "  0.008485172875225544,\n",
              "  0.009413521736860275,\n",
              "  0.010438889265060425,\n",
              "  0.008901202119886875,\n",
              "  0.011207816191017628,\n",
              "  0.011006251908838749,\n",
              "  0.008925427682697773,\n",
              "  0.00976327620446682,\n",
              "  0.009659083560109138,\n",
              "  0.01030929945409298,\n",
              "  0.010563580319285393,\n",
              "  0.008879488334059715,\n",
              "  0.008777575567364693,\n",
              "  0.009460659697651863,\n",
              "  0.010643519461154938,\n",
              "  0.008678828366100788,\n",
              "  0.00901964120566845,\n",
              "  0.009632925502955914,\n",
              "  0.009112901985645294,\n",
              "  0.0089365029707551,\n",
              "  0.008975351229310036,\n",
              "  0.009390145540237427,\n",
              "  0.009691007435321808,\n",
              "  0.009308798238635063,\n",
              "  0.010314829647541046,\n",
              "  0.008861665613949299,\n",
              "  0.008412620984017849,\n",
              "  0.008956052362918854,\n",
              "  0.010313163511455059,\n",
              "  0.010461184196174145,\n",
              "  0.009417247027158737,\n",
              "  0.012044649571180344,\n",
              "  0.009427404962480068,\n",
              "  0.008487281389534473,\n",
              "  0.00918013695627451,\n",
              "  0.010340324603021145,\n",
              "  0.009724048897624016,\n",
              "  0.00900751817971468,\n",
              "  0.010027975775301456,\n",
              "  0.010424508713185787,\n",
              "  0.008438591845333576,\n",
              "  0.008183149620890617,\n",
              "  0.008013321086764336,\n",
              "  0.009538915939629078,\n",
              "  0.008680078200995922,\n",
              "  0.009031515568494797,\n",
              "  0.009682093746960163,\n",
              "  0.009332229383289814,\n",
              "  0.009121192619204521,\n",
              "  0.008584542199969292,\n",
              "  0.010244525969028473,\n",
              "  0.008805091492831707,\n",
              "  0.008729951456189156,\n",
              "  0.009347555227577686,\n",
              "  0.009305985644459724,\n",
              "  0.009352932684123516,\n",
              "  0.009954373352229595,\n",
              "  0.009267140179872513,\n",
              "  0.009113138541579247,\n",
              "  0.009598053991794586,\n",
              "  0.008155189454555511,\n",
              "  0.008258168585598469,\n",
              "  0.011490888893604279,\n",
              "  0.00924499798566103,\n",
              "  0.009225227870047092,\n",
              "  0.008141341619193554,\n",
              "  0.008882488124072552,\n",
              "  0.00842575915157795,\n",
              "  0.00798317976295948,\n",
              "  0.008148367516696453,\n",
              "  0.009219111874699593,\n",
              "  0.009317034855484962,\n",
              "  0.01032982487231493,\n",
              "  0.009429516270756721,\n",
              "  0.008471347391605377,\n",
              "  0.009147646836936474,\n",
              "  0.008515812456607819,\n",
              "  0.008276333101093769,\n",
              "  0.009074111469089985,\n",
              "  0.00953639019280672,\n",
              "  0.00820816308259964,\n",
              "  0.008771845139563084,\n",
              "  0.008100779727101326,\n",
              "  0.007863704115152359,\n",
              "  0.009671434760093689,\n",
              "  0.009657698683440685,\n",
              "  0.009165912866592407,\n",
              "  0.008416123688220978,\n",
              "  0.010038761422038078,\n",
              "  0.008521763607859612,\n",
              "  0.008408986032009125,\n",
              "  0.011111771687865257,\n",
              "  0.008866717107594013,\n",
              "  0.007979813031852245,\n",
              "  0.008450500667095184]}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "ObGrOy0LHu3x",
        "outputId": "d59d2b35-13b4-4f57-a373-f1f613186261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1=model.get_layer(\"dense\")"
      ],
      "metadata": {
        "id": "Z0aNWNfsSHtz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1.activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu1aFN96coWC",
        "outputId": "1a34ae3e-5549-40f8-f6d1-298f5ff3f7e9"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function keras.activations.sigmoid>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o1=model.get_layer(\"vektor\")"
      ],
      "metadata": {
        "id": "EEaadwGCa_H9"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "mNNzsG6yhAvc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "hidden_states_fun = K.function([i1.input],[o1.output])"
      ],
      "metadata": {
        "id": "rzxk7T8HhQOn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(X, index):\n",
        "    \"\"\"Return the hidden state associated with an input at the given timestep.\n",
        "    \"\"\"\n",
        "    hidden_states = hidden_states_fun(X.iloc[index:index+1,:].values)\n",
        "    return hidden_states"
      ],
      "metadata": {
        "id": "vw9pCVighoUh"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_minmax.iloc[0:1,:].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q6HBgVOiCgB",
        "outputId": "64a20af0-4f97-4c89-b44a-9dec27c5ceaf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oo_list=[]\n",
        "for i in range( len(df_X_minmax)):\n",
        "    xo=get_embedding(df_X_minmax,i)\n",
        "    oo_list.append(xo[0].tolist()[0])"
      ],
      "metadata": {
        "id": "zA2s-WuVhCce"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oo_list[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBcmU0HGqhs1",
        "outputId": "b1e633ae-dd4a-4a04-bd8b-e679ea837a6e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.008064047433435917,\n",
              " 0.005548927932977676,\n",
              " 0.3249986469745636,\n",
              " 7.627852028235793e-05,\n",
              " 0.9969078898429871,\n",
              " 0.8911228179931641,\n",
              " 0.00989881157875061,\n",
              " 0.967979371547699,\n",
              " 0.6268854141235352,\n",
              " 0.01100628636777401,\n",
              " 7.262349754455499e-06,\n",
              " 0.018625332042574883,\n",
              " 0.9997472167015076,\n",
              " 5.355957455321914e-06,\n",
              " 0.9990239143371582,\n",
              " 0.9981653094291687,\n",
              " 0.015404192730784416,\n",
              " 0.866118311882019,\n",
              " 0.883919358253479,\n",
              " 3.139084867598285e-07,\n",
              " 0.9999933242797852,\n",
              " 0.8946037292480469,\n",
              " 0.8798108696937561,\n",
              " 0.7334153056144714,\n",
              " 3.4391806025269034e-07,\n",
              " 0.8652848601341248,\n",
              " 0.9045274257659912,\n",
              " 1.5020953469502274e-05,\n",
              " 0.04446347802877426,\n",
              " 0.00025046360678970814,\n",
              " 0.023085176944732666,\n",
              " 0.060834985226392746,\n",
              " 0.03319214656949043,\n",
              " 0.9162458181381226,\n",
              " 0.09581349045038223,\n",
              " 0.730032205581665,\n",
              " 0.0014517619274556637,\n",
              " 5.263480488792993e-05,\n",
              " 0.9999967813491821,\n",
              " 0.0031996609177440405,\n",
              " 0.30064114928245544,\n",
              " 0.66456538438797,\n",
              " 0.29270824790000916,\n",
              " 0.3436848521232605,\n",
              " 0.9999972581863403,\n",
              " 0.8209698796272278,\n",
              " 0.2545750141143799,\n",
              " 1.4313020813006005e-07,\n",
              " 0.8584469556808472,\n",
              " 0.3681463897228241,\n",
              " 0.9882418513298035,\n",
              " 0.002601729705929756,\n",
              " 0.9999699592590332,\n",
              " 0.567626953125,\n",
              " 0.9999905824661255,\n",
              " 0.9976893663406372,\n",
              " 0.9953137636184692,\n",
              " 0.002562338951975107,\n",
              " 0.9996522665023804,\n",
              " 0.9540659785270691,\n",
              " 0.017141060903668404,\n",
              " 0.2776186466217041,\n",
              " 0.09453215450048447,\n",
              " 0.01359866838902235,\n",
              " 0.006478907074779272,\n",
              " 0.9960317015647888,\n",
              " 0.0008944004075601697,\n",
              " 0.999983549118042,\n",
              " 0.43688467144966125,\n",
              " 0.00025189932784996927,\n",
              " 0.9984012246131897,\n",
              " 0.07276183366775513,\n",
              " 0.9989797472953796,\n",
              " 1.1015580270168357e-07,\n",
              " 0.0015879642451182008,\n",
              " 0.03376190736889839,\n",
              " 0.9999983310699463,\n",
              " 0.013119728304445744,\n",
              " 1.1514269317558501e-05,\n",
              " 0.999996542930603,\n",
              " 0.9953141212463379,\n",
              " 0.9999979734420776,\n",
              " 0.9978651404380798,\n",
              " 0.00042020250111818314,\n",
              " 0.004840195644646883,\n",
              " 0.000766125216614455,\n",
              " 0.9278220534324646,\n",
              " 0.9999650716781616,\n",
              " 0.003911550156772137,\n",
              " 1.806244472390972e-05,\n",
              " 0.0055112154223024845,\n",
              " 0.868919312953949,\n",
              " 0.0859222412109375,\n",
              " 0.08295715600252151,\n",
              " 3.6860983154696214e-09,\n",
              " 0.003069858765229583,\n",
              " 3.070179900532821e-06,\n",
              " 0.9914012551307678,\n",
              " 1.3652108464157209e-05,\n",
              " 0.26000458002090454,\n",
              " 2.9719413596751565e-09,\n",
              " 0.9601219892501831,\n",
              " 0.07924721390008926,\n",
              " 0.00041617584065534174,\n",
              " 1.4004451998061995e-07,\n",
              " 0.9329922795295715,\n",
              " 0.0019990105647593737,\n",
              " 0.0040993355214595795,\n",
              " 0.9126733541488647,\n",
              " 0.007391051389276981,\n",
              " 0.9669256806373596,\n",
              " 0.9976311922073364,\n",
              " 0.07529307901859283,\n",
              " 0.11270462721586227,\n",
              " 0.7938273549079895,\n",
              " 9.173538373374868e-09,\n",
              " 0.8133684396743774,\n",
              " 0.013741369359195232,\n",
              " 0.0424492172896862,\n",
              " 1.5347724911407568e-05,\n",
              " 0.010497750714421272,\n",
              " 0.009713961742818356,\n",
              " 0.9980244636535645,\n",
              " 0.9487950205802917,\n",
              " 0.0032878078054636717,\n",
              " 0.9993649125099182,\n",
              " 0.09694790095090866,\n",
              " 1.0,\n",
              " 0.9937678575515747,\n",
              " 4.8588631216262e-06,\n",
              " 0.9999955892562866,\n",
              " 0.04612021893262863,\n",
              " 0.02831565961241722,\n",
              " 0.004150496795773506,\n",
              " 0.9470480680465698,\n",
              " 0.12562653422355652,\n",
              " 0.0001489739224780351,\n",
              " 0.006055498495697975,\n",
              " 0.9599921107292175,\n",
              " 8.43521956994664e-06,\n",
              " 8.109438454084739e-08,\n",
              " 9.712032289499462e-10,\n",
              " 0.03365592658519745,\n",
              " 0.5999333262443542,\n",
              " 0.0005215612472966313,\n",
              " 0.7746061086654663,\n",
              " 0.007469840347766876,\n",
              " 0.2853506803512573,\n",
              " 0.024453070014715195,\n",
              " 0.30356162786483765]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"time\"].to_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTXd7qn4qhrN",
        "outputId": "022352f6-9254-49f6-af87-8aaccc736638"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['M13127N_detect3020',\n",
              " 'M13144N_detect3020',\n",
              " 'M13153N_detect3020',\n",
              " 'M14028N_detect3020',\n",
              " 'M14049N_detect3020',\n",
              " 'M14052N_detect3020',\n",
              " 'M14067N_detect3020',\n",
              " 'M14072N_detect3020',\n",
              " 'M14076N_detect3020',\n",
              " 'M14084N_detect3020',\n",
              " 'M14123N_detect3020',\n",
              " 'M15007N_detect3020',\n",
              " 'M15014N_detect3020',\n",
              " 'M15019N_detect3020',\n",
              " 'M15033N_detect3020',\n",
              " 'M15067N_detect3020',\n",
              " 'M15068N_detect3020',\n",
              " 'M15072N_detect3020',\n",
              " 'M15101N_detect3020',\n",
              " 'M1A054N_detect3020',\n",
              " 'M1A055N_detect3020',\n",
              " 'M1A113N_detect3020',\n",
              " 'M1A114N_detect3020',\n",
              " 'M1A133N_detect3020',\n",
              " 'M1A134N_detect3020',\n",
              " 'M1A141N_detect3020',\n",
              " 'M13115N_detect3020',\n",
              " 'M13002N_detect3020',\n",
              " 'M5A152N_detect3020',\n",
              " 'M5B122N_detect3020',\n",
              " 'M5B216N_detect3020',\n",
              " 'M5C011N_detect3020',\n",
              " 'M5C025N_detect3020',\n",
              " 'M68020N_detect3020',\n",
              " 'M68038N_detect3020',\n",
              " 'M69085N_detect3020',\n",
              " 'M69094N_detect3020',\n",
              " 'M6B041N_detect3020',\n",
              " 'M6B072N_detect3020',\n",
              " 'M6B081N_detect3020',\n",
              " 'M6B083N_detect3020',\n",
              " 'M68007N_detect3020',\n",
              " 'M6B101N_detect3020',\n",
              " 'M6B143N_detect3020',\n",
              " 'M6B155N_detect3020',\n",
              " 'M6C015N_detect3020',\n",
              " 'M6C027N_detect3020',\n",
              " 'M69068N_detect3020',\n",
              " 'M6B009N_detect3020',\n",
              " 'M71045N_detect3020',\n",
              " 'M73014N_detect3020',\n",
              " 'M73025N_detect3020',\n",
              " 'M73103N_detect3020',\n",
              " 'M79083N_detect3020',\n",
              " 'M7A058N_detect3020',\n",
              " 'M7A081N_detect3020',\n",
              " 'M7A090N_detect3020',\n",
              " 'M7A066N_detect3020',\n",
              " 'M72025N_detect3020',\n",
              " 'M72080N_detect3020',\n",
              " 'M81010N_detect3020',\n",
              " 'M81010N/2_detect3020',\n",
              " 'M81010N/3_detect3020',\n",
              " 'M81053N_detect3020',\n",
              " 'M89093N_detect3020',\n",
              " 'M81175N_detect3020',\n",
              " 'M8A017N_detect3020',\n",
              " 'M8A088N_detect3020',\n",
              " 'M82024N_detect3020',\n",
              " 'M8A180N_detect3020',\n",
              " 'M82044N_detect3020',\n",
              " 'M82051N_detect3020',\n",
              " 'M82106N_detect3020',\n",
              " 'M82130N_detect3020',\n",
              " 'M82132N_detect3020',\n",
              " 'M82165N_detect3020',\n",
              " 'M83018N_detect3020',\n",
              " 'M81054N_detect3020',\n",
              " 'M83074N_detect3020',\n",
              " 'M83092N_detect3020',\n",
              " 'M83124N_detect3020',\n",
              " 'M83135N_detect3020',\n",
              " 'M83170N_detect3020',\n",
              " 'M88042N_detect3020',\n",
              " 'M88054N_detect3020',\n",
              " 'M88058N_detect3020',\n",
              " 'M88093N_detect3020',\n",
              " 'M89014N_detect3020',\n",
              " 'M89022N_detect3020',\n",
              " 'M89030N_detect3020',\n",
              " 'M89030Nism_detect3020',\n",
              " 'M89077N_detect3020',\n",
              " 'M81104Nism_detect3020',\n",
              " 'M89078N_detect3020',\n",
              " 'M89079N_detect3020',\n",
              " 'M89080N_detect3020',\n",
              " 'M89095N_detect3020',\n",
              " 'M89158N_detect3020',\n",
              " 'M89160N_detect3020',\n",
              " 'M89163N_detect3020',\n",
              " 'M89166N_detect3020',\n",
              " 'M89169N_detect3020',\n",
              " 'M8A020N_detect3020',\n",
              " 'M83083N_detect3020',\n",
              " 'M8A026N_detect3020',\n",
              " 'M8A048N_detect3020',\n",
              " 'M8A058N_detect3020',\n",
              " 'M8A133N_detect3020',\n",
              " 'M8A134N_detect3020',\n",
              " 'M8A135N_detect3020',\n",
              " 'M8A136N_detect3020',\n",
              " 'M8A141N_detect3020',\n",
              " 'M8A142N_detect3020',\n",
              " 'M81126N_detect3020',\n",
              " 'M8A147N_detect3020',\n",
              " 'M8B007N_detect3020',\n",
              " 'M8B008N_detect3020',\n",
              " 'M8B020N_detect3020',\n",
              " 'M8B023N_detect3020',\n",
              " 'M88043N_detect3020',\n",
              " 'M88091N_detect3020',\n",
              " 'M81153N_detect3020',\n",
              " 'M91001N_detect3020',\n",
              " 'M92018N_detect3020',\n",
              " 'M92022N_detect3020',\n",
              " 'M92058N_detect3020',\n",
              " 'M9B085N_detect3020',\n",
              " 'M92106N_detect3020',\n",
              " 'M92107N_detect3020',\n",
              " 'M92109N_detect3020',\n",
              " 'M91013N_detect3020',\n",
              " 'M92126N_detect3020',\n",
              " 'M92141N_detect3020',\n",
              " 'M93014N_detect3020',\n",
              " 'M91020N_detect3020',\n",
              " 'M91070N_detect3020',\n",
              " 'M96040N_detect3020',\n",
              " 'M96045N_detect3020',\n",
              " 'M96091N_detect3020',\n",
              " 'M96092N_detect3020',\n",
              " 'M96104N_detect3020',\n",
              " 'M97022N_detect3020',\n",
              " 'M98006N_detect3020',\n",
              " 'M91074N_detect3020',\n",
              " 'M98009N_detect3020',\n",
              " 'M99028N_detect3020',\n",
              " 'M99038N_detect3020',\n",
              " 'M99054N_detect3020',\n",
              " 'M99077N_detect3020',\n",
              " 'M91097N_detect3020',\n",
              " 'M99132N_detect3020',\n",
              " 'M99152N_detect3020',\n",
              " 'M99159N_detect3020',\n",
              " 'M9A001N_detect3020',\n",
              " 'M9A014N_detect3020',\n",
              " 'M91104N_detect3020',\n",
              " 'M96034N_detect3020',\n",
              " 'M96034N2_detect3020',\n",
              " 'M9A061N_detect3020',\n",
              " 'M9A089N_detect3020',\n",
              " 'M9A111N_detect3020',\n",
              " 'M9A149N_detect3020',\n",
              " 'M9A175N_detect3020',\n",
              " 'M92005N_detect3020',\n",
              " 'M98007N_detect3020',\n",
              " 'M9B014N_detect3020',\n",
              " 'M9B060N_detect3020',\n",
              " 'M9B084N_detect3020',\n",
              " 'M9B116N_detect3020',\n",
              " 'M92012N_detect3020',\n",
              " 'M9B151N_detect3020',\n",
              " 'M9C011N_detect3020',\n",
              " 'M9C025N_detect3020',\n",
              " 'M92062N2_detect3020',\n",
              " 'M06020N_detect3020',\n",
              " 'M06117N_detect3020',\n",
              " 'M07012N_detect3020',\n",
              " 'M07070N_detect3020',\n",
              " 'M07073N_detect3020',\n",
              " 'M07074N_detect3020',\n",
              " 'M07075N_detect3020',\n",
              " 'M08040N_detect3020',\n",
              " 'M08042N_detect3020',\n",
              " 'M08061N_detect3020',\n",
              " 'M08093N_detect3020',\n",
              " 'M08101N_detect3020',\n",
              " 'M09004N_detect3020',\n",
              " 'M09008N_detect3020',\n",
              " 'M09038N_detect3020',\n",
              " 'M09072N_detect3020',\n",
              " 'M09072Njo_detect3020',\n",
              " 'M09073N_detect3020',\n",
              " 'M09105N_detect3020',\n",
              " 'M06018NS_detect3020',\n",
              " 'M0A020N_detect3020',\n",
              " 'M0A028N_detect3020',\n",
              " 'M0A033N_detect3020',\n",
              " 'M0B015N_detect3020',\n",
              " 'M0B107N_detect3020',\n",
              " 'M0B121N_detect3020',\n",
              " 'M07114N_detect3020',\n",
              " 'M09124N_detect3020',\n",
              " 'M0B014N_detect3020',\n",
              " 'M06018N_detect3020',\n",
              " 'M06018N2_detect3020']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchid=df[\"time\"].to_list()"
      ],
      "metadata": {
        "id": "tes6Op8Z5NGb"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(batchid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SaKM5pd5QTh",
        "outputId": "59ef63f4-875b-4d2a-d707-841945508b24"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(oo_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCho9OFr5U7n",
        "outputId": "250d2d82-e805-4cf2-98db-7489be0211a7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_l=pd.DataFrame(oo_list)"
      ],
      "metadata": {
        "id": "EJB4Esu70fwR"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l[\"y\"]=df_y2"
      ],
      "metadata": {
        "id": "D9akr29d1160"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l[\"batchid\"]=batchid"
      ],
      "metadata": {
        "id": "It-VVB3n5qlx"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "EMl6qcXv04D9",
        "outputId": "baa4eeb1-1580-47a3-b005-973465c3f1be"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.222424  0.030000  0.932073  0.000148  0.943448  0.070314  0.095956   \n",
              "1  0.394996  0.029227  0.918980  0.000208  0.903115  0.034615  0.154734   \n",
              "2  0.993589  0.971955  0.999260  0.828631  0.891263  0.713170  0.000019   \n",
              "3  0.331313  0.032794  0.933229  0.000196  0.935896  0.048515  0.107356   \n",
              "4  0.015676  0.003781  0.005359  0.001312  0.999999  0.995876  0.000052   \n",
              "\n",
              "          7         8         9  ...       142       143       144       145  \\\n",
              "0  0.570482  0.100810  0.218261  ...  0.040414  0.194629  0.000033  0.835839   \n",
              "1  0.354860  0.094588  0.277983  ...  0.045780  0.194267  0.000036  0.815306   \n",
              "2  0.008879  0.000028  0.000500  ...  0.998370  0.000004  0.000024  0.074779   \n",
              "3  0.429363  0.085992  0.225781  ...  0.049201  0.171825  0.000036  0.823906   \n",
              "4  0.947694  0.911940  0.000013  ...  0.849545  0.505630  0.813973  0.095484   \n",
              "\n",
              "            146       147       148       149  y             batchid  \n",
              "0  8.657558e-03  0.015196  0.009491  0.163045  0  M13127N_detect3020  \n",
              "1  1.887678e-02  0.013205  0.008827  0.257485  0  M13144N_detect3020  \n",
              "2  8.991792e-07  0.016991  0.006990  0.883925  0  M13153N_detect3020  \n",
              "3  1.113857e-02  0.014321  0.009821  0.240489  0  M14028N_detect3020  \n",
              "4  5.007820e-03  0.993173  0.853355  0.999196  0  M14049N_detect3020  \n",
              "\n",
              "[5 rows x 152 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d62b79f-2d8b-4270-84a4-67e557b5adf7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>y</th>\n",
              "      <th>batchid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.222424</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.932073</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.943448</td>\n",
              "      <td>0.070314</td>\n",
              "      <td>0.095956</td>\n",
              "      <td>0.570482</td>\n",
              "      <td>0.100810</td>\n",
              "      <td>0.218261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040414</td>\n",
              "      <td>0.194629</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.835839</td>\n",
              "      <td>8.657558e-03</td>\n",
              "      <td>0.015196</td>\n",
              "      <td>0.009491</td>\n",
              "      <td>0.163045</td>\n",
              "      <td>0</td>\n",
              "      <td>M13127N_detect3020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.394996</td>\n",
              "      <td>0.029227</td>\n",
              "      <td>0.918980</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.903115</td>\n",
              "      <td>0.034615</td>\n",
              "      <td>0.154734</td>\n",
              "      <td>0.354860</td>\n",
              "      <td>0.094588</td>\n",
              "      <td>0.277983</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045780</td>\n",
              "      <td>0.194267</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.815306</td>\n",
              "      <td>1.887678e-02</td>\n",
              "      <td>0.013205</td>\n",
              "      <td>0.008827</td>\n",
              "      <td>0.257485</td>\n",
              "      <td>0</td>\n",
              "      <td>M13144N_detect3020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.993589</td>\n",
              "      <td>0.971955</td>\n",
              "      <td>0.999260</td>\n",
              "      <td>0.828631</td>\n",
              "      <td>0.891263</td>\n",
              "      <td>0.713170</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.008879</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.998370</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.074779</td>\n",
              "      <td>8.991792e-07</td>\n",
              "      <td>0.016991</td>\n",
              "      <td>0.006990</td>\n",
              "      <td>0.883925</td>\n",
              "      <td>0</td>\n",
              "      <td>M13153N_detect3020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.331313</td>\n",
              "      <td>0.032794</td>\n",
              "      <td>0.933229</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.935896</td>\n",
              "      <td>0.048515</td>\n",
              "      <td>0.107356</td>\n",
              "      <td>0.429363</td>\n",
              "      <td>0.085992</td>\n",
              "      <td>0.225781</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049201</td>\n",
              "      <td>0.171825</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.823906</td>\n",
              "      <td>1.113857e-02</td>\n",
              "      <td>0.014321</td>\n",
              "      <td>0.009821</td>\n",
              "      <td>0.240489</td>\n",
              "      <td>0</td>\n",
              "      <td>M14028N_detect3020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.015676</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.005359</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.995876</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.947694</td>\n",
              "      <td>0.911940</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.849545</td>\n",
              "      <td>0.505630</td>\n",
              "      <td>0.813973</td>\n",
              "      <td>0.095484</td>\n",
              "      <td>5.007820e-03</td>\n",
              "      <td>0.993173</td>\n",
              "      <td>0.853355</td>\n",
              "      <td>0.999196</td>\n",
              "      <td>0</td>\n",
              "      <td>M14049N_detect3020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 152 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d62b79f-2d8b-4270-84a4-67e557b5adf7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d62b79f-2d8b-4270-84a4-67e557b5adf7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d62b79f-2d8b-4270-84a4-67e557b5adf7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_l.to_csv(\"encoded.csv\")"
      ],
      "metadata": {
        "id": "Ka9uIeG14QCQ"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "YrGI_BRDQrVW",
        "outputId": "e55c8843-6d10-4286-a250-4628d05cc798"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-29e0c3615294>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ---\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "82AeWLEG1vsQ"
      },
      "outputs": [],
      "source": [
        "_MODE_=\"Tesztel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I132le63oAkN",
        "outputId": "fb3dfaa9-90e8-465a-e8d7-dad55b422977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-13 21:25:51--  https://github.com/sipocz/LSTM_HPLC/raw/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/LSTM_HPLC/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5 [following]\n",
            "--2022-04-13 21:25:52--  https://raw.githubusercontent.com/sipocz/LSTM_HPLC/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3950608 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5’\n",
            "\n",
            "XXXX_loss_0.0795_ac 100%[===================>]   3.77M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-04-13 21:25:53 (48.4 MB/s) - ‘XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5’ saved [3950608/3950608]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "fname=\"https://github.com/sipocz/LSTM_HPLC/raw/d618ec12225d2dfeb07f48a4c10840bcf822d3ca/models/XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\"\n",
        "if _MODE_==\"Tesztel\":\n",
        "    #!rm XXXX*\n",
        "    !wget $fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el4jLBvoybTN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05jpd7BjeYez"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "IrAfG02Lj1wr"
      },
      "outputs": [],
      "source": [
        "fname=\"XXXX_loss_0.0795_acc_0.9783_val_acc_0.9524_20220410133100.hdf5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0mdKjZBjb7L"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "nk87VBSQndA5",
        "outputId": "816958d1-9441-4e0f-d986-ed8297411da4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-846d67d0b396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       raise ValueError(\n\u001b[0;32m--> 746\u001b[0;31m           \u001b[0;34mf'Weight count mismatch for layer #{k} (named {layer.name} in the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m           \u001b[0;34mf'current model, {name} in the save file). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m           \u001b[0;34mf'Layer expects {len(symbolic_weights)} weight(s). Received '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Weight count mismatch for layer #0 (named dense in the current model, bidirectional in the save file). Layer expects 2 weight(s). Received 6 saved weight(s)"
          ]
        }
      ],
      "source": [
        "model.load_weights(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1ks758inc_Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_CHGgYja_bB"
      },
      "outputs": [],
      "source": [
        "pred=model.predict(df_X_minmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loCl2-qmA6SL"
      },
      "outputs": [],
      "source": [
        "def maxpos(alist):\n",
        "    temp = max(alist)\n",
        "    res = [i for i, j in enumerate(alist) if j == temp]\n",
        "    return res[0]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIFRZ753bMNi"
      },
      "outputs": [],
      "source": [
        "#from IPython.lib.display import YouTubeVideo\n",
        "def show_difference(pred,ytrain,verbose=False, only_wrong=True, to_table=False):\n",
        "    ecounter=0\n",
        "    o=[]\n",
        "    for i in range(len(pred)):\n",
        "        predi=pred[i]\n",
        "        traini=ytrain[i]\n",
        "        if abs(predi-traini)>0.5:\n",
        "            ecounter+=1\n",
        "            o.append(i)\n",
        "        if verbose:\n",
        "            if only_wrong:\n",
        "                \n",
        "                if abs(predi-traini)>0.5: \n",
        "                    if to_table:\n",
        "                        print(f\"|{i}|{predi}|{traini}|\")\n",
        "                    else:\n",
        "                        print(f\"{i}, {predi}, {traini}\")\n",
        "                \n",
        "            else:\n",
        "                print(f\"{i}, {predi}, {traini}\")\n",
        "\n",
        "                \n",
        "    print(f\"Hiba szám: {ecounter:6}, arány: {ecounter/len(ytrain)*100:3.2f}% \")\n",
        "    return(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7akwJMQ8Oza"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FCxVKUDhery"
      },
      "outputs": [],
      "source": [
        "df.iloc[188]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWKt2DCcFbKc"
      },
      "outputs": [],
      "source": [
        "wrong_prediction_list=show_difference(pred,df_y2,verbose=True,to_table=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwUpz9KG73GH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UF3Gzmf7vFt"
      },
      "outputs": [],
      "source": [
        "def show_batchID(df,index_list):\n",
        "    for i in index_list:\n",
        "        print(f\"{i:4}. --> {df.iloc[i].time.split('_')[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV04pFpA8_dj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo65biL-8djO"
      },
      "outputs": [],
      "source": [
        "show_batchID(df,wrong_prediction_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x39xe3TmGHJ_"
      },
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsQ2eUE8GZc_"
      },
      "outputs": [],
      "source": [
        "!pip install pypng\n",
        "!pip install pyqrcode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzKez2bME9eD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pyqrcode\n",
        "link_1 = pyqrcode.create('https://www.linkedin.com/in/36204746473/')\n",
        "link_1.png(\"p1.png\",scale=3,module_color=(20,20,20,255), background=(200,200,200,255))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlKyrRqfG6GP"
      },
      "source": [
        "### Model usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvmmKvBIF4n8"
      },
      "outputs": [],
      "source": [
        "test_df=pd.read_csv(\"test_200.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc0vyTmcHRvY"
      },
      "outputs": [],
      "source": [
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5utaqxGNHU7y"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykk17oAEoPwM"
      },
      "outputs": [],
      "source": [
        "__MAXWORD__=200\n",
        "test_word_list=list(test_df.Words_in_Numbers)\n",
        "x_test=create_x(test_word_list,maxword=__MAXWORD__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg0DqmUWoj5s"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itw7lX_hqO5m"
      },
      "outputs": [],
      "source": [
        "o=[\"kitaifa\",\"michezo\",\"biashara\",\"kamataifa\",\"burudani\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOQFlEC2oj2i"
      },
      "outputs": [],
      "source": [
        "def data_generator(y_pred):\n",
        "    out=[]\n",
        "    for pred in y_pred:\n",
        "        t1=[0,0,0,0,0]\n",
        "        ox=pred.argmax()\n",
        "        t1[ox]=1\n",
        "        out.append(t1) \n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSRQc0kWojzb"
      },
      "outputs": [],
      "source": [
        "output=data_generator(y_pred)\n",
        "\n",
        "output0=[x[0] for x in output]\n",
        "output1=[x[1] for x in output]\n",
        "output2=[x[2] for x in output]\n",
        "output3=[x[3] for x in output]\n",
        "output4=[x[4] for x in output]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G88pQ7jYojvu"
      },
      "outputs": [],
      "source": [
        "df_csv=pd.DataFrame()\n",
        "df_csv[\"test_id\"]=test_df[\"swahili_id\"]\n",
        "df_csv[o[0]]=output0\n",
        "df_csv[o[1]]=output1\n",
        "df_csv[o[2]]=output2\n",
        "df_csv[o[3]]=output3\n",
        "df_csv[o[4]]=output4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NSaY0zkyu1t"
      },
      "outputs": [],
      "source": [
        "df_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw4CUPobbH7A"
      },
      "outputs": [],
      "source": [
        "from datetime  import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAbhqmcPdF6U"
      },
      "outputs": [],
      "source": [
        "a=datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiOwO9UTdeUt"
      },
      "outputs": [],
      "source": [
        "fname=\"submission_\"+a+\".csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELciPC_OpWA"
      },
      "outputs": [],
      "source": [
        "df_csv.to_csv(fname,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvmaQZGBYM_7"
      },
      "outputs": [],
      "source": [
        "!head $fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrtPBTgcYwtg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HPLC_dense.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}